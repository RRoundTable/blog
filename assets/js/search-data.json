{
  
    
        "post0": {
            "title": "Integrated Gradient 정리글",
            "content": "Integrated Gradient 정리글 . Abstract . 해당 논문에서는 Attribution method의 방법론의 Axiom을 정리하였다. . Sensitivity . | Implementation Invariance . | 그리고 위의 두 가지 조건을 만족하는 Integrated gradient 방법론을 제시한다. . Motivation . 해당 연구의 목표는 input-output간의 관계를 파악하는 것이다. deep network prediction이 있을 때, 각 input feature가 어떤 영향을 끼쳤는지 알고 싶다. . 아래와 같은 형식으로 attribution은 정의될 수 있다. . deep network: $F: R^n rightarrow [0, 1]$ | input: $x = (x_1, cdots, x_n) in R^n$ | baseline input: $ acute{x}$ | . AF(x,xˊ)=(a1,⋯ ,an)∈RnA_F(x, acute{x}) = (a_1, cdots, a_n) in R^nAF​(x,xˊ)=(a1​,⋯,an​)∈Rn . 여기서 baseline의 역할은 비교대상이다. 예를 들어, object recognition 과제가 있을 때, input image에서 어떤 pixel이 특정 class라고 판단하게 하는지 구할 수 있다. 아래의 이미지처럼, attribution을 알고싶다면, baseline을 모두 0으로 처리할 수 있다. . . baseline은 model의 행동의 원인을 파악하고자 필요한 개념이다. prediction이 중립적인 상황을 가정하며, image의 경우에는 위와 같이 나타낼 수 있다. . Two Fundamental Axioms . Gradients . gradient는 일반적으로 deep network의 model coefficient를 알 수 있는 방법이다. 특정 input feature의 gradient가 높게 나온다면, 해당 input feature가 중요한 역할을 한다고 생각할 수 있다. 하지만, sensitivity라는 성질에서 좋지 않은 결과를 보여준다. . Sensitivity . sensitivity를 만족했다는 것은 다음을 의미한다. input하고 baseline의 차이가 오직 하나의 feature이고 서로 다른 예측을 한다면, 차이나는 feature에서는 non-zero attribution이 있다. . 하지만, gradient는 이러한 특성을 반영하지 못한다. 아래의 이미지를 살펴보자. . . baseline: $x=0$ | . 위의 상황에서 $x=2$ 일 때, gradient의 값은 0이다. 하지만, baseline을 고려해보면, 분명히 함수 값의 차이가 있다. ($y=0 rightarrow y = 1$) 따라서 $x=2$에서 attribution은 0이면 안된다. . gradient의 한계로 인해서, 연관성 없는 feature에 gradient 값이 높게 나오기도 한다. . Implementation Invariance . attribution이 두 개의 기능적으로 동일한 network상에서 항상 같아야 한다는 것이다. . . 우선, gradient 자체로는 implementation invariant하다. 아래의 수식을 살펴보자. . input: $f$ | output: $g$ | network: $h$ | . ∂f∂g=∂f∂h⋅∂h∂g frac{ partial f}{ partial g} = frac{ partial f}{ partial h} cdot frac{ partial h}{ partial g}∂g∂f​=∂h∂f​⋅∂g∂h​ . 하지만, LRP 혹은 DeepLift와 같은 방법론에서는 변형된 discrete gradient를 사용한다. 하지만, 이와 같은 discrete gradient에서는 chain rule이 성립하지 않는다. . f(x1)−f(x0)g(x1)−g(x0)≠f(x1)−f(x0)h(x1)−h(x0)⋅h(x1)−h(x0)g(x1)−g(x0) for all x1,x0 frac{f(x_1) - f(x_0)}{g(x_1) - g(x_0)} ne frac{f(x_1) - f(x_0)}{h(x_1) - h(x_0)} cdot frac{h(x_1) - h(x_0)}{g(x_1) - g(x_0)} text{ for all } x_1, x_0g(x1​)−g(x0​)f(x1​)−f(x0​)​​=h(x1​)−h(x0​)f(x1​)−f(x0​)​⋅g(x1​)−g(x0​)h(x1​)−h(x0​)​ for all x1​,x0​ . $f(x_1, x_2) = ReLU(h(x1, x2)) = ReLU(k(x_1, x_2))^3$ | $h(x_1, x_2) = ReLU(x_1) - 1 - ReLU(x_2)$ | $k(x_1, x_2) = ReLU(x_1 - 1) - ReLU(x_2)$ | $h$와 $k$는 서로 다르지만, $f$ 와$g$는 동일한 함수이다. | . 증명: $ReLU(x_1) - 1 ne ReLU(x_1 - 1)$ 라면, $f$ 와$g$는 동일한 함수가 아니다. 이는 $x_1 &lt; 1$인 경우이고 이 경우에는 $f$ 와$g$ 모두 0 값을 가진다. 따라서 동일한 함수이다. . implementation에 따라서 attribution이 달라진다면, 중요하지 않은 feature에 집중할수 있다. . Integrated Gradients . 정리해보면, gradient는 sensitivity하지 않다. 이를 해결하기 위해서 discrete하게 gradient를 구하게 되면, implementation에 따라 다른 결과가 나올 수 있다. 이러한 문제를 해결하기 위해서 integrated gradient를 제안한다. . IntegratedGradsi(x)=(xi−xˊi)×∫α=01∂F(xˊ+α(x−xˊ))∂xidαIntegratedGrads_i(x) = (x_i - acute{x}_i) times int_{ alpha=0}^1 frac{ partial F( acute{x} + alpha(x- acute{x}) )}{ partial x_i} d alphaIntegratedGradsi​(x)=(xi​−xˊi​)×∫α=01​∂xi​∂F(xˊ+α(x−xˊ))​dα . Completeness . ∑i=1nIntegratedGradsi(x)=F(x)−F(xˊ) sum_{i=1}^n IntegratedGrads_i(x) = F(x) - F( acute{x})i=1∑n​IntegratedGradsi​(x)=F(x)−F(xˊ) . 각 dimension의 모든 integrated gradient값을 더하면, 함수값의 차이가 된다. 이는 결국 각 attribution의 합이 함수의 값의 차이와 동일하다는 것을 의미한다. . Uniqueness of Integrated Gradients . object recognition task에서 attribution score top-k pixel을 제거해가면서 성능의 저하가 있는지 확인한다. 좋은 attribution method라면 점수는 급격히 떨어질 것이다. 하지만, 이러한 방법에는 문제가 있다. . unnatural한 data가 만들어진다. 따라서 성능의 저하가 단순히 attribution 때문이 아니라 처음 본 데이터 형식이라서 그럴 수 있다. . 여기서는 두 가지 단계가 논리를 전개한다. . path method 소개 | integrated gradient가 왜 path method중에서선택되었는지 설명 | Path Methods . . $ gamma = ( gamma_1, cdots, gamma_n) : [0, 1] rightarrow R^n$ | $ gamma(0) = acute{x}$ | $ gamma(1) = x$ | . PathIntegratedGradsiγ(x)=∫α=01∂F(γ(α))∂γi(α)∂γi(α)∂αdαPathIntegratedGrads_i^ gamma(x) = int_{ alpha=0}^1 frac{ partial F( gamma( alpha))}{ partial gamma_i( alpha)} frac{ partial gamma_i( alpha)}{ partial alpha} d alphaPathIntegratedGradsiγ​(x)=∫α=01​∂γi​(α)∂F(γ(α))​∂α∂γi​(α)​dα . 모든 path methods는 implementation invariance 성질을 만족한다. 그리고 integrated gradient도 path method중 하나이며, 위의 이미지에서 $P2$ linear combination의 path에 해당한다. . Integrated Gradients is Symmetry-Preserving . 두 input variable이 서로 교환하여도 fucntion의 output의 변화가 없다면, symmetry하다고 한다. F(x,y)=F(y,x)F(x, y) = F(y, x)F(x,y)=F(y,x) attribution method는 동일한 symmetry value를 가지고 있고 baseline의 symmetric variable이 동일한 attribution을 가진다면, symmetry preserving하다고 한다. . 예시) Sigmoid(x1+x2,⋯ )Sigmoid(x1 + x2, cdots)Sigmoid(x1+x2,⋯) $x_1, x_2$는 symmetric variable이고 input에서는 $x_1=x_2=1$ 이며, basline에서는 $x_1=x_2=0$이다. symmetry preserving하다면, $x_1, x_2$에 모두 동일한 attribution 값이 나와야한다. . 그리고, integrated gradient는 이러한 조건을 만족한다. 아래를 간략히 정리하면, non-straightline은 symmetry preserving하지 않다는 것이다. . .",
            "url": "https://fastpages.fast.ai/deeplearning/xai/2020/05/05/integrated-gradient_.html",
            "relUrl": "/deeplearning/xai/2020/05/05/integrated-gradient_.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "# RAPP: NOVELTY DETECTION WITH RECONSTRUCTION ALONG PROJECTION PATHWAY 정리글",
            "content": "RAPP: NOVELTY DETECTION WITH RECONSTRUCTION ALONG PROJECTION PATHWAY 정리글 . Abstract . autoencoder를 활용하여 anomaly detection을 수행하는 시도들이 있다. 하지만, 이러한 방법론은 주로 input과 reconstruct된 output간의 차이에 집중하고, hidden space간의 관계에 대해서는 고려하지 않는다는 한계를 가지고 있다. 본 연구에서는 autoencoder 구조를 바탕으로 input space와 output간의 관계뿐만 아니라, hidden space간의 관계를 고려하는 방법을 제안한다. (encoder hidden space와 대응되는 decoder hidden space와 비교) 또한, reconstructed된 output을 다시 동일한 autoencoder에 넣어서 발생되는 activation value가 original input을 동일한 모델에 넣었을때의 대응되는 decoder activation value와 동일하다는 것을 보였다. . Introduction . 아쉽게도 encoder hidden space와 대응되는 decoder hidden space를 비교하는 것은 불가능하다. 왜냐하면, 학습이 진행되는 동안은 모델이 불안정하며 encoder-decoder layer pair가 대응된다고 볼 수 없기 때문이다. 하지만 reconstructed된 output을 다시 동일한 autoencoder에 넣어서 발생되는 activation value가 original input을 동일한 모델에 넣었을때의 대응되는 decoder activation value와 동일하다는 것을 보임으로써 문제를 해결할 수 있었다. . Contributions . hidden space도 함께 활용한 방법론 | RaPP의 motivation 소개 및 증명 | RaPP의 성능 증명 | . Proposed Method RaPP . . 위의 이미지 처럼 encoder와 decoder간의 hidden space를 비교하는 것이 목표이지만, 위에서 언급한 한계때문에 불가능하다. 그래서 새롭게 제안하는 방법은 reconstruction output을 다시 autoencoder에 넣어서 decoder의 hidden space들을 재현하는 것이다. . Reconstruction Based Novelty Detection . autoencoder $A$ 는 unsupervised 방법을 바탕으로 의미있는 representation을 만들어내며, 아래는 사용되는 objective이다. ϵ=∣∣x−A(x)∣∣2      whrere A = f(g(x))  epsilon = mid mid x - A(x) mid mid _2 text{whrere A = f(g(x)) }ϵ=∣∣x−A(x)∣∣2​      whrere A = f(g(x))  또한, $ epsilon(x)$가 높다면, anomaly일 가능성이 높아진다고 해석할 수 있다. 하지만, 모델의 구조가 깊어질수록 hierarchical information을 활용하지 못하는 점에서 아쉬움이 남는다. 딥러닝 모델의 모든 layer를 충분히 활용하려면 결국 hidden space를 잘 활용할 필요가 있다. . Reconstruction Error In Hidden Spaces . . 위에 보이는 이미지처럼, $h_i(x), hat{h}_i(x)$를 각각 구한다. . 다음과 같은 encoder를 정의하면 위의 과정을 전개할 수 있다. g:i=gi∘⋯∘g1g_{:i} = g_i circ cdots circ g1g:i​=gi​∘⋯∘g1 . hi(x)=g:ihi(x)^=g:i(x^)=g:i(A(x))   where A is autoencoderh_i(x) = g_{:i} hat{h_i(x)} = g_{:i}( hat{x}) = g_{:i}(A(x)) text{where A is autoencoder}hi​(x)=g:i​hi​(x)^​=g:i​(x^)=g:i​(A(x))   where A is autoencoder . 그리고 위와 같은 hidden space value들을 decoder hidden space의 수 만큼 모아준다. H(x)={(hi(x),h^i(x)):i≤i≤l}H(x) = { (h_i(x), hat{h}_i(x)) : i le i le l }H(x)={(hi​(x),h^i​(x)):i≤i≤l} $H(x)$를 활용해서 novelty score를 얻기 위해 다음과 같은 과정을 진행한다. . . 여기서 $S$ 는 novelty score를 측정하는 함수로 본 연구에서는 크게 두 가지 방향을 제시한다. . Simple Aggregation Along Pathway(SAP) . 두 hidden space간의 Euclidean distance를 구한다. . SSAP(x)=∑i=0l∣∣hi(x)−h^i(x)∣∣22=∣∣h(x)−h^i(x)∣∣x2S_{SAP}(x) = sum_{i=0}^l mid mid h_i(x) - hat{h}_i(x) mid mid_2^2 = mid mid h(x) - hat{h}_i(x) mid mid _x^2SSAP​(x)=i=0∑l​∣∣hi​(x)−h^i​(x)∣∣22​=∣∣h(x)−h^i​(x)∣∣x2​ . Normalized Aggregation Along Pathway(NAP) . 위의 SAP 방법은 각 hidden space간의 특성은 고려하지 못한다. 각 pair마다 distance distribution이 다르게 나타날 수 있는데 이러한 문제를 해결하기 위해서 orthogonalization과 scaling을 통한 normalization 방법을 제안한다. . SNAP(x)=∣∣d(x)−uX)TVΣ−1∣∣22S_{NAP}(x) = mid mid d(x) - uX)^T V Sigma^{-1} mid mid_2^2SNAP​(x)=∣∣d(x)−uX)TVΣ−1∣∣22​ . $d(x) = h(x) - hat{h}(x)$ | $D$ 는 matrix이며 각 row i는 data point $x_i$가 가지고 있는 pair들의 distance들로 이루어져있다. | $ bar{D} $는 $D$의 column wise centered matrix 이다. | normalization을 위해서 SVD를 수행한다. $ bar{D} = U Sigma V^T$ | . Motivation Of RAPP . ‘hidden space정보를 활용할 수 있지 않을까’라는 동기에서 연구가 출발했다. 하지만, 이런 문제의식을 가지더라도 해결해야될 이슈가 있다. 대응하는 encoder decoder의 layer pair가 서로 같은 space를 표현한다고 할 수 없다. 왜냐하면, autoencoder의 objective는 각 layer에 들어오는 input에 대해서 어떤 제약도 하지 않기 때문이다. 결과적으로 $f_{l:i+1}(g(x)) = g_{:i}(x)$과 같은 관계가 성립한다고 볼 수 없다. . 그럼에도 불구하고, $ hat{h}i(x) = g(A(x))$의 관계를 바탕으로, 위의 문제의식을 실현할 수 있었다. 전반적인 프로세스는 아래의 Figure1 (b)를 확인하면 알 수 있다. . . Computation Of Hidden Reconstruction . $A = f circ g$은 학습된 autoencoder라고 가정한다. | $M_0 = { A(x): x in R^n }$은 reconstruction된 output 집합이다. $A$는 다음과 같이 표현된다. $ x in M_0, x= A(x)$ | 해석하자면, reconstruction된 결과를 다시 autoencoder에 넣으면 input으로 넣은 reconstruction과 동일하다는 것이다. | . | $M_i = { g_{:i(x)}: x in M_0 }$ | . . 다음과 같은 decoder $ tilde{f}$ 가 있다고 가정하자. . ∀x∈Ml,f~(x)=f(x)∀a∈Mi,a=(gi∘f~)(a) forall x in M_l, tilde{f}(x) = f(x) forall a in M_i, a = (g_i circ tilde{f})(a)∀x∈Ml​,f~​(x)=f(x)∀a∈Mi​,a=(gi​∘f~​)(a) . 첫 번째 조건 $x in M_l, tilde{f}(x) = f(x) $은 feed foward의 결과가 같다는 것을 의미한다. . 두 번째 조건$a in M_i, a = (g_i circ tilde{f})(a)$은 $ tilde{f}{l:i+1}$과 $g{i+1:}$이 서로 적합한 encoder decoder의 pair로 만들어준다. 즉 서로 inverse의 관계를 가진다. 따라서 다음과 같은 수식 전개가 가능하다. h′^i=(f~l:i+1∘gi+1:)(hi(x)) hat{h^{&amp;#x27;}}_i = ( tilde{f}_{l:i+1} circ g_{i+1:})(h_i(x))h′^i​=(f~​l:i+1​∘gi+1:​)(hi​(x)) 이는 Figure1 (b)를 보면, 어떤 의미인지 알 수 있다. 간략히 설명하면, decoder 부분의 hidden space다. . 그리고 다음과 같은 과정을 통해서 $ hat{h^{‘}}_i(x) = hat{h}_i(x)$임을 증명할 수 있다. begin{aligned} begin{split} hat{h^{&amp;#x27;}}_i(x) = ( tilde{f}_{l:i+1} circ g_{i+1:})(h_i(x)) &amp;= ( tilde{f}_{l:i+1} circ g)(x) &amp;= (g_{:i} circ tilde{f} circ g)(x) &amp;= (g_{:i} circ A)(x) = h_i( hat{x}) = hat{h}_i(x) end{split} end{aligned} . 주목할 점은 $ hat{h^{‘}}_i$을 얻기 뒤해서 $ tilde{f}_i$가 필요하지 않다는 것이다. 그리고 $x in M_0$에 대해서는 다음과 같은 관계도 성립한다. hi(x)=h^i(x)=h′^i(x), mboxforevery1≤i≤lh_i(x) = hat{h}_i(x) = hat{h^{&amp;#x27;}}_i(x), mbox{ for every } 1 le i le lhi​(x)=h^i​(x)=h′^i​(x), mboxforevery1≤i≤l . Existence of f_tilde . 전제조건은 다음과 같다. . $x = A(x) text{ for } x in M_0$ | $g_i(x: x in M_{i-1}) = hat{x}: hat{x} in M_i$ | $f_i(x: x in M_{i-1}) = hat{x}: hat{x} in M_i$ | . 여기에 다음과 같이 정의해보자. . $ tilde{f}_i =g_i^{-1} text{ for } M_i$이라면, 이런관계가 성립한다. $ tilde{f} = g^{-1}$ 해석하면, 각 encoder decoder layer pair에 대해서 inverse 관계가 성립하게 되면, encoder decoder에 대해서도 inverse 관계가 성립하게 된다. . 이런 정의는 아래와 같은 조건을 충족시킨다. . x=(f~∘g)(x) for x∈M0x = ( tilde{f} circ g)(x) text{ for } x in M_0x=(f~​∘g)(x) for x∈M0​ . $x = A(x)$ 라는 전제조건을 생각해보면, . f~=f on Ml tilde{f} = f text{ on } M_lf~​=f on Ml​ . Existence of f_tilde With Neural Networks . neural network는 유연한 구조를 가졌기때문에, 특정 함수를 쉽게 근사할 수 있다. . reference: 마키나락스 | .",
            "url": "https://fastpages.fast.ai/2020/04/02/RaPP.html",
            "relUrl": "/2020/04/02/RaPP.html",
            "date": " • Apr 2, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Variational Autoencoder based Anomaly Dection using Reconstruction Probability",
            "content": "Backgraound . Anomaly deteciton . anomaly detection 방법은 다음과 같은 3가지 관점으로 분류할 수 있다. . statistical | proximity | deviation | . statistical관점은 data가 특정 분포를 따른다고 가정한다. 예를 들어, parametric model인 Gaussian Mixture Model이나 nonparametric model인 kernel density estimation은 이런 분포를 정의하는데 사용될 수 있다. 만약 특정 data point가 가정한 분포에서 낮은 확률을 가진다면, anomaly라고 정의할 수 있다. probability를 기반으로 anomaly를 정의할 수 있다는 장점을 가지고 있다. . proximity관점은 anomaly data가 다른 데이터와 고립되어 있다고 가정한다. 이 관점은 크게 세가지 방법을 가지고 있다. . clustering . cluster의 크기나 centroid의 거리를 가지고 anomaly를 결정한다. . | density . 특정 위치의 data가 밀집된 정도가 낮다면, anomaly로 정의할 수 있다. . | distance . neighbor와의 거리가 멀다면, anomaly로 정의할 수 있다. . | . deviation . reconstruction error를 바탕으로 anomaly를 정의할 수 있다. 이를 위해서는 우선, dimension reduction방법을 통해 압축된 정보를 바탕으로 reconstruct해야한다. 그리고 실제 데이터와의 차이를 reconstruction error로 정의한다. 만약 reconstruction error가 특정 수준보다 높다면, anomaly라고 정의한다. . Autoencoder and anomaly detection . autoencoder의 objective는 아래와 같다. . $$ L_ text{AE}( theta, phi) = frac{1}{n} sum_{i=1}^n ( mathbf{x}^{(i)} - f_ theta(g_ phi( mathbf{x}^{(i)})))^2 $$autoencoder를 활용하여, anomaly detection을 수행하면, reconstruction error $ mid mid mathbf{x}^{(i)} - f_ theta(g_ phi( mathbf{x}^{(i)})) mid mid$를 anomaly를 판별하는데 사용한다. . Variational Autoencoder . variation Autoencoder는 다음 링크에서 자세히 확인 할 수 있다. . https://rroundtable.github.io/FastPages/vae/deeplearning/2020/03/25/VAE.html . autoencoder와의 다른 점은 autoencoder는 input variable자체를 압축하고 복원하는 과정을 거치지만, VAE는 복원하는 것이 input variable로 부터 만들어진 paramters(mean, var)를 바탕으로 복원한다. . Proposed Method . Algorithm . Monte-Carlo estimate를 통해서 $ mathbb{E}_{q_{ phi (x mid z)} [ log p_{ theta}(x mid z)]}$를 근사할 수 있다. . 한 데이터 포인트의 reconstruction probability는 다음과 같다. . $$ frac{1}{L} sum_{l=1}^L p_{ theta}(x^{(i)} mid u_{ hat{x}^{(i, l)}}, sigma_{ hat{x}^{(i, l)}}) $$Reconstruction Probability . VAE는 복원하는 것이 input variable로 부터 만들어진 paramters(mean, var)를 바탕으로 복원한다. 이 mean과 var로 만들어진 분포는 확률분포로 해석할 수 있다. . 또한, mean값 뿐만 아니라 variance도 함께 고려한다는 점도 중요하다. . Difference from an autoencoder based anomaly detection . 첫 번째 차이점은 VAE의 latent variable이 stochastic variable이라는 것이다.(latent variable이 복원해야하는 대상이 아니고 복원에 필요한 parameter를 의미한다.) autoencoder의 latent variable은 deterministic하게 정해진다. 이런 차이점이 VAE의 표현능력을 키워준다. 특정 분포에서 sampling하기 때문에, 다양한 variable를 경험할 수 있다. 그리고 anomaly data는 variance가 높아서 reconstructiuon probability가 낮다. . 두 번째 차이점은 VAE의 reconstruction variable 또한 stochastic하다는 것이다. reconstruction probability는 original input과 reconstruction간의 차이와 함께 variance도 고려한다. . 세 번째 차이점은 VAE는 복원정도를 probability 확률로 측정할 수 있다는 것이다. autoencoder는 reconstruction error를 통해서 measure해야하기 때문에, input variable이 heterogeneous하다면 측정하기 힘들다. heterogeneous하다면, 각각 다른 가중치를 곱해줘야 하며, anomaly를 판단하기 위한 threshold를 결정하기도 쉽지 않다. . 반면에, VAE는 heterogeneous한 데이터에도 각각 다른 가중치를 곱할 필요가 없다. 이는 각 heterogeneous한 데이터 마다 variance를 구해주기 때문에 각각의 분포를 정의할 수 있게 되기 때문이다. 또한 1%의 확률은 어떤 데이터에서든 1%의 의미를 가진다. . Pytorch &#44396;&#54788;&#52404; . https://github.com/GunhoChoi/PyTorch-FastCampus | . 1) Import required libraries . import numpy as np import torch import torch.nn as nn import torch.optim as optim import torch.nn.init as init import torchvision.datasets as dset import torchvision.transforms as transforms from torch.utils.data import DataLoader from torch.autograd import Variable import matplotlib.pyplot as plt %matplotlib inline . 2) Set hyperparmeters . batch_size = 128 learning_rate = 0.0005 num_epoch = 10 hidden_size = 50 . 3) Download Data . mnist_train = dset.MNIST(&quot;./&quot;, train=True, transform=transforms.ToTensor(), target_transform=None, download=True) mnist_test = dset.MNIST(&quot;./&quot;, train=False, transform=transforms.ToTensor(), target_transform=None, download=True) . Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw Processing... Done! . 4) Set DataLoader . train_loader = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size, shuffle=True,num_workers=2,drop_last=True) test_loader = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size, shuffle=False,num_workers=2,drop_last=True) . 5) Model . class Encoder(nn.Module): def __init__(self): super(Encoder,self).__init__() self.fc1_1 = nn.Linear(784, hidden_size) # for mu self.fc1_2 = nn.Linear(784, hidden_size) # for logvar self.relu = nn.ReLU() def encode(self,x): batch_size = x.size()[0] x = x.view(batch_size,-1) mu = self.relu(self.fc1_1(x)) log_var = self.relu(self.fc1_2(x)) return mu,log_var def reparametrize(self, mu, logvar): &#39;&#39;&#39; Transformation for differentiable function Return z = mu + std * eps &#39;&#39;&#39; std = logvar.mul(0.5).exp_() eps = torch.FloatTensor(std.size()).normal_() eps = Variable(eps).cuda() return eps.mul(std).add_(mu) def forward(self,x): mu, logvar = self.encode(x) reparam = self.reparametrize(mu,logvar) return mu,logvar,reparam class Decoder(nn.Module): def __init__(self): super(Decoder,self).__init__() self.fc1 = nn.Linear(hidden_size, 784) self.sigmoid = nn.Sigmoid() def forward(self,x): batch_size = x.size()[0] out = self.fc1(x) out = self.sigmoid(out) out = out.view(batch_size,28,28,1) return out class VAE(nn.Module): def __init__(self): super(VAE,self).__init__() self.encoder = Encoder() self.decoder = Decoder() self.bce = nn.BCELoss(size_average=False) self.L = 30 def forward(self, x): mu, logvar, reparam = self.encoder(x) out = self.decoder(reparam) return mu, logvar, out def get_recon_prob(self, x): &#39;&#39;&#39;Not Completed: Ask to someone&#39;&#39;&#39; mu, logvar = self.encoder.encode(x) bce_loss = 0 for i in range(self.L): reparam = self.encoder.reparametrize(mu, logvar) recon = self.decoder(reparam) bce_loss += self.bce(recon, x) bce_loss /= self.L return bce_loss model = VAE().cuda() . /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction=&#39;sum&#39; instead. warnings.warn(warning.format(ret)) . 6) Loss function &amp; Optimizer . reconstruction_function = nn.BCELoss(size_average=False) def loss_function(recon_x, x, mu, logvar): BCE = reconstruction_function(recon_x, x) # reconstruction error # see Appendix B from VAE paper: # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014 # https://arxiv.org/abs/1312.6114 # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2) KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar) KLD = torch.sum(KLD_element).mul_(-0.5) # regularizer term return BCE + KLD parameters = list(model.parameters()) optimizer = torch.optim.Adam(parameters, lr=learning_rate) . /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction=&#39;sum&#39; instead. warnings.warn(warning.format(ret)) . 7) Train . res = [] for i in range(num_epoch): cum_loss = 0 for j,[image,label] in enumerate(train_loader): optimizer.zero_grad() image = Variable(image).cuda() mu, logvar, output = model(image) loss = loss_function(output, image, mu, logvar) cum_loss += loss loss.backward() optimizer.step() res.append(cum_loss / len(train_loader)) # Train graph plt.plot(res) plt.show() . /usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([128, 1, 28, 28])) that is different to the input size (torch.Size([128, 28, 28, 1])) is deprecated. Please ensure they have the same size. return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction) . 8) Make anomaly data . ood1 = np.zeros((28, 28), dtype=np.float32) plt.imshow(ood1) plt.title(&quot;ood 1&quot;) plt.show() ood2 = np.random.random((28, 28)) plt.imshow(ood2) plt.title(&quot;ood 2&quot;) plt.show() . Test . for j,[in_dist,label] in enumerate(test_loader): break in_dist = torch.unsqueeze(in_dist[0], 0) in_recon_prob = model.get_recon_prob(in_dist.cuda()) . /usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 28, 28])) that is different to the input size (torch.Size([1, 28, 28, 1])) is deprecated. Please ensure they have the same size. return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction) . out_dist = np.random.random((1, 1, 28, 28)) out_dist = torch.FloatTensor(out_dist).cuda() out_recon_prob = model.get_recon_prob(out_dist) . /usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 28, 28])) that is different to the input size (torch.Size([1, 28, 28, 1])) is deprecated. Please ensure they have the same size. return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction) . in_recon_prob, out_recon_prob . (tensor(110.3578, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;), tensor(1126.5494, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;)) .",
            "url": "https://fastpages.fast.ai/2020/03/30/anomaly-detection-by-VAE.html",
            "relUrl": "/2020/03/30/anomaly-detection-by-VAE.html",
            "date": " • Mar 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "VAE Tutorial",
            "content": "Auto-Encoding Variational Bayes . Abstract . intractable한 posterior distribution의 random variable인 latent variable과 probabilitstic model이 주어졌을 때, 어떻게 효율적인 inference와 learning을 진행할 수 있을까? . 해당 논문에서는 대형 데이터셋에도 적용가능한 stochastic variational inference와 learning 알고리즘을 제안한다. . 이 논문의 contribution은 두 가지이다. . reparameterisztion of variational lower bound를 통해서 SGD를 사용하여 최적화할 수 있다. | 독립적인 데이터라는 가정하에서 lower bound estimator를 이용하여 poseterior inference(intractable)를 근사하여 구할 수 있다.probabilistic model과 대비되는 개념으로는 deterministic model이 있다. 일반적으로 output이 특정확률 분포에서 추출된 random variable이라고 가정한다. . | graph model . 위의 그림을 보면, 대략적인 구조를 알 수 있다. 우선, 실선은 generative model $p_{ theta}(Z)p_{ theta}(X mid Z)$을 나타내며, 점선은 variational approximation $q_{ phi}(Z mid X)$과정을 나타낸다. . Method . 우선, 다음과 같은 가정을 하였다. . i.i.d dataset with latent variables per datapoint | . 위와 같은 가정에서 global parameter 기반으로 MLE 혹은 MAP inference를 진행하고 latent variable으로 variational inference를 진행했다. global parameter에 variational inference를 진행해도 되지만, 해당 논문에서는 다루지 않았다. . Problem scenario . i.i.d dataset $X = { x^{(i)} }^N_{i=1}$ 은 관측되지 않고 연속적인 변수인 $z$에 생성된 것이라고 가정하고 이는 다음과 같은 과정을 통해서 이루어진다. . $z$는 prior distribution에서 생성된다. . $$z sim p_{ theta *}(z)$$ . | 데이터는 조건부 확률을 통해서 생성된다. . $$x^{(i)} sim p_{ theta *}(x mid z)$$ . | 그리고, $p_{ theta *}(z)$와 $p_{ theta *}(x mid z)$는 각각 미분가능한($ theta, z$에 대해서) PDF(확률밀도함수)를 가진 $p_{ theta}(z)$와 $p_{ theta}(x mid z)$에서 출발했다고 가정한다. . 중요한 점은 위의 과정에서는 marginal probability와 posterior probability에 대해서는 어떤 가정도 하지 않았다는 점이다. 이는 아래의 경우에서 보편적으로 사용할 수 있다는 뜻이다. . Intractability . margianl likelihood $p_{ theta}(x) = int p_{ theta}(z) p_{ theta}(x mid z)$는 intractable하다. 일반적으로 neural network는 수많은 hidden layer로 이루어져있기 때문에 해당 liklihood$p_{ theta}(x mid z)$는 intractable하다. 또한 이런 이유 때문에 unsupervised learning에서 사용되는 EM알고리즘은 사용하기 힘들다. . | A large dataset . Monte Carlo EM은 너무 느리다. . | 결론적으로 해당 논문이 하고자 하는바는 다음과 같다. . Efficient approximate ML or MAP estimation for the paramters $ theta$ | $$ p_{ theta}(x mid z) $$ Efficient approximate posterior inference of the latent variable z given an observed value x for a choice of parameters $ theta$ | $$ p_{ theta}(z mid x) $$ Efficient approximate marginal inference of the variable X | $$ p_{ theta}(x) = int p_{ theta}(x mid z) p_{ theta}(z) dz $$위의 목적을 달성하기 위해서, recognition model $q_{ phi}(z mid x)$를 사용하였다. . $$ q_{ phi}(z mid x) approx p_{ theta}(z mid x) $$아래에는 $ theta, phi$을 함께 학습시키는 방법을 소개할 것이다. . mean-field variational inference 추후 작성 예정 . $q_{ phi}(Z mid X)$는 probabilistic encoder 역할을 하고 $p_{ theta}(X mid Z)$는 probabilistic decoder의 역할을 한다. . . The variational bound . variational inference란, intractable한 posetrior분포 $p(z mid x)$를 다루기 쉬운 확률분포 $q(z)$로 근사하는 것을 의미한다. intractable한 posterior를 직접다루지 않고, lower bound를 증가하는 방식으로 ${ D }_{ KL } left( q left( z right) mid mid p left( z mid x right) right)$를 감소시킬 수 있다. . $$ D_{ mathrm{KL}}(Q parallel P) = sum_ mathbf{Z} Q( mathbf{Z}) left[ log Q( mathbf{Z}) - log P( mathbf{Z}, mathbf{X}) right] + log P( mathbf{X}) $$$$ D_{ mathrm{KL}}(Q parallel P) = mathbb{E}_{ mathbf Z } left[ log Q( mathbf{Z}) - log P( mathbf{Z}, mathbf{X}) right] + log P( mathbf{X}) $$ marginal probability, 즉 사후확률의 분모인 $p(x)=Σzp(x,z)$를 계산하기 힘든 경우 | likelihood, 즉 $p(x mid z)$를 더 복잡하게 모델링하고 싶은 경우 | prior, 즉 p(z)를 더 복잡하게 모델링하고 싶은 경우 | . 전체 marginal likelihood는 각 data point들의 marginal likelihood의 합으로 구성된다. . $$ log p_{ theta}(X^{(1)}, cdots, X^{(N)} = sum _{i=1}^N log p_{ theta}(X^{(i)}) $$그리고 각 data point의 marginal likelihood는 아래와 같이 구성된다. . $$ log p_{ theta}(X^{(i)}) = D_{KL}(q_{ phi}(Z mid X^{(i)}) mid mid p_{ theta}(Z mid X^{(i)})) + mathcal{L}( theta, phi; X^{(i)}) $$첫 번째 term인 $D_{KL}(q_{ phi}(Z mid X^{(i)}) mid mid p_{ theta}(Z mid X^{(i)}))$은 true posterior와 pred posterior간의 kl-divergence 값으로 non-negative하다는 성격을 가진다. 두 번째 term $ mathcal{L}( theta, phi; X^{(i)})$은 lower bound이며 다음과 같이 표현될 수 있다. . $$ log p_{ theta}(X^{(i)}) ge mathcal{L}( theta, phi; X^{(i)}) = mathbb{E}_{q_{ phi}(Z mid X)}[- log q_{ phi}(z mid x) + log p_{ theta}(X)] $$$$ mathcal{L}( theta, phi; X^{(i)}) = -D_{KL}(q_{ phi}(Z mid X^{(i)}) mid mid p_{ theta}(Z )) + mathbb{E}_{q_{ phi}(Z mid X)}[ log p_{ theta}(X)] $$위의 형태는 $ mathcal{L}( theta, phi; X^{(i)})$를 미분하고 최적화를 variational parameters $ phi$와 generative parameters $ theta$에 대해서 할 수 있다. 하지만, $ phi$에 대해서 lower bound의 gradient를 구하는 것은 약간의 문제가 있다. 일반적인 Monte Carlo를 이용하게 되면, variance가 너무 높다는 한계가 있다. . The SGVB estimator and AEVB algorithm . 이 섹션에서는 효율적으로 lower bound와 lower bound의 parameter에 대한 미분을 구하는 것을 다룬다. . reparameterize를 통해서 $ tilde{z} sim q_{ phi}(z mid x)$를 다음과 같은 미분가능한 fuction의 형태로 바꿔줄 수 있다. . $$ tilde{z} = g_{ phi}( epsilon, X) text{ with } epsilon sim p( epsilon) $$이를 통해서, Monte Carlo를 이용하여 $f(z)$ w.r.t $q_{ phi}(z mid x)$의 기댓값을 구할 수 있다. . $$ mathbb{E}_{q_{ phi}(z mid x ^{(i)})}[f(z)] = mathbb{E}_{p( epsilon)}[f(g_{ phi}( epsilon, X ^{(i)}))] = frac{1}{L} sum_{l=1}^L f(g_{ phi}( epsilon^{(l)}, X ^{(i)})) text{ where } epsilon^{(l)} sim p( epsilon) $$여기서, variational lower bound에 Monte Carlo Estimation을 적용하면 아래와 같이 전개된다. ($ mathcal{L}( theta, phi; X^{(i)}) = mathbb{E}_{q_{ phi}(Z mid X)}[- log q_{ phi}(z mid x) + log p_{ theta}(X)]$) . SGVB(stochastic gradient variational bayes) 1 . $$ tilde{ mathcal{L}}^A( theta, phi; X^{(i)}) = frac{1}{L} sum_{l=1}^L log p_{ theta}(X^{(i)}, Z^{(i, l)}) - log q_{ phi}(Z^{(i, l)} mid X^{(i)}) text{ where } Z^{(i, l)} = g_{ phi}( epsilon^{(i, l)}, X^{(i)}), epsilon^{(l)} sim p( epsilon) $$또한, kl-divergence term $D_{KL}(q_{ phi}(Z mid X^{(i)}) mid mid p_{ theta}(Z))$은 적분될 수 있는데, 이는 해당논문의 appendix B를 참고하길 바란다. . kl-divergence는 $ phi$를 regularization한다고 해석할 수 있는데, approximate posterior가 prior $p_{ theta}(z)$를 근사하도록 한다. . 이러한 관점을 바탕으로 아래와 같은 SGVB(stochastic gradient variational bayes)가 전개된다. . SGVB(stochastic gradient variational bayes) 2 . sampling을 통해서 gradient estimate 방법보다 더 낮은 variance를 가진다. $ log p_{ theta}(x^{(i)}) mid z^{(i, l)})$은 reconstruction error를 나타낸다. . $$ tilde{ mathcal{L}}^B( theta, phi; X^{(i)}) = -D_{KL}(q_{ phi}(Z mid X^{(i)}) mid mid p_{ theta}(Z)) + frac{1}{L} sum_{l=1}^L ( log p_{ theta}(x^{(i)}) mid z^{(i, l)}) text{ where } Z^{(i, l)} = g_{ phi}( epsilon^{(i, l)}, X^{(i)}), epsilon^{(l)} sim p( epsilon) $$ $ log p_{ theta}(x^{(i)}) mid z^{(i, l)})$ 은 다음과 같은 관계를 가진다. | . $$ p_{ theta}(x^{(i)}) mid z^{(i, l)}) = p_{ theta}(x^{(i)}_1) mid z^{(i, l)}) times cdots times p_{ theta}(x^{(i)}_M) mid z^{(i, l)}) $$$$ log p_{ theta}(x^{(i)}) mid z^{(i, l)}) = log p_{ theta}(x^{(i)}_1) mid z^{(i, l)}) + cdots + log p_{ theta}(x^{(i)}_M) mid z^{(i, l)}) $$The reparameterization trick . reparameterize를 통해서 $ tilde{z} sim q_{ phi}(z mid x)$를 다음과 같은 미분가능한 fuction의 형태로 바꿔줄 수 있다. . $$ tilde{z} = g_{ phi}( epsilon, X) text{ with } epsilon sim p( epsilon) $$여기서 $ epsilon$은 auxiliary variable이며, 이를 통해서 $g_{ phi}( cdot)$은 vector z를 도출한다. . reparameterization이 필요한 이유는 미분가능한 상태를 얻기 위해서이다. 아래는 증명이다. . $$ q_{ phi}(Z mid X) dz = p( epsilon) d epsilon $$그러므로, . $$ int q_{ phi}(Z mid X) f(z) dz = int p( epsilon) f(Z) d epsilon = int p( epsilon) f(g_{ phi}( epsilon, X)) d epsilon $$그리고, Monte Carlo Estimation을 고려하면, . $$ int q_{ phi}(Z mid X) f(z) dz approx frac{1}{L} sum_{l=1}^Lf(g_{ phi}( epsilon^{(l)}, X)) $$가우시안 분포를 예시로 살펴보자 . $$ z sim p(z mid x) = mathcal{N}(u, sigma^2) $$reparameterization을 적용하게 되면, 다음과 같이 생각할 수 있다. . $$ z = u + sigma epsilon text{ where } epsilon sim mathcal{N}(0, 1) $$이를 바탕으로, 다음과 같은 수식전개가 가능하다. . $$ mathbb{E}_{ mathcal{N}(z;u, sigma^2)}[f(z)] = mathbb{E}_{ mathcal{N}( epsilon;0,1)}[f(u + sigma epsilon)] approx frac{1}{L} sum_{l=1}^L f(u + sigma epsilon ^{(l)}) text{ where } epsilon^{(l)} sim mathcal{N}(0,1) $$어떤 $q_{ phi}(z mid x)$에대해 변환 함수 $g_{ phi}( cdot)$과 $ epsilon sim p( epsilon)$을 사용하여 위와 같이 reparmeterization 할 수 있을지 살펴보자. . Tractable Inverse of CDF | Gaussian과 유사하게 location, scale 개념의 분포 | random variables as different transformations of auxiliary variables | 위의 과정이 모두 실패하면, inverse of CDF를 근사하는 방식으로 접근할 수 있으며, time complexity는 PDF만큼 필요하다고 한다. . inverse CDF . 아래와 같이 CDF가 정의된다면, . $$ F(X) = U text{ where X is random variable} $$inverse CDF는 다음과 같이 정의될 수 있다. . $$ F^{-1}(U) = X text{ where X is random variable} $$즉, $g_{ phi}( cdot)$를 inverse CDF로 사용한다는 것은 $ epsilon$을 활용하여, 난수를 생성한다고 이해할 수 있다. . Pytorch &#44396;&#54788;&#52404; . reference: https://github.com/GunhoChoi/PyTorch-FastCampus | . 1) Import required libraries . import numpy as np import torch import torch.nn as nn import torch.optim as optim import torch.nn.init as init import torchvision.datasets as dset import torchvision.transforms as transforms from torch.utils.data import DataLoader from torch.autograd import Variable import matplotlib.pyplot as plt %matplotlib inline . 2) Set hyperparameters . batch_size = 128 learning_rate = 0.0005 num_epoch = 10 hidden_size = 50 . 3) Download Data . mnist_train = dset.MNIST(&quot;./&quot;, train=True, transform=transforms.ToTensor(), target_transform=None, download=True) mnist_test = dset.MNIST(&quot;./&quot;, train=False, transform=transforms.ToTensor(), target_transform=None, download=True) . Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw Processing... Done! . 4) Set DataLoader . train_loader = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size, shuffle=True,num_workers=2,drop_last=True) test_loader = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size, shuffle=False,num_workers=2,drop_last=True) . 5) Model . class Encoder(nn.Module): def __init__(self): super(Encoder,self).__init__() self.fc1_1 = nn.Linear(784, hidden_size) # for mu self.fc1_2 = nn.Linear(784, hidden_size) # for logvar self.relu = nn.ReLU() def encode(self,x): x = x.view(batch_size,-1) mu = self.relu(self.fc1_1(x)) log_var = self.relu(self.fc1_2(x)) return mu,log_var def reparametrize(self, mu, logvar): &#39;&#39;&#39; Transformation for differentiable function Return z = mu + std * eps &#39;&#39;&#39; std = logvar.mul(0.5).exp_() eps = torch.FloatTensor(std.size()).normal_() eps = Variable(eps).cuda() return eps.mul(std).add_(mu) def forward(self,x): mu, logvar = self.encode(x) reparam = self.reparametrize(mu,logvar) return mu,logvar,reparam class Decoder(nn.Module): def __init__(self): super(Decoder,self).__init__() self.fc1 = nn.Linear(hidden_size, 784) self.sigmoid = nn.Sigmoid() def forward(self,x): out = self.fc1(x) out = self.sigmoid(out) out = out.view(batch_size,28,28,1) return out encoder = Encoder().cuda() decoder = Decoder().cuda() . 6) Loss function &amp; Optimizer . Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014 | https://arxiv.org/abs/1312.6114 | . $$ tilde{ mathcal{L}}^B( theta, phi; X^{(i)}) = -D_{KL}(q_{ phi}(Z mid X^{(i)}) mid mid p_{ theta}(Z)) + frac{1}{L} sum_{l=1}^L ( log p_{ theta}(x^{(i)}) mid z^{(i, l)}) text{ where } Z^{(i, l)} = g_{ phi}( epsilon^{(i, l)}, X^{(i)}), epsilon^{(l)} sim p( epsilon) $$From appendix B $$ -D_{KL}(q_{ phi}(Z mid X^{(i)}) mid mid p_{ theta}(Z)) = frac{1}{2} sum_{j=1}^J(1 + log( sigma_j^2) - u_j^2 - sigma_j^2) $$ . reconstruction_function = nn.BCELoss(size_average=False) # 각 element들의 BCE loss를 모두 sum해야 kld scale과 일치한다. def loss_function(recon_x, x, mu, logvar): BCE = reconstruction_function(recon_x, x) # reconstruction error # see Appendix B from VAE paper: # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014 # https://arxiv.org/abs/1312.6114 # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2) KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar) KLD = torch.sum(KLD_element).mul_(-0.5) # regularizer term return BCE + KLD parameters = list(encoder.parameters())+ list(decoder.parameters()) optimizer = torch.optim.Adam(parameters, lr=learning_rate) . /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction=&#39;sum&#39; instead. warnings.warn(warning.format(ret)) . 7) Train . try: encoder, decoder = torch.load(&#39;./model/variational_autoencoder.pkl&#39;) print(&quot; n--model restored-- n&quot;) except: print(&quot; n--model not restored-- n&quot;) pass res = [] for i in range(num_epoch): cum_loss = 0 for j,[image,label] in enumerate(train_loader): optimizer.zero_grad() image = Variable(image).cuda() mu,log_var,reparam = encoder(image) output = decoder(reparam) loss = loss_function(output, image, mu, log_var) cum_loss += loss loss.backward() optimizer.step() res.append(cum_loss / len(train_loader)) # Train graph plt.plot(res) plt.show() . --model not restored-- . /usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([128, 1, 28, 28])) that is different to the input size (torch.Size([128, 28, 28, 1])) is deprecated. Please ensure they have the same size. return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction) . 8) Check with Train Image . 아래의 이미지를 통해서 encoding된 train image가 decoding된 것을 확인할 수 있다. . out_img = torch.squeeze(output.cpu().data) print(out_img.size()) for i in range(5): plt.imshow(out_img[i].numpy(),cmap=&#39;gray&#39;) plt.show() . torch.Size([128, 28, 28]) . 9) Check with Test image . for i in range(1): for j,[image,label] in enumerate(test_loader): image = Variable(image,volatile=True).cuda() output,mean,var = encoder(image) output = decoder(output) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead. This is separate from the ipykernel package so we can avoid doing imports until . out_img = torch.squeeze(output.cpu().data) print(out_img.size()) for i in range(5): plt.imshow(torch.squeeze(image[i].cpu()).data.numpy(),cmap=&#39;gray&#39;) plt.show() plt.imshow(out_img[i].numpy(),cmap=&#39;gray&#39;) plt.show() . torch.Size([128, 28, 28]) .",
            "url": "https://fastpages.fast.ai/vae/deeplearning/2020/03/25/VAE.html",
            "relUrl": "/vae/deeplearning/2020/03/25/VAE.html",
            "date": " • Mar 25, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Missing Semester 정리글",
            "content": "Missing Semester 정리글 . 1. Shell . 리눅스의 쉘은 명령어와 프로그램을 실행할 때 사용하는 인터페이스이다. . . echo . 리눅스 명령어 echo는 주어진 문자열을, 문자열 사이에 포함된 공백과 줄 마지막에 개행문자를 포함하여 표준출력으로 출력하는 명령어다. . echo &quot;Hellow Wolrd&quot; &gt;&gt;&gt; Hellow World echo Hellow Wolrd &gt;&gt;&gt; Hellow World echo $HOME &gt;&gt;&gt; /root echo $PATH &gt;&gt;&gt; $PATH에 포함된 주소들을 출력 which echo &gt;&gt;&gt; /usr/bin/echo # 사용하고 있는 echo 위치 출력 . pwd . pwd &gt;&gt;&gt; /home # present working directory . cd . cd /roundtable # change directory . . (dot) / .. (dot dot) . .. # parent directory cd .. # /home/roundtable -&gt; /home . # current directory cd ./roundtable # /home -&gt; /home/roundtable . ls . ls &gt;&gt;&gt; print all files in the current directory ls -help &gt;&gt;&gt; 다양한 옵션에 대한 정보를 받아볼 수 있음 ls -l &gt;&gt;&gt; 파일에 대한 추가적인 정보를 얻을 수 있음 . ~ . cd ~ # home directory cd ~/roundtable # cd /home/roundtable . cd - . cd - # back to parent directory . mv . mv &lt;current_file&gt; &lt;new_file&gt; # rename the file or move the file in dirrent directory . cp . cp &lt;current_file&gt; &lt;new_file&gt; # copy cp -r &lt;current folder&gt; &lt;new_folder&gt; . rm . rm &lt;current_file&gt; # remove rmdir &lt;folder&gt; . mkdir . mkdir &lt;new directory name&gt; # make directory . man . man ls # manual page for ls # if you want to quik, press q . ctrl + l: clear the terminal . Angle bracket signs . input stream, output stream이 존재한다. 이를 적절히 조절할 수 있다. . # &lt; file # &gt; file echo hello &gt; hello.txt # hello(print)가 hello.txt의 입력으로 들어간다. cat hello.txt &gt;&gt;&gt; hello cat &lt; hello.txt &gt; hello2.txt cat hello2.txt &gt;&gt;&gt; hello . ** | (pipe)** | . # file1 | file2 # make output of file1 input of file2 # tail # 마지막 line만 출력해준다. ls -l / tail -n1 &gt;&gt;&gt; drwxrwxr-x 11 ubuntu ubuntu 4096 Mar 4 12:55 dev # print last line curl --head --silent google.com | grep -i content-length . tail . 마지막 line만 출력해준다. . curl . curl --head --silent google.com | grep -i content-length . sudo: root permission . sudo find -L /sys/class/backlight -maxdepth 2 -name &#39;*brightness*&#39; &gt;&gt;&gt; /sys/class/backlight/thinkpad_screen/brightness cd /sys/class/backlight/thinkpad_screen sudo echo 3 &gt; brightness &gt;&gt;&gt; An error occurred while redirecting file &#39;brightness&#39; open: Permission denied echo 3 | sudo tee brightness . cat: concatenate . 파일의 내용을 출력 . cat file1 # file1의 내용 출력 cat file1 file2 file3 # file1, file2, file3 이어서 출력 cat &gt; file1 # (내용을 입력하고 ctrl + d를 눌러 저장한다.) 기존 내용을 지우고 cat &gt;&gt; file1 # (내용을 입력하고 ctrl + d를 눌러 저장한다.) 기존의 내용에 이어서 cat file1 file2 &gt; file3 # file1 + file2 = file3 . cd /sys . cd /sys # to access various kernel parameters total 0 drwxr-xr-x 2 root root 0 Mar 5 08:03 block drwxr-xr-x 47 root root 0 Mar 3 06:53 bus drwxr-xr-x 69 root root 0 Mar 3 06:51 class drwxr-xr-x 4 root root 0 Mar 5 08:03 dev drwxr-xr-x 71 root root 0 Mar 3 04:43 devices drwxrwxrwt 2 root root 40 Mar 3 04:43 firmware drwxr-xr-x 12 root root 0 Mar 3 04:43 fs drwxr-xr-x 2 root root 0 Mar 5 08:03 hypervisor drwxr-xr-x 14 root root 0 Mar 5 08:03 kernel drwxr-xr-x 219 root root 0 Mar 5 08:03 module drwxr-xr-x 2 root root 0 Mar 5 08:03 power . shell은 단순한 argument가 아니라 일종의 프로그래밍이라고 볼 수 있다. 예를 들어서 조건문이나 반복문같은 설정을 할 수 있다. . 2. Shell Tools and Scripting . ’’ 하고 “” 는 유사해보이지만, 서로 다르다. . foo=bar echo &quot;$foo&quot; # prints bar echo &#39;$foo&#39; # prints $foo echo &quot;Value is $foo&quot; # prints &#39;Value is foo&#39; echo &#39;Value is $foo&#39; # prints &#39;Value is $foo&#39; . bash는 if, case, while, for와 같은 구문을 제공한다. . mcd (){ mkdir -p &quot;$1&quot; cd &quot;$1&quot; } . /home: vim mcd.sh /home: source mcd.sh /home: mcd.sh test /home/test: # /home -&gt; mkdir /home/test -&gt; cd /home/test . $0 - Name of the script | $1 to $9 - Arguments to the script. $1 is the first argument and so on. | $@ - All the arguments | $# - Number of arguments | $? - Return code of the previous command | $$ - Process Identification number for the current script | !! - Entire last command, including arguments. A common pattern is to execute a command only for it to fail due to missing permissions, then you can quickly execute it with sudo by doing sudo !! | $_ - Last argument from the last command. If you are in an interactive shell, you can also quickly get this value by typing Esc followed by . | . echo &quot;Hello&quot; &gt;&gt;&gt; Hello echo $? &gt;&gt;&gt; 0 # No Error grep foobar mcd.sh echo $? &gt;&gt;&gt; 1 # Error . # || or false || echo &quot;Oops, fail&quot; # Oops, fail true || echo &quot;Will not be printed&quot; # # &amp;&amp; and true &amp;&amp; echo &quot;Things went well&quot; # Things went well false &amp;&amp; echo &quot;Will not be printed&quot; # false ; echo &quot;This will always run&quot; # This will always run . #!/bin/bash echo &quot;Starting program at $(date)&quot; # Date will be substituted echo &quot;Running program $0 with $# arguments with pid $$&quot; for file in $@; do grep foobar $file &gt; /dev/null 2&gt; /dev/null # When pattern is not found, grep has exit status 1 # We redirect STDOUT and STDERR to a null register since we do not care about them if [[ $? -ne 0 ]]; then echo &quot;File $file does not have any foobar, adding one&quot; echo &quot;# foobar&quot; &gt;&gt; &quot;$file&quot; fi done . ? : one of character ***** : any amount of characters . convert image.{png,jpg} # Will expand to convert image.png image.jpg cp /path/to/project/{foo,bar,baz}.sh /newpath # Will expand to cp /path/to/project/foo.sh /path/to/project/bar.sh /path/to/project/baz.sh /newpath # Globbing techniques can also be combined mv *{.py,.sh} folder # Will move all *.py and *.sh files mkdir foo bar # This creates files foo/a, foo/b, ... foo/h, bar/a, bar/b, ... bar/h touch {foo,bar}/{a..j} touch foo/x bar/y # Show differences between files in foo and bar diff &lt;(ls foo) &lt;(ls bar) # Outputs # &lt; x # # &gt; y . in python . Shebang은 (사전에 검색해보면) 쉬뱅이라고 읽습니다. 쉬뱅은 #!로 시작하는 문자열이며 스크립트의 맨 첫번째 라인에 있습니다. 쉬뱅은 유닉스 계열 운영체제에서 스크립트가 실행될 때, 파이썬, 배쉬쉘 등 어떤 인터프리터에 의해서 동작이 되는지 알려줍니다. . #!/usr/local/bin/python import sys for arg in reversed(sys.argv[1:]): print(arg) . shellcheck . $ shellcheck test.sh In test.sh line 2: T0=`date +%s` ^-- SC2006: Use $(..) instead of legacy `..`. In test.sh line 4: T1=`date +%s` ^-- SC2006: Use $(..) instead of legacy `..`. In test.sh line 5: ELAPSED_TIME=$((T1-T0)) ^-- SC2034: ELAPSED_TIME appears unused. Verify it or export it. In test.sh line 7: echo &quot;START_TIME: &quot; ${T0} ^-- SC2086: Double quote to prevent globbing and word splitting. In test.sh line 8: echo &quot;END_TIME: &quot; ${T1} ^-- SC2086: Double quote to prevent globbing and word splitting. In test.sh line 9: echo &quot;ELAPSED_TIME: ${ELAPSES_TIME} sec&quot; ^-- SC2153: Possible misspelling: ELAPSES_TIME may not be assigned, but ELAPSED_TIME is. . export . 환경변수를 저장하는 역할, 터미널이 꺼지면 사라진다. . vi ~/.bashrc # 해당 주소에서 작업을 하게되면 영구적으로 남는다. export water=&quot;삼다수&quot; export TEMP_DIR=/tmp export BASE_DIR=$TEMP_DIR/backup . # gpu idx를 지정할 때 사용할 수도 있다. export CUDA_VISIBLE_DEVICES = 1 . Finding how to use commands . ls -h ls --help man ls . manpage | TLDR pages: 간단하게 찾아볼 수 있음 | . Finding files . # Find all directories named src find . -name src -type d # Find all python files that have a folder named test in their path find . -path &#39;**/test/**/*.py&#39; -type f # Find all files modified in the last day find . -mtime -1 # Find all zip files with size in range 500k to 10M find . -size +500k -size -10M -name &#39;*.tar.gz&#39; # Delete all files with .tmp extension find . -name &#39;*.tmp&#39; -exec rm {} ; # Find all PNG files and convert them to JPG find . -name &#39;*.png&#39; -exec convert {} {.}.jpg ; . Finding code . grep . grep foobar mcd.sh grep -R foobar . # source code 검색도 가능 . ripgrep . # Find all python files where I used the requests library rg -t py &#39;import requests&#39; # Find all files (including hidden files) without a shebang line rg -u --files-without-match &quot;^#!&quot; # Find all matches of foo and print the following 5 lines rg foo -A 5 # Print statistics of matches (# of matched lines and files ) rg --stats PATTERN . Finding shell commands . history . history 1 cd . OneDrive sourceCode CPPS 2 cd .. 3 cd . EEN-with-Keras . Ctrl + R : history 추적, 유용 . zsh: 유용한 bash 도구 . Directory Naviation . ls -R | tree | broot | nnn | ranger | . ls -R tree . 3. Git . Git’s data model . snapshots | . &lt;root&gt; (tree): snapshots, top-level directory | +- foo (tree) | | | + bar.txt (blob, contents = &quot;hello world&quot;) | +- baz.txt (blob, contents = &quot;git is wonderful&quot;) . Modeling history: relating snapshots | . o &lt;-- o &lt;-- o &lt;-- o ^ o &lt;-- o . with the newly created merge commit shown in bold: . o &lt;-- o &lt;-- o &lt;-- o &lt;- o ^ / v o &lt;-- o . // a file is a bunch of bytes type blob = array&lt;byte&gt; // a directory contains named files and directories type tree = map&lt;string, tree | file&gt; // a commit has parents, metadata, and the top-level tree type commit = struct { parent: array&lt;commit&gt; author: string message: string snapshot: tree } type object = blob | tree | commit # 모두 다 object다 objects = map&lt;string, object&gt; def store(object): id = sha1(object) objects[id] = object def load(id): return objects[id] . References: HEAD . references = map&lt;string, string&gt; def update_reference(name, id): references[name] = id def read_reference(name): return references[name] def load_reference(name_or_id): if name_or_id in references: return load(references[name_or_id]) else: return load(name_or_id) . Hook . 특정상황에서 특정 스크립트를 실행할 수 있도록 하는 기능 . 위치: cd ./git/hook . #!/bin/sh # git diff --exit-code --cached --name-only --diff-filter=ACM -- &#39;*.png&#39; &#39;*.jpg&#39; # 위의 명령어는 현재 add 되어있는 파일 중, .png와 .jpg 확장자를 가진 파일들을 &#39;이름만&#39; 추출합니다. images=$(git diff --exit-code --cached --name-only --diff-filter=ACM -- &#39;*.png&#39; &#39;*.jpg&#39;) # 추출된 이미지 파일들을 ImageOptimCLI에 넘겨주기만 하면 되는 것이죠! # 이미지들이 압축되어 변경되었으니 다시 add 해줘야겠죠? $(exit $?) || (echo &quot;$images&quot; | ~/.woowa/imageoptim-cli/bin/imageOptim &amp;&amp; git add $images) . pre-commit: https://pre-commit.com/ . | reference: https://woowabros.github.io/tools/2017/07/12/git_hook.html . | . Github Deployment &amp; Actions . https://blog.banksalad.com/tech/become-an-organization-that-deploys-1000-times-a-day/?fbclid=IwAR1X6CC1mz6Akrxcyt-BpeMZ-ZnpLOvdGlK7dvxh0De85D1qsEoLN2JEhAU . Git command-line interface . git help &lt;command&gt;: get help for a git command git init: creates a new git repo, with data stored in the .git directory git status: tells you what’s going on git add &lt;filename&gt;: adds files to staging area git commit: creates a new commit git log: shows a flattened log of history git log --all --graph --decorate: visualizes history as a DAG git diff &lt;filename&gt;: show differences since the last commit git diff &lt;revision&gt; &lt;filename&gt;: shows differences in a file between snapshots git checkout &lt;revision&gt; . Branching and merging . git branch: shows branches git branch &lt;name&gt;: creates a branch git checkout -b &lt;name&gt;: creates a branch and switches to it same as git branch &lt;name&gt;; git checkout &lt;name&gt; git merge &lt;revision&gt;: merges into current branch git mergetool: use a fancy tool to help resolve merge conflicts git rebase: rebase set of patches onto a new base . Remotes . git remote: list remotes git remote add &lt;name&gt; &lt;url&gt;: add a remote git push &lt;remote&gt; &lt;local branch&gt;:&lt;remote branch&gt;: send objects to remote, and update remote reference git branch --set-upstream-to=&lt;remote&gt;/&lt;remote branch&gt;: set up correspondence between local and remote branch git fetch: retrieve objects/references from a remote git pull: same as git fetch; git merge git clone: download repository from remote . Undo . git commit --amend: edit a commit’s contents/message git reset HEAD &lt;file&gt;: unstage a file git checkout -- &lt;file&gt;: discard changes . https://git-scm.com/book/en/v2 | . Github로 협업하기 . checkout https://mytory.net/archives/10078 . stash . 특정 커밋 선택해서 반영하기:cherry pick . git cherry-pick {Commit ID} . 여러개의 커밋을 반영하기: rebase . git rebase {가져올 Branch 이름} . https://www.tuwlab.com/ece/22218 | . Github Actions . Build . 코드 스타일 검사를 위한 lint . 유닛 테스트를 실행하는 test . docker image를 build하는 build . | Deploy . 아직은 경험하기 힘든 영역이라고 생각 . | . Gitflow . https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow | . Github slack integration . https://slack.github.com/ . | http://wepla.net/?p=2353 . | . Github Commit message . Type: 제목 본문 꼬리말 . feat: 새로운 기능을 추가할 경우 fix: 버그를 고친 경우 docs: 문서 수정한 경우 style: 코드 포맷 변경, 세미 콜론 누락, 코드 수정이 없는 경우 refactor: 프로덕션 코드 리팩터링 test: 테스트 추가, 테스트 리팩터링 (프로덕션 코드 변경 없음) chore: 빌드 테스크 업데이트, 패키지 매니저 설정할 경우 (프로덕션 코드 변경 없음) . feat: Summarize changes in around 50 characters or less More detailed explanatory text, if necessary. Wrap it to about 72 characters or so. In some contexts, the first line is treated as the subject of the commit and the rest of the text as the body. The blank line separating the summary from the body is critical (unless you omit the body entirely); various tools like `log`, `shortlog` and `rebase` can get confused if you run the two together. Explain the problem that this commit is solving. Focus on why you are making this change as opposed to how (the code explains that). Are there side effects or other unintuitive consequenses of this change? Here&#39;s the place to explain them. Further paragraphs come after blank lines. - Bullet points are okay, too - Typically a hyphen or asterisk is used for the bullet, preceded by a single space, with blank lines in between, but conventions vary here If you use an issue tracker, put references to them at the bottom, like this: Resolves: #123 See also: #456, #789 . https://sujinlee.me/professional-github/ | https://junwoo45.github.io/2020-02-06-commit_template/?fbclid=IwAR2HKgwO9imOxWAvWUPtaXDymUzMRRJ18LnwR_Cwa3s6kcrFidIwvz8CvmY | . 4. Python CI: 핑퐁팀 사례 . 코드 스타일 확인 및 포맷 자동화** . black Python Software Foundation에서 작성한 Python 자동 포맷팅 도구입니다. pycodestyle을 따른다. . | flake8 . pycodestyle + pyflakes + 복잡도 검사 기능 . Python Code Quality Authority (PyCQA)에서 작성한 스타일 체크 도구로, 플러그인을 붙이기 쉬운 것이 장점이다. docstring 형식 부분을 잡아내기 위해서 적용하기도 한다. . | yapf 구글에서 배포하는 자동 포맷팅 도구입니다. 다른 포맷팅 도구들이 스타일 가이드를 어긴 부분만 잡아준다면, yapf는 스타일 가이드를 어기지 않았더라도 다시 포맷팅을 진행하는 상당히 엄격한 자동 포맷팅 도구입니다. . | isort: import statement 정렬하는 도구 . | . Type Checker . Type Hints란, 예상치 못한 타입이 변수에 할당되는 것을 검사기가 막아주는 역할을 한다. . pyright: vscode와 연동가능 | mypy | pyre-check | . CI . CircleCI | Jenkins Blueocean | GitHub Actions | travisCI | . 테스트 코드 . pytest | unittest | . Code Coverage . pytest-cov | . 초기 템플릿 생성 . 5. Debugging and Profiling . Logging . print문 삽입하는 법 | log: 일반적으로 log가 더 좋은 방법 | . Log의 장점 . log를 사용하면 file로 저장할 수 있다. (remote server에도 가능) | Serverity level별로 나타낼 수 있다. INFO | DEBUG | WARN | ERROR | | 새로운 이슈가 추가될 때, 더 골고루 살펴볼 수 있다. | python logging 모듈 활용하기 . import logging # logger instance logger = logging.getLogger(__name__) # handler streamHandler = logging.StreamHandler() fileHandler = logging.FileHandler(&#39;./server.log&#39;) logger.addHandler(streamHandler) logger.addHandler(FileHandler) logger.setLevel(level=logging.DEBUG) logger.debug(&#39;원하는 log 문 작성하기&#39;) . https://hamait.tistory.com/880 | . Debugger . pdb . Static Analysis . shellcheck . linting . 잠재적인 오류에 대한 코드를 분석하는 프로그램. . flake8을 이용하여 진행할 수 있다. . Profiling . Timing . import time, random n = random.randint(1, 10) * 100 # Get current time start = time.time() # Do some work print(&quot;Sleeping for {} ms&quot;.format(n)) time.sleep(n/1000) # Compute time between start and now print(time.time() - start) # Output # Sleeping for 500 ms # 0.5713930130004883 . 위의 예시처럼 시간을 측정하면, 실제 시간과 차이가 나는 경우가 있다. 예를 들면, 다른 작업이 cpu를 할당 받고 있어서 그 후에 실행된 경우가 그러한 경우이다. 따라서, 해당 소스코드의 동작시간을 알고 싶다면, User가 사용한 시간 + system이 사용한 시간을 더해서 구할 수 있다. (User + Sys) . Real - Wall clock elapsed time from start to finish of the program, including the time taken by other processes and time taken while blocked (e.g. waiting for I/O or network) | User - Amount of time spent in the CPU running user code | Sys - Amount of time spent in the CPU running kernel code | . Profilers . CPU . tracing, sampling profilers 두 가지의 종류가 있다. . tracing profiler는 모든 function call을 기록하는 반면에 sampling profiler는 특정 간격마다 기록한다. . python의 경우 cProfile 모듈을 이용할 수 있다. 아래와 같은 소스코드가 있다고 가정하자. . #!/usr/bin/env python import sys, re def grep(pattern, file): with open(file, &#39;r&#39;) as f: print(file) for i, line in enumerate(f.readlines()): pattern = re.compile(pattern) match = pattern.search(line) if match is not None: print(&quot;{}: {}&quot;.format(i, line), end=&quot;&quot;) if __name__ == &#39;__main__&#39;: times = int(sys.argv[1]) pattern = sys.argv[2] for i in range(times): for file in sys.argv[3:]: grep(pattern, file) . 아래와 같이 프로파일링을 진행할 수 있다. 모든 function call을 확인할 수 있다. . $ python -m cProfile -s tottime grep.py 1000 &#39;^(import| s*def)[^,]*$&#39; *.py [omitted program output] ncalls tottime percall cumtime percall filename:lineno(function) 8000 0.266 0.000 0.292 0.000 {built-in method io.open} 8000 0.153 0.000 0.894 0.000 grep.py:5(grep) 17000 0.101 0.000 0.101 0.000 {built-in method builtins.print} 8000 0.100 0.000 0.129 0.000 {method &#39;readlines&#39; of &#39;_io._IOBase&#39; objects} 93000 0.097 0.000 0.111 0.000 re.py:286(_compile) 93000 0.069 0.000 0.069 0.000 {method &#39;search&#39; of &#39;_sre.SRE_Pattern&#39; objects} 93000 0.030 0.000 0.141 0.000 re.py:231(compile) 17000 0.019 0.000 0.029 0.000 codecs.py:318(decode) 1 0.017 0.017 0.911 0.911 grep.py:3(&lt;module&gt;) [omitted lines] . line마다 profiling 하고 싶다면, kernprof를 사용할 수 있다. 단, 데코레이터를 사용해야한다. . #!/usr/bin/env python import requests from bs4 import BeautifulSoup # This is a decorator that tells line_profiler # that we want to analyze this function @profile def get_urls(): response = requests.get(&#39;https://missing.csail.mit.edu&#39;) s = BeautifulSoup(response.content, &#39;lxml&#39;) urls = [] for url in s.find_all(&#39;a&#39;): urls.append(url[&#39;href&#39;]) if __name__ == &#39;__main__&#39;: get_urls() . $ kernprof -l -v a.py Wrote profile results to urls.py.lprof Timer unit: 1e-06 s Total time: 0.636188 s File: a.py Function: get_urls at line 5 Line # Hits Time Per Hit % Time Line Contents ============================================================== 5 @profile 6 def get_urls(): 7 1 613909.0 613909.0 96.5 response = requests.get(&#39;https://missing.csail.mit.edu&#39;) 8 1 21559.0 21559.0 3.4 s = BeautifulSoup(response.content, &#39;lxml&#39;) 9 1 2.0 2.0 0.0 urls = [] 10 25 685.0 27.4 0.1 for url in s.find_all(&#39;a&#39;): 11 24 33.0 1.4 0.0 urls.append(url[&#39;href&#39;]) . Memory . python을 사용할 시에, 데코레이터를 사용한 후, memory_profiler를 사용하면, 메모리 사용량을 검사할 수 있다. . @profile def my_func(): a = [1] * (10 ** 6) b = [2] * (2 * 10 ** 7) del b return a if __name__ == &#39;__main__&#39;: my_func() . $ python -m memory_profiler example.py Line # Mem usage Increment Line Contents ============================================== 3 @profile 4 5.97 MB 0.00 MB def my_func(): 5 13.61 MB 7.64 MB a = [1] * (10 ** 6) 6 166.20 MB 152.59 MB b = [2] * (2 * 10 ** 7) 7 13.61 MB -152.59 MB del b 8 13.61 MB 0.00 MB return a . 파이썬에서 데코레이터란 . https://wikidocs.net/23106 . Event Profiling . strace | perf | . Visualization . Call graphs: python - pycallgraph | Flame Graph | . Resource Monitoring . General Monitoring - Probably the most popular is htop, which is an improved version of top. htop presents various statistics for the currently running processes on the system. htop has a myriad of options and keybinds, some useful ones are: `` to sort processes, t to show tree hierarchy and h to toggle threads. See also glances for similar implementation with a great UI. For getting aggregate measures across all processes, dstat is another nifty tool that computes real-time resource metrics for lots of different subsystems like I/O, networking, CPU utilization, context switches, &amp;c. | I/O operations - iotop displays live I/O usage information and is handy to check if a process is doing heavy I/O disk operations | Disk Usage - df displays metrics per partitions and du displays disk usage per file for the current directory. In these tools the -h flag tells the program to print with human readable format. A more interactive version of du is ncdu which lets you navigate folders and delete files and folders as you navigate. | Memory Usage - free displays the total amount of free and used memory in the system. Memory is also displayed in tools like htop. | Open Files - lsof lists file information about files opened by processes. It can be quite useful for checking which process has opened a specific file. | Network Connections and Config - ss lets you monitor incoming and outgoing network packets statistics as well as interface statistics. A common use case of ss is figuring out what process is using a given port in a machine. For displaying routing, network devices and interfaces you can use ip. Note that netstat and ifconfig have been deprecated in favor of the former tools respectively. | Network Usage - nethogs and iftop are good interactive CLI tools for monitoring network usage. | . 6. Metaprogramming . paper | source code | tool | dependency | . Build systems . make . ubuntu에서 make 명령어는 파일 관리 유틸리티이다. make 명령어를 실행하는 위치에 Makefile이 있어야 한다. . Makefile은 다음과 같은 구조를 가진다. . Target: 명령어가 실행되어 나온 결과를 저장할 파일 | Dependency: Target을 만들기 위해 필요한 재료 | Command: 실행되어야 할 명령어들 | macro: 코드를 단순화 시키기 위한 방법 | . CC=&lt;컴파일러&gt; CFLAGS=&lt;컴파일 옵션&gt; LDFLAGS=&lt;링크 옵션&gt; LDLIBS=&lt;링크 라이브러리 목록&gt; OBJS=&lt;Object 파일 목록&gt; TARGET=&lt;빌드 대상 이름&gt; all: $(TARGET) clean: rm -f *.o rm -f $(TARGET) $(TARGET): $(OBJS) $(CC) -o $@ $(OBJS) . 빌드규칙 블록 . &lt;Target&gt;: &lt;Dependencies&gt; &lt;Recipe&gt; . Target: 빌드 대상 이름. 통상 이 Rule에서 최종적으로 생성해내는 파일명을 써 줍니다. | Dependencies: 빌드 대상이 의존하는 Target이나 파일 목록. 여기에 나열된 대상들을 먼저 만들고 빌드 대상을 생성합니다. | Recipe: 빌드 대상을 생성하는 명령. 여러 줄로 작성할 수 있으며, 각 줄 시작에 반드시 Tab문자로 된 Indent가 있어야 합니다. | . 예시 . paper.pdf: paper.tex plot-data.png pdflatex paper.tex plot-%.png: %.dat plot.py ./plot.py -i $*.dat -o $@ # %: pattern # $@: current target # reference: http://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html . 자세한 정보를 알고 싶다면, https://www.tuwlab.com/ece/27193 | . Dependency management . semantic versioning . Continuous integration systems . Github Actions | Travis CI | Pipelines | . A brief aside on testing . test suite | unit test | intergration test | regression test | Mocking . | Reference: https://missing.csail.mit.edu/2020/?fbclid=IwAR2gQe5LToKuqVUwbfegqSOk6BnIqscbnqjK0e3js64EceMswNqW0KgeSEo | .",
            "url": "https://fastpages.fast.ai/shell/bash/2020/03/05/missing-semeste-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "relUrl": "/shell/bash/2020/03/05/missing-semeste-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "date": " • Mar 5, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://fastpages.fast.ai/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "늦은 2019 회고",
            "content": "1. 2019년 계획했던 일 . 2019년은 학생에서 사회인으로 나아가는 경계였다. 2019년에는 특히 좋은 사람들을 많이 만났고, 개인적으로 많은 도움을 받은 해라고 생각한다. . (1) 딥러닝 논문 구현: 목표 달성 . 지금도 어렵지만, 처음에 딥러닝 논문을 구현하려고 시도했을때는 정말 큰 벽처럼 느껴졌다. 이전에는 딥러닝 구현체를 주로 가져다 쓰는 식으로 공부를 지속하였고, 돌이켜보면 전혀 성장하지 못했던 시기이다. . 처음 딥러닝 논문을 구현하기로 마음먹은 것은 모두의 연구소 SafeAI랩에서 좋은 사람들을 만나서이다. 그 당시에는 구현하려는 주제가 명확했고, 누군가에게 피드백을 받을 환경이 조성되어 있어서 조금씩 성장했다. . 처음에는 tensorflow code를 keras code로 옮겨서 구현하는 연습을 하였다. . | 논문의 결과를 조금 더 명확하게 보여주기 위해서 시각화에 힘을 쓰기도 하였다. . | 그리고, 논문의 처음부터 끝까지 오로지 나의 코드로 채우기도 하였다. 저자분이 한국분이셔서 커뮤니티에서 도움을 주기도 하였다. . | . (2) 코딩 테스트 합격하기 . 처음에는 딥러닝 엔지니어 직군이 코딩테스트를 보는지 몰랐다. 그래서 괜찮은 기회들을 놓치기도 했다. 코딩 테스트에 합격하기 위해서 Codility 를 활용하였지만, 나의 실력은 제자리 걸음이였다. 지금 생각해보면, 잘못된 방식으로 알고리즘을 대하고 있었다. . 결국은 2019년에 코딩 테스트에서는 모두 좋은 결과를 얻지 못했다. . (3) 딥러닝 엔지니어로 인턴 경험 쌓기 . 수료 후에 인턴을 해야겠다는 계획을 가지고 있었다. 평소에 관심을 가지고 있던 의료기업들에 이메일 혹은 다른 매체로 지원서를 작성하였다. 한 기업에 면접을 보게 되었고, 긍정적인 인상을 가지고 회사에 입사하게 되었다. . 처음하는 사회생활이였다. . 이런 점이 인상깊었다. . 실제 인공지능 기술이 상업화되가는 과정을 지켜볼 수 있었다. . | 내가 이전에 놓치고 있던 개념들에 대해서 깨달음을 얻을 수 있었다. . | 능력있는 동료들 . | 나에게 맡겨지는 책임과 권한 . | 하지만, 나에게 버거웠던 것은 . 명확한 경쟁업체의 존재와 시장에 빠른 진입이 요구되었다. | 경험해보지 않은 도메인에 대한 빠른 적응이 요구되었다. | 문서체계 및 코드체계가 정해진 룰이 없었다. | 처음하는 일이라서 잘하고 싶었던 마음이 더 컸다. 요구사항을 충족시키기 위해서 자신을 몰아세우고 있는 ‘나’를 발견하게 되었다. . 하지만, 다행스럽게도 주변에 조언을 구할 사람이 있었다. (감사합니다.) 혼자 생각하기보다 조언을 구하고 심정을 공유했던 것이 악순환을 벗어나는데 큰 도움이 되었다. . 돌이켜보니, . 일하면서 고민하고 고뇌했던 과정이 나에게 많은 영감을 주었다. 특히, 모델의 성능을 끌어올리기 위해서 정말 많은 고민과 스트레스를 받았는데, 그 과정에서 얻은 교훈은 나에게 많은 시도를 할 수 있게 해준 환경도 한 몫 했다고 생각한다. 또한 스트레스에 대해서 더 진지하게 생각해보는 계기가 되었다. . 2. 2019년 계획하지 않았던 일 . (1) 풀잎스쿨 CPPS . 기업에서 퇴사하기 약 1 ~ 2주전에 CPPS[competitive programming problem solving]을 시작하게 되었다. leetcode에서 일주일에 다섯문제를 풀고 발표하는 형식으로 이루어졌다. 이 모임에 참여하기 전에는 다른 누군가에게 발표하는 과정의 필요성에 대해서 못느꼈다. 그리고 다른사람들의 중요성도 못 느꼈던 거 같다. . 하지만, 다른 사람들의 생각을 듣게 되면서, 새로운 방향성에 대해서 접하게 되었다. 또한, 내가 처음에 발표를 하는데 내용이 정리가 안되니까, 당황스러울 정도로 매끄럽지 못하게 발표하게 된적도 있다. 이런 경험을 한 후에는 앞에 나가기전에 잠깐 생각을 정리하기도 하고, discuss에서 다른 사람들이 어떤 생각으로 문제를 풀이하는지 어떤 것을 더 배울 수 있는지에 대한 고찰을 많이 하였다. . 혼자하는 것보다 여럿이 하니까, 더 깊은 생각과 생각을 정리할 수 있는 시간을 가질 수 있어서 좋았다. . (2) 성남시 이노베이션 해커톤 . 퇴사 후에 해커톤에 참여하게 되었다. 좋은 분들과 함께 할 수 있어서 정말 재밌었다. 물론 그 당시에는 잠을 못자는게 힘들었다. . (3) 의료인공지능 기업 면접 . 의료 도메인 인턴을 한 후, 헤드 헌터 업체를 통해서 Job description이 가끔 왔다. 그 중 하나의 기업에 면접을 보게 되었고, 나름 면접을 원할하게 볼 수 있었다. 하지만, 면접을 진행하면서 알게된 것은, 이전 기업과 너무나 유사한 사업분야였고, 가서 할 일도 비슷해보였다. 오퍼는 받았지만, 장기적으로는 기업과 나의 방향성이 일치하지 않을 것이 분명하여 죄송한 마음으로 거절하게 되었다. . 3. 2019년을 보내고 난 후 . 언제나처럼 시간이 너무나 빨랐다. 몇 가지 얻은 교훈이 있다. . 성장을 하는데 중요한 자세는 어제보다 나은 오늘이다. . 같은 실수를 반복하고 있지 않는지 돌이켜 볼 필요가 있다. . | 멀리가고 싶으면 함께 가는 것이 좋겠다. . 아직도 여럿이 무엇을 하는데 익숙하지는 않으나, 노력중이다. . | 신체와 정신 모두 케어 받아야 한다. . 내가 감당할 수 있는 스트레스 영역에 대한 이해가 필요하다. 불행하다고 느끼지 않는 선에서 노력하고 있다. . | . 4. 2020년을 맞이하며 . 아직은 어떤 기술 스텍을 쌓아야 할지 명확하게 모르겠다. . 하지만, 점점 더 소프트웨어 개발자라는 직업이 매력적으로 느껴진다. 배움의 두려움을 이겨낼 수 있는 개발자가 되고 싶다. . 이런 기술 스텍들은 기본적으로 가져가고 싶다. . 자료구조/알고리즘, 문제해결능력 | 운영체제 | 컴퓨터 네트워크 | NLP, VISION의 중요한 개념들 | .",
            "url": "https://fastpages.fast.ai/2020/01/22/%EB%8A%A6%EC%9D%80-2019-%ED%9A%8C%EA%B3%A0.html",
            "relUrl": "/2020/01/22/%EB%8A%A6%EC%9D%80-2019-%ED%9A%8C%EA%B3%A0.html",
            "date": " • Jan 22, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "word representation",
            "content": "Word Representation 정리글 . Word2vec: distributed representation . 분산 표현(distributed representation) 방법은 기본적으로 분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법입니다. 이 가정은 ‘비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다’라는 가정입니다. . reference: https://wikidocs.net/22660 . 1. CBOW(Continuous Bag of Words) . 주변에 있는 단어로 중간에 있는 단어를 예측하는 방법입니다. 중심단어를 예측하기 위해서 주변단어를 보는 범위를 window라고 합니다. 예를 들어서, 아래 이미지의 첫 번째 글에서 파란색 부분이 ‘fat’, ‘cat’이므로 window는 2입니다. . . 보통 딥 러닝이라함은, 입력층과 출력층 사이의 은닉층의 개수가 충분히 쌓인 신경망을 학습할 때를 말하는데 Word2Vec는 입력층과 출력층 사이에 하나의 은닉층만이 존재합니다. 이렇게 은닉층(hidden Layer)이 1개인 경우에는 일반적으로 심층신경망(Deep Neural Network)이 아니라 얕은신경망(Shallow Neural Network)이라고 부릅니다. 또한 Word2Vec의 은닉층은 일반적인 은닉층과는 달리 활성화 함수가 존재하지 않으며 룩업 테이블이라는 연산을 담당하는 층으로 일반적인 은닉층과 구분하기 위해 투사층(projection layer)이라고 부르기도 합니다. . reference: https://wikidocs.net/22660 . . 아래의 이미지를 보면 연산과정을 알 수 있다. . $x_{cat}$: word one-hot vector | $W_{V * M}$: input layer와 prejection layer사이의 matrix | $W’_{M* V}$: projection layer와 output layer간의 matrix | $V, M$: 단어의 개수, embedding 차원 | . . . loss function으로는 cross-entropy 함수를 사용하게 됩니다. . 이제 역전파(Back Propagation)를 수행하면 W와 W’가 학습이 되는데, 학습이 다 되었다면 M차원의 크기를 갖는 W의 행이나 W’의 열로부터 어떤 것을 임베딩 벡터로 사용할지를 결정하면 됩니다. 때로는 W와 W’의 평균치를 가지고 임베딩 벡터를 선택하기도 합니다. . 2. Skip-Gram . 중간에 있는 단어로 주변의 단어들을 예측하는 방법입니다. . . 3. Negative Sampling . 대부분 word2vec은 negative sampling을 함께 사용합니다. word2vec의 학습과정을 잘 살펴보면, 출력층에 있는 softmax 함수는 단어집합 크기의 vector내의 모든 값을 0과 1사이의 값이면서 모두 더하면 1이 되도록 바꾸는 작업을 수행합니다. 이는 그 단어가 주변단어와 전혀 상관이 없는 단어라도 똑같이 적용되는 부분입니다. . 만약 마지막 단계에서 ‘강아지’와 ‘고양이’와 같은 단어에 집중하고 있다면, Word2Vec은 사실 ‘돈가스’나 ‘컴퓨터’와 같은 연관 관계가 없는 수많은 단어의 임베딩을 조정할 필요가 없습니다. 전체 단어집합이 아니라 일부 단어집합에 대해서만 고려해도 되지 않을까요? . ‘강아지’, ‘고양이’, ‘애교’와 같은 주변 단어들을 가져옵니다. 그리고 여기에 ‘돈가스’, ‘컴퓨터’, ‘회의실’과 같은 랜덤으로 선택된 주변 단어가 아닌 상관없는 단어들을 일부만 갖고옵니다. 이렇게 전체 단어 집합보다 훨씬 작은 단어 집합을 만들어놓고 마지막 단계를 이진 분류 문제로 바꿔버리는 겁니다. 즉, Word2Vec은 주변 단어들을 긍정(positive)으로 두고 랜덤으로 샘플링 된 단어들을 부정(negative)으로 둔 다음에 이진 분류 문제를 수행합니다. . reference: https://wikidocs.net/22660 . 이는 기존의 다중 클래스 분류 문제를 이진 분류 문제로 바꾸면서도 연산량에 있어서 훨씬 효율적입니다. . negative sampling은 word2vec을 만들때 현재 문장에 없는 단어를 전체 데이터셋에서 추출하는 방법이다. 목적 단어와 연관성이 없을 것이라고 추정되는 단어를 추출한다. . 자주 뽑히는 단어일 수록 연관성이 낮다고 본다. 흔한 단어일수록 목적 단어와의 관계는 강하지 않기 때문 | . 아래는 한 단어 $w_i$가 negative sample로 뽑힐 확률이다. . $f(w_i)$ : $w_i$의 등장빈도 | . P(wi)=f(wi)0.75∑j=0n(f(wj)0.75)P(w_i) = frac{f(w_i)^{0.75}}{ sum_{j=0}^n (f(w_j)^{0.75})}P(wi​)=∑j=0n​(f(wj​)0.75)f(wi​)0.75​ . word2vec의 한계점 . out of vocalbulary에 대해서는 word representation을 얻을 수 없다. | infrequent words는 학습이 불안정하다. | . Glove . 카운트 기반과 예측기반을 모두 사용하는 방법론입니다. . LSA: 카운트 기반, 단어의미 유추 성능이 떨어짐. . | Word2Vec: 예측기반, 전체적인 통계정보를 반영하지 못함. . | . 단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들로 구성하고, window size내에서 k단어가 등장한 횟수를 기록하는 행렬을 말합니다. 이 행렬은 transpose를 해도 동일합니다. (대칭행렬) . 예시 . I like deep learning . I like NLP . I enjoy flying . 카운트 I like enjoy deep learning NLP flying . I | 0 | 2 | 1 | 0 | 0 | 0 | 0 | . like | 2 | 0 | 0 | 1 | 0 | 1 | 0 | . enjoy | 1 | 0 | 0 | 0 | 0 | 0 | 1 | . deep | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . learning | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . NLP | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . flying | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . Co-occurrence probability . 동시 등장 확률 $P(k | i), P(k | i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률입니다. | . 예를 들어서, 위의 동시등장행렬을 바탕으로 P(‘I’|’like’)를 구해보자. ‘I’ 는 총 3번 등장하였다. ‘I’와 ‘like’가 동시에 등장한 횟수는 2번이다. 따라서 P(‘I’|’like’) 는 2/3이다. P(A∣B)=P(A)∩P(B)P(B)P(A|B) = frac{P(A) cap P(B)}{P(B)}P(A∣B)=P(B)P(A)∩P(B)​ . 동시 등장 확률과 크기 관계 비(ratio) k=solid k=gas k=water k=fasion . P(k l ice) | 0.00019 | 0.000066 | 0.003 | 0.000017 | . P(k l steam) | 0.000022 | 0.00078 | 0.0022 | 0.000018 | . P(k l ice) / P(k l steam) | 8.9 | 0.085 | 1.36 | 0.96 | . 위의 표를 통해 알 수 있는 사실은 solid가 등장했을 때 ice가 등장할 확률 0.00019은 solid가 등장했을 때 steam이 등장할 확률인 0.000022보다 약 8.9배 크다는 겁니다. 그도 그럴 것이 solid는 ‘단단한’이라는 의미를 가졌으니까 ‘증기’라는 의미를 가지는 steam보다는 당연히 ‘얼음’이라는 의미를 가지는 ice라는 단어와 더 자주 등장할 겁니다. . Loss function . Embedding 된 중심단어와 주변 단어 벡터의 내적이 전체 코퍼스에서 동시 등장 확률이 되도록 모델을 학습시킨다. mboxdocproduct(Wi,Wk)≈P(k∣i)=Pik mbox{doc product}(W_i, W_k) approx P(k|i) = P_{ik} mboxdocproduct(Wi​,Wk​)≈P(k∣i)=Pik​ . 실제 학습에서는 아래의 식을 활용한다. . mboxdocproduct(Wi,Wk)≈log⁡P(k∣i)=log⁡Pik mbox{doc product}(W_i, W_k) approx log P(k|i) = log P_{ik} mboxdocproduct(Wi​,Wk​)≈logP(k∣i)=logPik​ . $X$ : 동시 등장 행렬(Co-occurrence Matrix) | $X_{ij}$ : 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 j가 등장하는 횟수 | $X_i$:$ sum_{j}X_{ij}$: 동시 등장 행렬에서 i행의 값을 모두 더한 값 | $P_{ik}$ :$P(k|i)$ = $ frac{X_{ik}}{X_i}$ : 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 k가 등장할 확률 Ex) P(solid l ice) = 단어 ice가 등장했을 때 단어 solid가 등장할 확률 | $ frac{P_{ik}}{P_{jk}}$ : $P_{ik}$를 $P_{jk}$로 나눠준 값 Ex) P(solid l ice) / P(solid l steam) = 8.9 | $W_i$ : 중심 단어 i의 임베딩 벡터 | $W_k$ : 주변 단어 k의 임베딩 벡터 | . embedding vector의 목적은 단어간의 관계를 잘 표현하는데 있습니다. 위에서 살펴본 $ frac{P_{ik}}{P_{jk}}$ 를 목적함수에 사용합니다. 먼저 함수의 input과 output을 정의해봅니다. F(Wi,Wj,Wk)=PikPjkF(W_i, W_j, W_k) = frac{P_{ik}}{P_{jk}}F(Wi​,Wj​,Wk​)=Pjk​Pik​​ . input에서 output을 도출하는 방법은 많겠지만, 해당 연구에서는 두 단어간의 차이를 input으로 넣는 것을 제안합니다. . F(Wi−Wj,Wk)=PikPjkF(W_i - W_j, W_k) = frac{P_{ik}}{P_{jk}}F(Wi​−Wj​,Wk​)=Pjk​Pik​​ . 그리고 선형 공간에서 두 vector간의 유사도를 보기 위해서 dot product를 선택했습니다. F((Wi−Wj)TWk)=PikPjkF((W_i -W_j) ^ T W_k) = frac{P_{ik}}{P_{jk}}F((Wi​−Wj​)TWk​)=Pjk​Pik​​ . 정리하자면 선형공간에서 단어의 의미 관계를 표현하기 위해 뺄셈과 내적(dot procut)를 활용했습니다. . 여기서 함수 F는 중심단어와 주변단어의 선택기준이 무작위이기 때문에, 이 둘의 관계는 함수 F안에서 자유롭게 교환가능해야 합니다. 이 조건을 만족하기 위해서는 Homomorphism이라는 조건을 만족해야합니다. F(a+b)=F(a)F(b) ∀a,b∈RF(a+b) = F(a)F(b) forall a, b in RF(a+b)=F(a)F(b) ∀a,b∈R . In algebra, a homomorphism is a structure-preserving map between two algebraic structures of the same type (such as two groups, two rings, or two vector spaces). . Homomorphism의 조건하에서 a와 b가 각각 vector라면 scalar값이 나올 수 없지만 내적값이라고 하면 scalar값이 나올 수 있습니다. . v1, v2, v3, v4 모두 vector입니다. F(v1Tv2+v3Tv4)=F(v1Tv2)F(v3Tv4)  ∀v1,v2,v3,v4∈VF(v1^T v2 + v3^Tv4) = F(v1^T v2) F(v3^Tv4) forall v1, v2, v3, v4 in VF(v1Tv2+v3Tv4)=F(v1Tv2)F(v3Tv4)  ∀v1,v2,v3,v4∈V F는 두 vector의 차이를 받았기 때문에, 뺄셈에 대한 homomorphism으로 변경했습니다. . (간단하게 덧셈 ~ 곱셈 관계를 뺄셈 ~ 나누기 관계로 치환하였습니다.) F(v1Tv2−v3Tv4)=F(v1Tv2)F(v3Tv4)  ∀v1,v2,v3,v4∈VF(v1^T v2 - v3^Tv4) = frac{F(v1^T v2)}{F(v3^Tv4)} forall v1, v2, v3, v4 in VF(v1Tv2−v3Tv4)=F(v3Tv4)F(v1Tv2)​  ∀v1,v2,v3,v4∈V 이제 glove식에 적용해보겠습니다. F((wi−wj)Twk)=F(wiTwk)F(wjTwk)=PikPjkF((w_i - w_j)^Tw_k) = frac{F(w_i^T w_k)}{F(w_j^T w_k)}= frac{P_{ik}}{P_{jk}}F((wi​−wj​)Twk​)=F(wjT​wk​)F(wiT​wk​)​=Pjk​Pik​​ 위의 식에서 조금 더 자세히 살펴보면, F(wiTwk)=Pik=XikXiF(w_i^Tw_k) = P_{ik} = frac{X_{ik}}{X_i}F(wiT​wk​)=Pik​=Xi​Xik​​ 또한 좌변을 풀어쓰면, F((wi−wj)Twk)=F(wiTwk−wjTwk)=F(wiTwk)F(wjTwk)F((w_i - w_j)^Tw_k) = F(w_i^T w_k - w_j^T w_k) = frac{F(w_i^T w_k)}{F(w_j^T w_k)}F((wi​−wj​)Twk​)=F(wiT​wk​−wjT​wk​)=F(wjT​wk​)F(wiT​wk​)​ 따라서 homomorphism을 형태와 일치하게 됩니다. . 그리고 이러한 조건을 만족시키는 함수는 지수 함수(Exponential function)입니다. . . $x^{a + b} = x ^ a * x ^ b$ | $x^{a - b} = x ^ a / x ^ b$ | . 이제 F를 $ exp$라고 해봅시다. exp⁡(wiTwk−wjTwk)=exp⁡(wiTwk)exp⁡(wjTwk) exp(w_i^T w_k - w_j^T w_k) = frac{ exp(w_i^T w_k)}{ exp(w_j^T w_k)}exp(wiT​wk​−wjT​wk​)=exp(wjT​wk​)exp(wiT​wk​)​ . 학습의 안정성을 위해서, log를 사용합니다. . wiTwk=log⁡Pik=log⁡(XikXi)=log⁡Xik−log⁡Xiw_i^T w_k = log P_{ik}= log ( frac{X_{ik}}{X_i})= log X_{ik} - log X_iwiT​wk​=logPik​=log(Xi​Xik​​)=logXik​−logXi​ . 위의 식은 homomorphism이 성립해야하지만, $ log X_i$ 때문에 성립하지 않게 됩니다. ($a - b ne b- a$)그래서 glove 연구팀은 $ log X_i$항을 bias $b_i$라는 상수항으로 대체합니다. 같은 이유로 $w_k$에 대한 bias $b_k$를 추가합니다. wiTwk+bi+bk=log⁡Xikw_i^T w_k + b_i + b_k = log X_{ik}wiT​wk​+bi​+bk​=logXik​ . mboxlossfunction=∑m,n=1V(wmTwn+bm+bn−log⁡Xmn)2 mbox{loss function} = sum_{m,n = 1}^V (w_m ^ T w_n + b_m + b_n - log X_{mn})^2 mboxlossfunction=m,n=1∑V​(wmT​wn​+bm​+bn​−logXmn​)2 . $V$ : 단어집합의 크기 | $X_{ik}$ 이 0이 될수도 있으므로 $ log(1+X_{ik})$로 바꾼다. | . 또한 동시등장행렬이 희소행렬일 가능성이 높다. glove 연구진은 동시 등장 행렬에서 등장 빈도의 값 $X_{ik}$가 매우 낮은 경우에는 정보가 거의 도움이 되지 않는다고 판단합니다. 따라서 동시등장행렬을 바탕으로 가중치함수를 구상하게 됩니다. . 가중치 함수의 그래프는 아래와 같습니다. 특정 값보다 크다면 모두 같은 가중치를 주게 됩니다. . f(x)=min⁡(1,(xxmax⁡)0.75)f(x) = min(1, ( frac{x}{x_{ max}})^{0.75})f(x)=min(1,(xmax​x​)0.75) 최종적으로 목적함수는 아래의 식과 같습니다. mboxlossfunction=∑m,n=1Vf(Xmin⁡)(wmTwn+bm+bn−log⁡Xmn)2 mbox{loss function} = sum_{m,n = 1}^V f(X_{ min})(w_m ^ T w_n + b_m + b_n - log X_{mn})^2 mboxlossfunction=∑m,n=1V​f(Xmin​)(wmT​wn​+bm​+bn​−logXmn​)2 . FastText . FastText는 단어를 구성하는 Subwords(substrings)의 vector 합으로 단어 vector를 표현합니다. 이 방법은 typo(오식)가 있는 단어라 할지라도 비슷한 representation을 얻을 수 있으며, 새로운 단어에 대해서도 형태적 유사성을 고려한 적당한 word representation을 얻도록 도와줍니다. . 자연어 처리에서 자주 등장하는 문제는 (1) out of vocabulary, (2) infrequent words(모호성) 입니다. word2vec에서는 앞/뒤에 등장하는 단어로 가운데 단어를 예측하게 학습함으로서 문맥이 비슷한 단어를 유사한 vector로 표현합니다. .",
            "url": "https://fastpages.fast.ai/nlp/2020/01/14/word-representation.html",
            "relUrl": "/nlp/2020/01/14/word-representation.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://fastpages.fast.ai/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "BERT 정리글",
            "content": "BERT, Bidirectional Encoder Representations from Transformers . Abstract . BERT는 unlabeled text를 기반으로 deep bidirectional representation을 pretrain하기 위해서 만들어졌다. 이는 모든 layer에서 왼쪽과 오른쪽 모두의 context정보를 바탕으로 만들어진다. . 결과적으로 pretrained된 BERT 모델은 output layer을 추가하고 fine-tuning을 함으로써 question answering 그리고 language inference분야에서 state-of-the-art 성능을 낼 수 있다. 이는 task-specific한 구조의 변화없이 가능하다. . 1. Introduction . pretrained language representation을 활용하는데는 두 가지 전략이 있다. . Feature-based: ELMO . task-specific한 architecture를 사용한다. . | fine-tuning: OpenAI GPT . task-specific한 parameter를 적게 사용한다. . | . 위의 두 가지 전략은 pretraining을 하는동안 동일한 objective를 사용하게 된다. 또한 모두 unidirectional representation을 사용하고 있다. . . 하지만, 본 논문에서는 unidirectional representation이 pretrained representation 능력을 제한한다고 주장하며, 이는 특히 fine-tuning 방법론에 영향을 많이 끼친다고 말한다. 가장 큰 한계는 standard language models가 unidirectional하며 이는 architecture를 선택할 때 많은 제한을 두게 된다. 예를 들어서, openAI GPT의 경우 left-to-right architecture를 사용한다. 모든 토큰은 self-attention layer에서 자기 자신보다 이전의 있는 토큰에만 영향을 끼친다. 이런 제한은 sentence-level task에서 sub-optimal하다. . 이 논문에서는 세 가지 contribution을 제공한다. . Bidirectional pre-training for language representation | pre-trained representation은 task-specific architecture의 필요성을 감소시킨다. | 11가지 NLP task에 대해서 SOTA의 성능을 보인다. | . 2. Related Work . pre-training general language representations에 관련된 연구들이다. . 2.1 Unsupervised Feature-based Approaches . word embedding vectors . | ELMO . | . 2.2 Unsupervised Fine-tuning Approaches . word embedding parameter를 unlabeled text로 pre-trained 하는 것으로 연구가 시작되었다. . OpenAI GPT | . fine-tuning approach는 학습과정에서 적은 parameter를 학습시키면 된다는 장점을 가지고 있다. . 2.3 Transfer Learning from Supervised Data . NLP, vision task에서 모두 tranfer learning은 효과적임을 보여주고 있다. . Imagenet pretrained | . 3. BERT: unified architecture across different tasks. . BERT는 두 가지 단계를 가지고 있다. . pre-training: unlabeled data를 이용해서 학습을 진행한다. . | fine-tuning: weight값을 pre-trained된 값으로 초기화해준다. 그 후 labeled data를 이용해서 지도학습을 진행하게 된다. . | Model Architecture . Transformer 기반으로 만들어졌다. Transformer 논문은 encoder decoder구조로 이루어져 있지만, BERT에서는 encoder 위주로 사용한다. . . | **Input/Output Representations ** . down-stream task에서 잘 작동하려면, input representation이 single sentence 혹은 a pair of sentence에 모두 유연하게 반응할 수 있어야 한다. . . 해당 논문은 WordPiece embeddings을 사용하였다. 첫번째 token은 항상 [CLS]이다.(special classification token의 역할을 수행) 그리고 [CLS]에 반응하는 hidden state는 classification task를 위한 representation을 모으는 역할을 수행한다. . 또한 두 가지 방법으로 sentence를 구분한다. . special token [SEP]를 이용하여 sentence를 구분한다. | 각 sentence에 sentence embedding을 더하여 어떤 sentence정보를 가지고 있는지 알려준다. | | . 3.1 Pre-training BERT . Task #1: Masked LM . 직관적으로, bidirectional representation이 unidirectional representation보다 좋다는 것은 쉽게 알 수 있다. 하지만, 기존의 연구에서 사용하지 않은 이유는 바로 bidirectional representation을 하게 되면, 간접적으로 각 word가 자신에 대한 정보를 쉽게 얻을 수 있게 되며, 모델이 multi layerd contex에서 target word를 참조하기 힘들게 한다. . 위의 한계점을 극복하고 deep bidirectional representation한 train을 하기 위해서 해당 연구에서는 input token중 일부를 랜덤하게 마스크하고 이렇게 mask된 단어들을 학습하도록 유도하였다. 이를 masked LM(MLM)이라고 한다. 실험적으로 약 15%의 token을 마스크하는것이 효과적이였다. . 하지만, fine-tuning을 할 때, 문제가 발생한다. 바로 [Mask] 했던 token들은 fine-tuning과정에서는 발생하지 않기 때문이다. 이러한 문제점을 해결하기 위해서, 다음과 같은 조치를 취하였다. . i-token이 mask되었다면, 80% 확률로 [Mask]로 대체한다. . 나는 학생이다. -&gt; 나는 [Mask]이다. . | i-token이 mask되었다면, 10% 확률로 random하게 다른 단어로 대체한다. . 나는 학생이다 -&gt; 나는 아파트이다. . | i-token이 mask되었다면, 10% 확률로 변화시키지 않는다. . 나는 학생이다. -&gt; 나는 학생이다. . | 이런 조치를 취하게 되면, Transformer layer가 cross entropy loss로 학습을 진행하게 된다. 참고로 random하게 단어를 변화시키는 확률은 $0.15 * 0.1 = 0.015$ 로 모델의 표현력을 저하시키지 않는다. . | Task #2: Next Sentence Prediction(NSP) . Question Answering, Natural Language Inference와 같은 task에서 두 문장 사이의 관계를 파악하는 것은 중요하다. 이는 language modeling에서 직접적으로 관계를 파악하기 힘들다. 두 문장 사이의 관계를 파악하기 위해서 해당 논문은 next sentence prediction task로 pre-train을 하였다. . 특히 A문장과 B문장을 고를때, 50%확률로 B가 실제로 뒤에 있는 문장이다. | 나머지 50% 확률로 random하게 배치한다. | . 하지만, 이전 연구에서는 sentence embedding만 down-task에 전이가 한다. (BERT는 모든 parameter를 전이한다.) . | . 3.2 Fine-tunning BERT . self-attention layer의 영향으로 fine-tuning은 매우 순조롭다. | input sequence에 대해서 일정한 차원수의 representation 결과를 얻고 싶기 때문에, [CLS] token의 Transformer output값을 사용합니다. | [CLS] token의 벡터는 H차원을 가집니다. $C∈R^H$ | 여기서 classify하고 싶은 갯수(K)에 따라 classification layer를 붙여 줍니다. classification layer : $W∈R^{K×H} W∈R^{K×H}$ | label probabilities는 standard softmax로 계산 됩니다.$P=softmax(CWT)P=softmax(CWT)$ | W matrix와 BERT의 모든 파라미터가 같이 fine-tuning 됩니다. . | hyperparmeter . Batch size: 16, 32 Learning rage (Adam): 5e-5, 3e-5, 2e-5 Number of epochs : 3, 4 . | pretraining과 비교했을때, 상대적으로 fine-tuning은 적은 비용으로 학습시킬수 있다. . | fine-tuning시 dataset의 크기가 클수록 hyperparameter의 영향을 덜 받고 잘 training됨을 관측할 수 있었다고 합니다. | . 3.3 Comparision of BERT, ELMO and OpenAI GPT . . BERT와 OpenAI GPT는 fine-tuning approach인 반면에, ELMo는 feature-based approach이다. | BERT와 OpenAI GPT는 유사한 pre-training방법론을 사용하였다. | BERT와 ELMo는 bidirectional representation을 얻는다. OpenAI GPT는 unidirectional representation을 사용한다. | . 4.Experiments . Fine-tunning on Different Tasks | . . 위의 그림은 각 task에 대한 fine-tuning 방법론을 시각화한 것이다. (a), (b)는 sentence level, (c)와 (d)는 token-level task이다. 참고로 [CLS]는 classification을 위한 단어이고, [SEP]는 연속적이지 않은 token을 나누기 위한 단어이다. . GLUE . | SQuAD v1.1 . | SQuAD v2.0 . | SWAG . | . 5. Ablation Studies . 5.1 Effect of Pre-training Tasks . 5.1 Effect of Model Size . 5.3 Feature-based Approach with BERT . 6. Conclusion .",
            "url": "https://fastpages.fast.ai/deeplearning/nlp/2019/12/30/BERT-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "relUrl": "/deeplearning/nlp/2019/12/30/BERT-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "date": " • Dec 30, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Sorting Algorithm",
            "content": "Sorting Algorithm . Content . Insert sort | Merge sort | Quick sort | Heap sort | . Insert Sort . insert sort는 일반적으로 적은 수의 요소들을 정렬할 때 유리하다. . insert sort는 배열이 주어졌을 때, 순차적으로 순회하면서 각 요소의 올바른 위치로 정렬한다. 만약, i 번째까지 순회하였다면, 0 ~ i번째의 배열은 0 ~ i번째 배열의 성분 기준으로 모두 알맞은 위치에 정렬된 상태이다. i + 1번째 요소는 알맞게 정렬된 0 ~ i번째 배열기준으로 알맞은 위치로 삽입된다. . 이를 의사코드로 나타내면 다음과 같다. . for i in range(1, len(A)): key = A[i] # 알맞은 위치로 가야하는 성분 # Insert into the sorted sequence A[:i] j = i - 1 while j &gt; 0 and A[j] &gt; key: A[j + 1] = A[j] j -= 1 A[j + 1] = key . 이해를 돕기 위해서 아래의 이미지를 첨부하였다. . . Loop invariants and the correctness of insertion sort . 위의 의사코드에서 i index는 현재 정렬해야 되는 성분을 의미하며, A[:i]는 0 ~ i - 1의 성분들을 의미한다. 이 때 A[:i ]는 정렬되기전의 A[:i]와 구성요소는 모두 같으면서 정렬된 배열을 의미한다. 이러한 성질을 CS에서는 loop invariants라고 한다. 그리고 이런 성질은 correcteness를 증명하는데 사용된다. . loop invariants는 3가지 조건을 충족해야한다. . Initialization: 첫 번째 순회에서 참이어야 한다. | Maintenance: 순회를 돌기전에 참이라면, 다음순회를 시작하기 전까지 참이어야 한다. | Termination: 전체 순회가 끝날때, loop invariant가 해당 알고리즘이 정확하다는 것을 알려줘야 한다. | . 예시를 들어보겠다. . A = [0, 3, 8, 2, 1, 4] . Initialization . index번호가 1부터 시작한다.(A[i] = 3) 따라서 A[:i] = [0]이다. 그리고 이는 항상 참이다. . | Maintenance . 순회를 하는 동안 i는 1, 2, 3, 4 …의 값을 가질 것이다. 정렬된 subarray A[:i]는 기존의 정렬되지 않은 A[:i]와 구성요소는 같다. (Loop invariant) . | Termination . 전체 순회가 종료되었을 때를 생각해보자. (i = len(A) - 1) . 순회를 할때마다, i는 1씩 증가하게 되므로, 알고리즘이 종료될 때는 i=len(A) - 1이 된다. 이때 subarray는 A[:i] 이므로 0 ~ len(A) - 2의 범위를 가지게 된다. (정렬된 상태이다.) 그러므로 마지막 성분 A[i]만 올바른 위치에 정렬하면 된다. . | . Analyzing algorithms . input size: array size | running time: 주어진 input에 대해서 주된 연산의 call 수 | . . T(n)=c1n+c2(n−1)+c4(n−1)+c5∑j=2ntj+c6∑j=2n(tj−1)+c7∑j=2n(tj−1)+c8(n−1)T(n) = c_1n + c_2(n-1) + c_4(n-1) + c_5 sum_{j=2}^n t_j + c_6 sum_{j=2}^n (t_j - 1) + c_7 sum_{j=2}^n (t_j - 1) + c_8 (n-1)T(n)=c1​n+c2​(n−1)+c4​(n−1)+c5​j=2∑n​tj​+c6​j=2∑n​(tj​−1)+c7​j=2∑n​(tj​−1)+c8​(n−1) . best case: O(N)/O(1) 모두 정렬되어 있을 경우 $c_5 sum_{j=2}^n t_j + c_6 sum_{j=2}^n (t_j - 1) + c_7 sum_{j=2}^n (t_j - 1)$ 은 무시해도 좋다. | . | worst case: O(N^2)/O(1) 모두 거꾸로 정렬되어 있을 경우 | . | order of growth: O(N^2)/O(1) 해당 알고리즘의 upper bound | . | . Merge sort: devide-and-conquer . devide-and-conquer 방법론은 문제를 본래의 문제와 유사한 작은 문제로 나누어 해결한다. . . devide: problem -&gt; subproblem, subproblem의 성질은 기존의 problem과 동일하다. | conquer: subproblem의 크기가 충분히 작다면, 해를 도출한다. | combine: subproblem을 해를 고려하여 상위의 problem의 문제를 해결한다. | . merge sort는 전형적인 devide-and-conquer 방법론이다. . devide: array를 2개의 작은 array로 나눈다. | conquer: 두개의 작은 array를 재귀적으로 정렬한다. | combine: 두개의 정렬된 작은 array를 합치면서, 정렬된 array를 도출한다. | . . merge sort는 두 가지 방법으로 구현할 수 있다. 아래는 python으로 제작한 의사코드이다. . Top-Down approach: recursive . def merge(A, p, q, r): &quot;&quot;&quot; A[p:q+1]와 A[q+1:r+1]을 합친다. A: 합쳐지기 전 array, A[p:q+1], A[q+1:r+1]은 각각 정렬된 상태 &quot;&quot;&quot; arr, i = [0] * (r - p + 1), 0 while p &lt; q and q &lt; r: if A[p] &lt; A[q]: arr[i] = A[p] p += 1 else: arr[i] = A[q] q += 1 if p &lt; q - p - 1: arr[i+1:] = A[p:q+1] if q &lt; r - q - 2: arr[i+1: ] = A[q+1:r+1] A[p:r+1] = arr def merge_sort(A, p, r): &#39;&#39;&#39; A: 정렬되지 않은 array p: left index r: right index &#39;&#39;&#39; if p &lt; r: q = (p + r) // 2 merge_sort(A, p, q) merge_sort(A, q + 1, r) merge(A, p, q, r) . Bottom-up approach: iterative . def merge(A, p, q, r): &quot;&quot;&quot; A[p:q+1]와 A[q+1:r+1]을 합친다. A: 정렬되지 않은 array &quot;&quot;&quot; arr, i = [0] * (r - p + 1), 0 while p &lt; q and q &lt; r: if A[p] &lt; A[q]: arr[i] = A[p] p += 1 else: arr[i] = A[q] q += 1 if p &lt; q - p - 1: arr[i+1:] = A[p:q+1] if q &lt; r - q - 2: arr[i+1: ] = A[q+1:r+1] A[p:r+1] = arr group_size = 1 while group_size &lt; len(A): i = 0 while i &lt; len(A): merge(A, i, i + group_size, i + 2*group_size + 1) i += group_size * 2 + 2 group_size *= 2 . Loop invariant . insert sort와 마찬가지로 loop invariant 성질을 이용하여 corrteness를 증명할 수 있다. . Initialization: 초기에는 subarray가 비워진 상태로 시작한다. | Maintainance: 위의 의사코드에서 알 수 있듯이 subarray의 성분의 index는 바뀌지만 전체 성분은 변하지 않는다. | Termination: 마지막 두 개의 subarray를 합칠 때를 고려해보자. 각각의 subarray는 정렬되어 있는 상태이다. 따라서 merge 알고리즘을 그대로 적용한다면, 결과는 모두 정렬되어 나올 것이다. | . Analyzing divide-and-conquer algorithms . recursive한 알고리즘을 사용했다면, running time을 recurrence equation을 활용해서 표현할 수 있다. 아래는 예시이다. T(n)={O(1) mboxifn≤c,aT(n/b)+D(n)+C(n) mboxotherwiseT(n) = begin{cases} O(1) &amp; mbox{if } n le c, aT(n/b) + D(n) + C(n) &amp; mbox{otherwise} end{cases}T(n)={O(1)aT(n/b)+D(n)+C(n)​ mboxifn≤c, mboxotherwise​ . subarray가 충분히 작다면: $ mbox{if } n &gt; c$ | $T(n/b)$: $n/b$ subarray의 크기, $a$ 는 subarray의 개수 | $D(n)$: array를 subarray로 나누는데 걸리는 시간 | $C(n)$: combine | . 이제 merge sort를 분석해보자. . divide: $D(n) = O(1)$, subarray의 중간까지만 나눈다. | conquer: $2T(N/2)$ | combine: $C(n) = O(n)$ | . T(n)={O(1) mboxifn≤c,2T(n/2)+O(n) mboxotherwiseT(n) = begin{cases} O(1) &amp; mbox{if } n le c, 2T(n/2) + O(n) &amp; mbox{otherwise} end{cases}T(n)={O(1)2T(n/2)+O(n)​ mboxifn≤c, mboxotherwise​ . . time complexity: O(nlogn) | space complexity: O(nlogn) | . Quick Sort . worst case: O(n ^ 2) . | expected: O(nlogn) constant factor가 다른 기법에 비해서 작은 편이다. . | . quick sort 역시 divide-and-conquer의 한 방법이다. . Divide: Array를 두개의 subarray로 나눈다. $A[p…r]$을 $A[p..q-1]$,$A[q + 1.. r]$ . 로 나누는 것이다. $A[p..q-1]$은 $A[q]$보다 작거나 같다. 또한 $A[q + 1.. r]$은 $A[q]$ 보다 크거나 같다. 나중에 언급하지만 $A[q]$는 pivot이다. . | Conquer: $A[p..q-1]$,$A[q + 1.. r]$를 재귀적으로 정렬한다. . | Combine: subarray가 모두 정렬되어 있기 때문에, 따로 결합할 필요가 없다. . | . quick sort의 의사코드는 아래와 같다. . def quick_sort(A, p, r): if p &lt; r: q = partition(A, p, r) quick_sort(A, p, q - 1) quick_sort(A, q + 1, r) def partition(A, p, r): x = A[r] # pivot value i = p - 1 for j in range(p, r-1): if A[j] &lt;= x: # pivot보다 왼쪽에 위치해야한다. i += 1 A[i], A[j] = A[j], A[i] A[i+1], A[r] = A[r], A[i + 1] return i + 1 . partition이 이해가 안되다면 아래의 그림을 참고해보면 좋다. 참고로 i index는 pivot보다 작거나 같은 값을 가졌다는 것을 알려주며, j index는 탐색하는 의도로 구성되었다. . . Loop invariant . Initialization . i = p - 1 이고 j = p 이다. p와 i 사이에는 value가 없다. 그리고 i + 1과 j- 1사이에도 아무것도 없다. 따라서 loop invariant 조건을 충족한다. (swap이 일어나지 않는다.) . | Maintainance . 위의 그림에서 볼 수 있듯이, pivot value에 따라서 성분의 배치가 달라질 수 있다. 하지만, 배치만 달라질뿐 전체 성분은 변하지 않는다. . | Termination . 알고리즘이 종료될 때는 3가지 set이 주어진다. pivot보다 작은 set, pivot, pivot보다 큰 set 이렇게 주어지게 되는데, 이는 내부적으로 모두 정렬된 상태이다. . | . Performance of quicksort . worst-case . partitioning이 1: n-1식으로 계속해서 나눠지면 최악의 성능 O(N^2)이다. . . | balanced partioning . partitioning이 모두 고루게 나눠지면 성능은 O(NlogN)이다. . | . .",
            "url": "https://fastpages.fast.ai/algorithm/2019/12/28/Sorting-Algorithm.html",
            "relUrl": "/algorithm/2019/12/28/Sorting-Algorithm.html",
            "date": " • Dec 28, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "Big-O notation 정리하기",
            "content": "Big-O notation 정리하기 . In computer science, big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows.[3] In analytic number theory, big O notation is often used to express a bound on the difference between an arithmetical function and a better understood approximation; a famous example of such a difference is the remainder term in the prime number theorem. . reference: https://en.wikipedia.org/wiki/Big_O_notation . # . Formal definition . $F$ 는 real or complex valued function이며 , $g$는 real valued function이다. 두 함수는 무한한 양의 실수의 subset으로 정의된다. 따라서 $g(x)$는 x가 충분히 큰 값일때 항상 양수이다. . F(x)=O(g(x)) as x→∞F(x) = O(g(x)) text{as} x rightarrow inftyF(x)=O(g(x)) as x→∞ . 충분히 큰 x값을 가질 수 있을때만, $F(x)$의 절대값의 최대값이 $g(x)$에 양의 상수를 곱한 것을 넘지 못할 때, $F(x) = O(g(x))$라고 표현한다. (Upper Bound)이를 아래 식으로 표현한다. . ∣F(x)∣≤Mg(x)for all x≥x0 rvert F(x) rvert le Mg(x) text{for all } x ge x_0∣F(x)∣≤Mg(x)for all x≥x0​ . 이를 간단하게 아래의 이미지로 확인할 수 있다. . . 파란색 선은 $Mg(x)$ 를 빨간색 선은 $F(x)$를 의미한다. $x_0$보다 큰 x값을 가지면, $F(x)$는 항상 $Mg(x)$보다 작으며, 이는 $F(x)$의 upper bound가 $Mg(x)$라는 것을 의미한다. .",
            "url": "https://fastpages.fast.ai/algorithm/2019/12/28/Big-O-notation-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0.html",
            "relUrl": "/algorithm/2019/12/28/Big-O-notation-%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0.html",
            "date": " • Dec 28, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "attention is all you need 정리글",
            "content": "attention is all you need 정리글 . Transformer 구조 . . 위의 이미지는 Transformer의 구조이다. encoder와 decoder 구조로 이루어져 있다. 마지막 encoder layer의 output이 각 decoder stack에 input으로 들어가게 된다. (residual connection) 각 encoder layer와 decoder layer는 모두 동일한 구조를 가지나 서로 parameter를 공유하지 않는다. 아래의 그림처럼 논문에서는 각 6개의 layer를 가지고 있다. . . 아래의 이미지는 encoder와 decoder의 세부 구조이다. . . 각 세부 layer사이에는 Normalization 및 bias를 더하는 과정이 추가된다. . Matrix Calculation of Self-Attention . 이제 복수의 embeddinb vector를 matrix 연산으로 대체하는 과정을 살펴보자. 위의 그림과는 다르게 embedding vector가 matrix형태로 제공되어서 병렬 연산이 가능해졌다. 아래의 이미지 참고. . . . Attention . . 1. Scaled Dot-Product Attention . Attention(Q,K,V)=softmax(QKTdk)VAttention(Q, K, V) = softmax( frac{QK^T}{ sqrt{d_k}}) VAttention(Q,K,V)=softmax(dk​ . ​QKT​)V . Query, Key - Value의 역할 . Query: , matrix | Key: 각 embedding vector의 key, matrix | Value: 각 key가 가지고 있는 value, matrix | . 추가적인 설명 우선 query와 key, value에 대해서 설명하면 query가 어떤 단어와 관련되어 있는지 찾기 위해서 모든 key들과 연산한다. 여기서 실제 연산을 보면 query와 key를 dot-product한뒤 softmax를 취하는데, 의미하는 것은 하나의 query가 모든 key들과 연관성을 계산한뒤 그 값들을 확률 값으로 만들어 주는 것이다. 따라서 query가 어떤 key와 높은 확률로 연관성을 가지는지 알게 되는 것이다. 이제 구한 확률값을 value에 곱해서 value에 대해 scaling한다고 생각하면된다. . 추가적인 설명 key와 value는 사실상 같은 단어를 의미한다. 하지만 두개로 나눈 이유는 key값을 위한 vector와 value를 위한 vector를 따로 만들어서 사용한다. key를 통해서는 각 단어와 연관성의 확률을 계산하고 value는 그 확률을 사용해서 attention 값을 계산하는 용도이다. . reference: https://reniew.github.io/43/ . 아래의 이미지는 scaled dot product attention과정의 일부이다. query, key 그리고 value는 각 $W^Q, W^K, W^V$matrix와 dot product를 진행한 결과이다. . . 그리고 query와 key의 dot product의 결과를 $ sqrt{d_k}$만큼 scaling 해준다. . . 2. Multi-Head Attention . MultiHead(Q,K,V)=Concat(head1,⋯ ,headh)WO  where headi=Attention(QWiQ,KWiK,VWiV)MultiHead(Q, K, V) = Concat(head_1, cdots, head_h) W^O where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)MultiHead(Q,K,V)=Concat(head1​,⋯,headh​)WO  where headi​=Attention(QWiQ​,KWiK​,VWiV​) . $W_i^Q in R_{d_{model} times d_k}$ | $W_i^K in R_{d_{model} times d_k}$ | $W_i^V in R_{d_{model} times d_k}$ | . 해당 연구에서는 multi head attention을 적용하였다. 이는 두 가지 방식으로 성능향상에 기여하였다. . model이 다른 위치에 집중할 수 있는 능력을 향상시켰다. “The animal didn’t cross the street because it was too tired” 과 같은 문장을 번역하는데 효과적인데 그 이유는 it이 가르키는 것이 무엇인지 중요하기 때문이다. | layer multiple representation subspace를 제공한다. 복수의 Q, K, V matrix를 가지게 되고 이는 random하게 초기화된다. | . 아래의 그림은 두 개의 embedding vector(Thinking, Machines)의 복수의 head를 가지게 되는 과정을 시각화 한 것이다. . . Representing The Order of The Sequence Using Positional Encoding . 위에서의 attention 과정에서 word의 위치정보를 잃어버리게 된다. 이를 어떻게 복구할 것인가? . 이런 문제를 극복하기 위해서 Transformer에서는 input embedding vector에 특별한 vector를 더한다. 이는 각 word의 위치를 파악하는데 도움을 주거나 각 word의 distance를 구하는데 도움을 줄 것이다. . . . 아래의 이미지는 실제 20개의 word의 positional encoding의 시각화 결과이다. (512 dimension) . . PE(pos,2i)=sin(pos/100002i/dmodel)PE_{(pos, 2i)}=sin(pos/10000^{2i/d_{model}})PE(pos,2i)​=sin(pos/100002i/dmodel​) . PE(pos,2i+1)=cos(pos/100002i/dmodel)PE_{(pos, 2i+1)}=cos(pos/10000^{2i/d_{model}})PE(pos,2i+1)​=cos(pos/100002i/dmodel​) . pos는 word의 위치를 나타낸다. | i 는 dimension의 index를 나타낸다. | . Reference . http://jalammar.github.io/illustrated-transformer/ .",
            "url": "https://fastpages.fast.ai/deeplearning/nlp/transformer/2019/11/18/attention-is-all-you-need-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "relUrl": "/deeplearning/nlp/transformer/2019/11/18/attention-is-all-you-need-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "date": " • Nov 18, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency 정리글",
            "content": "Unsupervised Monocular Depth Estimation with Left-Right Consistency 정리글 . Abstract . 많은 방법론들이 depth estimation부분에서 성과를 보여주었다. 하지만, 대부분 지도학습이라는 한계를 가지고 있으며, 이는 결국 많은 수의 ground-truth data가 필요하다는 것을 의미한다. 하지만, depth를 기록하는 것은 매우 어려운 문제이다. 따라서 이 연구에서는 얻기 쉬운 binocular stereo footage를 이용하여 문제를 해결한다. . epipolar geometry constraints를 활용해서 reconstruction loss로 학습을 시킬 수 있다. 하지만 이 결과물은 depth image의 질이 낮아진다. 이러한 문제를 해결하기 위해서, consistency를 유지할 수 있게하는 loss를 제안한다. . Epipolar geometry . . stereo vision의 geometry, . 2개의 각기 다른 위치에서 3D 이미지의 정보를 얻을 때. 3D points와 2D points간의 많은 기하학적 관계가 있다. . Introduction . 이전의 많은 연구들이 multiple observation이 가능하다는 가정아래에서 진행되었다. multiple viewpoint와 다른 조명조건에서의 data도 필요하다. 이러한 한계를 극복하기 위해서, supervised learning방식으로 mono depth estimation 방법론들이 연구되었다. 이러한 방법론들은 많은 수의 data를 기반으로 depth를 직접적으로 추정하고자 하였다. 많은 성과를 이루었지만, 데이터의 수가 많아야된다는 한계를 가지고 있다.(depth 관련 데이터를 수집하기는 힘들다) . image의 외관과 관계없이 장면의 모양을 이해할 수 있는 것은 machine perception분야에서 매우 중요한 이슈이다. . 사람은 monocular depth estimation을 잘하는데 이를 위해서 다음과 같은 힌트를 사용한다. . 원근법 | 이미 알려진 물체크기로 상대적인 추정 | 조명 혹은 그림자 가려진 상태에서의 모양 | etc | . 위와 같은 top-down, bottom-up 힌트들을 조합해서 정확하게 depth를 추정할 수 있다. 이 연구에서는 depth data가 필요하지 않다. 학습과정에서는 synthesize depth를 이용한다. 두 이미지(left view, right view) 사이에서 해당 모델은 pixel level의 예측을 한다. (regression) 다른 연구에서도 이와 같은 방법론을 사용했지만 아래와 같은 한계가 존재한다. . memory issue | not fully diferentiable | . Related Work . Learning-Based Stereo: between two images . 대게 stereo estimation은 첫번째 이미지의 특정 픽셀과 두번째 이미지의 모든 pixel간의 similarity를 계산하는 알고리즘이다. 대부분의 stereo pairs는 정제되어 있으며, disparity estimation은 각 pixel의 1D search 문제이다. . 하지만, 최근 연구에 따르면, head defined similarity measure방법론보다 matching function을 일종의 supervised learning problem으로 두고 문제를 해결하는 것이 더 좋은 성능을 보였다. 특히 Mayer et al.fully convolutional deep network를 활용하여 DispNet을 고안하였다. 이 방법론은 매우 많은 수의 ground-truth data가 필요했다. . Supervised Single Image Depth Estimation . Saxena et al. patch based model(Make3D)을 제안하였다. 이는 laser scan data를 이용하여 학습되었으며 prediction은 MRF를 활용하여 결합하였다. 이 방법론의 단점은 얇은 구조물을 modeling하는데 적절하지 않았다. 이는 현실적인 이미지를 만드는데 적절치 않다는 것을 의미한다. . 위의 방법론은 hand tuning이 필요하다. 이런 방법론과 다르게, Liu et al은 CNN을 이용하여 이를 학습하고자 했다. . Karsch et al. consistent한 image output을 가질려고 노력했다. 이는 training set으로부터 depth image를 복사하여 이루어졌다. 따라서 이 방법론은 test할 때, 모든 training set이 필요하다는 한계를 가진다. . Eigen et al.은 두개의 scale deep network를 활용해서 depth estimation이 가능하다는 것을 보였다. 이들은 hand craft feature를 사용하지 않았고, initial over segmentation을 이용하지 않았다. 대신에 raw pixel value를 활용하여, representation을 하였다. . 많은 연구들이 CRF기반으로 accuracy향상을 이루었다. . Unsupervised Depth Estimation . Flynne et al.의 DeepStereo는 새로운 view의 이미지를 만들어낸다. 학습하는동안, 다양한 카메라의 상대적인 pose가 근처 이미지의 모습을 만들어내는데 사용된다. test할 때, image synthesis는 겹치는 작은 patch에서 작동한다. 이 모델은 근처의 다른 posed image의 view가 필요하므로, monocular depth estimation에 부적절하다. . Xie et al의 Deep3D의 목표는 left image 기반으로 right view이미지를 만들어내는 것이다. reconstruction loss를 활용하며, 각 픽셀에 대해서 가능한 disparities에 대해서 분포를 만들어낸다. 이 방법론의 단점은 scalable하지 않다는 것이다. 가능한 disparities가 많아질수록 많은 memory가 필요하다. . 해당연구와 비슷한 연구는 Garg et al이 제안하였다. 이 nework는 monocular depth estimation방법론이긴 하지만, fully differentiable하지 않다는 한계를 가진다. 이런 점을 극복하기 위해서 taylor approximation을 사용하였다. . Method . 1. Depth Estimation as Image Reconstruction . 해당 연구에서는 직접 depth를 추정하는 것이 아니라 image reconstruction을 이용하여 추정한다. 기존의 연구에서는 depth estimation문제를 supervised task로 인식해서 해결했지만, 앞서 설명했듯이 depth ground truth data를 구하는 것은 매우 힘든 일이다. 비싼 하드웨어를 사용하더라고 실제환경에서는 정확하지 않을 수 있다. . 이 연구에서는 training과정에서 image reconstruction을 활용하여 해결한다. 메인 아이디어는 left-view image에서 right-view image를 만들 수 있는function을 구할 수 있다면, 3D shape에 대한 지식을 알고 있는 것이라고 볼 수 있다.(right view -&gt; left-view도 마찬가지) . left-view image로부터 right-view image를 만든다음에 각 픽셀이 가지는 depth value를 추정한다. . Image disparity . https://www.quora.com/What-are-disparity-maps-and-How-are-they-created . baseline distance between the camera and the camera focal length $f$: $b$ | image disparity: $d$ | depth $ hat{d} = bf/d$ | . 2. Depth Estimation Network . Disparity map . . 이 연구에서는 left-to-right , right-to-left disparities를 모두 구할 수 있으며, 서로 consistency를 유지하게 함으로써 더 좋은 성능을 가져온다. 아래는 기존의 연구의 architecture와 해당 연구의 architecture를 보여주고 있다. . . 오른쪽 이미지를 보면, left image를 가지고 right image를 생성해낸다. 하지만, 우리는 right-view image에서 sampling된 left-view image가 필요하다. No LR을 보면 right-view image에서 left view image를 생성한다. 하지만 이는 texture-copy artifact라는 현상을 보이며, detph가 연속적이지 않은 부분에서 error가 많이 발생한다. 아래의 이미지를 보면 알 수 있다. . . 이 연구에서는 이 문제를 model이 두가지 disparity map를 생성해내도록 만들어서 해결하였다. (sampling from the opposite input images) 이 방법은 여전히 left-view image하나만 필요하며, right image는 training과정에서만 사용된다. left-right consistency loss를 이용하여, 더 정확한 정확도를 가질 수 있다. . 3. Training Loss . C=Σs=14CsC = Sigma_{s=1}^4C_sC=Σs=14​Cs​ . Cs=αap(Capl+Capr)+αds(Cdsl+Cdsr)+αlr(Clrl+Clrr)C_{s} = alpha_{ap}(C_{ap}^l + C_{ap}^r) + alpha_{ds}(C_{ds}^l + C_{ds}^r) + alpha_{lr}(C_{lr}^l + C_{lr}^r)Cs​=αap​(Capl​+Capr​)+αds​(Cdsl​+Cdsr​)+αlr​(Clrl​+Clrr​) . Apperance Matching Loss: $C_{ap}$ | Disparity Smoothness Loss: $C_{ds}$ | Left-Right Disparity Consistency: $C_{lr}$ | . . Apperance Matching Loss는 oposite setero이미지로부터 image를 생성하도록 학습시킨다. 여기서는 spatial transformer network를 사용하여 disparity map을 만든다. 여기서 bilinear sampler를 사용하는데 locally fully differentiable하며 fully convolutional architecture를 가진다. . Capl=1NΣi,jα1−SSIM(Iijl,I^i,jl)2+(1−α)∥Iijl−I^i,jl∥C_{ap}^l = frac{1}{N} Sigma_{i, j} alpha frac{1 -SSIM(I_{ij}^l, hat{I}_{i,j}^l)}{2} + (1 - alpha) rVert I_{ij}^l - hat{I}_{i,j}^l rVertCapl​=N1​Σi,j​α21−SSIM(Iijl​,I^i,jl​)​+(1−α)∥Iijl​−I^i,jl​∥ 이 연구에서는 $ alpha$ 는 0.85 SSIMd에서는 3 x 3 block filter를 사용했다. . SSIM: structural similarity . 사람의 시각 시스템은 이미지에서 구조 정보를 도출하는데 특화되어 있기 때문에 구조 정보의 왜곡정도가 지각 품질에 가장 큰 영향을 미친다. 이것이 SSIM의 기본이 되는 핵심가설이다. 구체적으로는 원본 이미지 x와 왜곡 이미지 y의 brightness, contrast, structure를 비교한다. . brightness . $u_x$: x 이미지의 평균밝기 . $u_y$: y 이미지의 평균 밝기 . $I(x, y) = frac{2u_xu_y + C_1}{u_x^2 + u_y^2 + C_2}$ . | contrast . $C(x, y) = frac{2 sigma_x sigma_y+C_2}{ sigma_x^2 + sigma_y^2 + C_2}$ . | structure . structure = $ frac{x-u_x}{ sigma_x}$ . $S(x, y) = frac{ sigma_{xy} + C_3}{ sigma_x sigma_y + C_3}$ . | SSIM . $SSIM(x,y) = I(x,y)C(c,y)S(x,y)$ . $SSIM(x, y)= frac{(2u_xu_y + C_1) (2 sigma_{xy} +C_2)}{(u_x^2 + u_y^2 + C_1)( sigma_x^2 + sigma_y^2 +C_2)}$ . | . Disparity Smoothness Loss의 경우에는 아래와 같이 수식으로 표현된다. 이는 disparities가 locally smooth하게 하는 효과를 가져온다. Ddsl=1NΣi,j∣∂xdi,jl∣e∥∂xIi,j∥+∣∂ydi,jl∣e∥∂yIi,j∥D_{ds}^l = frac{1}{N} Sigma_{i,j} rvert partial_{x}d_{i,j}^l rvert e ^{ rVert partial_{x}I_{i,j} rVert} + rvert partial_y d_{i,j}^l rvert e ^{ rVert partial_y I_{i,j} rVert}Ddsl​=N1​Σi,j​∣∂x​di,jl​∣e∥∂x​Ii,j​∥+∣∂y​di,jl​∣e∥∂y​Ii,j​∥ . Left-Right Disparity Consistency Loss 는 더 정확한 disparity map을 만들기 위한 term이다. 이 term은 left-view disparity map을 projected right-view disparity map과 동일하게 만들어주는 역할을 한다. Clrl=1NΣi,j∣dijl−dij+dijlr∣C_{lr} ^l = frac{1}{N} Sigma_{i,j} rvert d_{ij}^l - d_{ij+d_{ij}^l} ^r rvertClrl​=N1​Σi,j​∣dijl​−dij+dijl​r​∣ .",
            "url": "https://fastpages.fast.ai/deeplearning/computer%20vision/2019/11/04/Unsupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "relUrl": "/deeplearning/computer%20vision/2019/11/04/Unsupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "date": " • Nov 4, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "Feature Visulatization 번역글",
            "content": "Feature Visulatization 번역글 . Introduction . neural network의 해석가능성에 대한 필요성이 늘어나고 있다. Deep learning의 해석가능성은 크게 두 가지 문제로 나뉜다. . feature visualization . . network 혹은 network의 부분이 무엇을 보고자 하는가 . | attribution . network가 다음과 같이 동작하는 이유가 무엇인가 . class activation map이 하나의 예시가 될 수 있다. . | Feature Visualization by Optimization . 일반적으로 neural network는 input에 대해서 differentiable하다. 만약 당신이 어떤 종류의 input이 특정한 행동양상을 가지는지 알고 싶다면(내부적인 뉴런의 동작 혹은 마지막 결과물의 양상이 예시가 될 수 있다.), iteratively 미분하면서 목표를 이룰수 있다. 이렇게 얘기하면 매우 쉬울 거 같지만, 이를 하기 위해서는 많은 문제를 해결해야 한다. . . 위의 예시는 random noise를 input으로 두고 특정 뉴런을 활성화 시키기 위해서 input을 변화시켜나가는 과정으로 보인다. . Optimization Objective . . . Neuron: $layer_n [x,y,z]$ | Channel: $layer_n[:, :, z]$ | Layer: $layer_n[:,:,:]$ | Class Logit:pre_softmax[k] | Class Probability: softmax[k] | . 주목할 점은 특정 class의 softmax 값을 증가시키는 쉬운 방법은 해당 class에 가깝게 만드는 것이 아니라, 다른 class과 유사하지 않게 만드는 식으로 optimization이 진행된다는 것이다. 경험상 class logit을 objective로 삼는 것이 더 좋은 결과를 얻을 수 있었다. . 위의 방법론 말고도 여러가지 방법을 시도해 볼 수 있다. style transfer도 좋은 예시이다. style transfer에서는 content와 style이라는 개념이 나온다. model이 optimization을 진행할 때 어떤 정보는 유지하고 어떤 정보는 버리는지에 대해서 알 수 있다. . Why visualize by optimization? . 왜 dataset 그 자체로는 feature visualization을 하지 않고 optimization을 사용하는가? . 이는 optimization이 model이 실제로 보고 있는 것을 시각화 할 수 있는 효과적인 방법이기 때문이다. 실제 dataset은 neuron이 보고 있는 것과 차이가 생길 수 있다. . 그 이유는 optimization이 model의 행동을 유발하는 요소와 상관관계가 있는 요소를 분리할 수 있기 때문이다. 아래의 예시 이미지를 보면 쉽게 알 수 있다. . . 또한 optimization은 유연하다는 장점을 가진다. 특정 neuron을 활성화 시키고 싶다면 그에 맞게 수식을 적용하면 된다. . Diversity . optimization을 사용할 때는 주의할 필요가 있다.예를 들어, genuine을 표현하고 싶은데, facet의 특징으로 설명할 수 도 있다. . 여기서 Dataset example이 매우 큰 장점을 가진다. 이를 통해서 diverse example을 찾을 수 있었다. . Achieving Diversity with Optimization . nework는 inputs의 넓은 범위에 활성화될 수 있다. 예를 들어서 class level에서 생각해보자. 만약 classifier가 개를 인식하게 학습이 되었다면, 해당 classifier는 개의 얼굴과 전체적인 시각적인 특징을 잡아내야 한다. (비록 시각적으로 그 둘의 차이가 크더라도) . related work | . 이전의 연구에서 intra-class diversity에 대해서 밝히려는 시도가 있었다. [1] . training set에서 나오는 모든 activation을 수집해서 clustering을 하였다. . 다른 방식으로 접근한 예도 있다. [2] 이 방법은 optimization process의 staring point를 가지고 intra class diversity를 증명하려고 했다. . 최근의 연구로는 generative model과 결합한 시도가 있다[3] . 이 글에서 제시하는 방법은 간단하게 적용할 수 있다. diversity term을 objective에 추가해서 multiple example이 서로 다르다고 하게끔 학습이 진행된다. 결과가 개선되었는데 정확한 이유는 아직 알 수 없다. 다만 추측하기로는 penalize the cosine similarity 혹은 feature가 다른 style로 보일 수 있게끔 학습이 진행되어서 그런것이라고 보고 있다.(style transfer) . . 오른쪽을 보면 다양한 뷰의 강아진 사진이있다. 그리고 왼쪽의 결과물은 diversity를 고려한 optimization의 결과물이다. . . 위의 그림은 diversity를 고려하지 않은 결과물이다. . Interaction between Neurons . neuron은 혼자서 작용하는 것이 아니라 다른 neuron과의 상호작용을 통해서 결과물을 도출한다. 이를 이해하기 위해서 geometrically하게 생각하는 것을 추천한다. . activation space를 activation의 모든 조합이 나올 수 있는 공간이라고 정의하자. 그렇다면 우리는 activation자체를 basis로 생각할 수 있다. 그리고 activation의 조합은 activation space에서 vector의 역할을 한다. . 위에서 언급한 activaiton space, combination of activation, vector는 basis vector가 다른 vector에 비해서 더 해석하기 쉬울까에 대해서 논의할 수 있다. . 이전의 연구에서 basis vector의 direction이 더 쉽게 이해할 수 있다고 한다. [1] [2] . 그리고 이글에서의 실험도 위의 견해와 일치하게 결과가 나왔다. . . 위의 이미지는 각 이미지에 대한 optimization을 적용하였을 때의 결과물이다. . . 위의 결과물도 흥미롭다. direction을 정의할 수 있다는 것인데, mosaic neuron에 흑백의 neruon을 더하면 흑백의 mosaic neuron이 나오게 된다. 이는 word2vector 혹은 generative model의 latent space와 유사한 개념이다. . (interpolation) . . 위의 이미지는 두 뉴런의 interpolation의 결과물이다. 해당 뉴런이 어떤식으로 결합되는지 확인할 수 있다. 위의 방법으로는 아주 작은 힌트만 얻을 수 있다. 예를 들면, 몇개의 interaction이 존재하는지 하지만 실제상황에서는 수백개의 interaction이 존재한다. . The Enemy of Feature Visualization . 위에서 말한 optimization 방법론은 실제로 잘 작동하지 않는다. 아래 이미지와 같은 약간 이상하면서 자주 나타나는 패턴이 있다. 이 이미지는 실제 data상에서 잘 보이지 않는 패턴이며, 특정 뉴런을 활성화 시키기 위한 cheeting 같은 느낌이 든다. 이는 adversarial attack과 유사해보인다 . . 위에서 언급한 자주 보이는 패턴이 convolution과 pooling 연산에 의존적임을 확인할 수 있었다. . . 정리하자면, constraint없는 optimization은 매력적이긴 하지만, 위의 예시처럼 의미없는 결과를 불러올 수 있다. (결국 adversarial example과 유사하게 만들어진다.) . The Spectrum of Regularization . 위의 자주 보이는 패턴을 다루는 것은 feature visualization 연구에서 매우 중요한 영역이다. 만약 더 유용한 visualization을 원한다면, prior, regularizer, constraint를 조합하여 만들어야한다. . 연구분야에서는 regularization에 대한 관심이 많아보인다. . Reference . https://distill.pub/2017/feature-visualization/ .",
            "url": "https://fastpages.fast.ai/deeplearning/interpretability/2019/10/29/Feature-Visulatization-%EB%B2%88%EC%97%AD%EA%B8%80.html",
            "relUrl": "/deeplearning/interpretability/2019/10/29/Feature-Visulatization-%EB%B2%88%EC%97%AD%EA%B8%80.html",
            "date": " • Oct 29, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "tensorrt 정리글",
            "content": "“tensorrt 정리글” . [1.3. How Does TensorRT Work?] . inference 과정을 최적화 시키기 위해서, TensorRT는 network definition을 가져와서 해당 환경(GPU)에서 최적화를 하며, inference engine을 생성한다. 이 과정은 상당한 시간이 소요되며, embedded 된 platform(하드웨어)에서는 더 오래걸린다. 이런 이유 때문에, 보통 engine을 만들면 그것을 serialize화 하여, 저장하고 후에 읽어와서 사용하는 방법을 선호한다. . Note: 위의 generated된 file은 다른 tensorRT 버전 혹은 다른 플렛폼에서 사용할 수 없다. 특정 gpu에서만 사용해야한다. . serialize . 직렬화(直列化) 또는 시리얼라이제이션(serialization)은 컴퓨터 과학의 데이터 스토리지 문맥에서 데이터 구조나 오브젝트 상태를 동일하거나 다른 컴퓨터 환경에 저장(이를테면 파일이나 메모리 버퍼에서, 또는 네트워크 연결 링크 간 전송)하고 나중에 재구성할 수 있는 포맷으로 변환하는 과정이다.[1] . https://ko.wikipedia.org/wiki/%EC%A7%81%EB%A0%AC%ED%99%94 . 다음과 같은 과정을 통해서 inference engine이 만들어진다. . 사용되지 않는 output을 가지는 layer를 삭제 | convolution, bias, Relu operations를 결합하여 연산 | 비슷한 parameter 혹은 같은 source tensor를 사용하는 연산을 묶기 | layer output을 correct eventual destination에 지정하여 concatenation layer 합치기 | . 또한 precision of weights를 변경하여 최적화하기도 한다. (half-precision) . [1.4. What Capabilities Does TensorRT Provide?] . import, calibrate, generate, deploy optimized networks . Network는 Caffe 혹은 다른 framework를 통해서 import할 수 있다. (UFF or ONNX formax) . 다음은 tensorRT의 대표적인 라이브러리이다. . Network Definition . input, output tensor를 정의하거나, 특정 layer를 정의할 때 사용할 수 있다. . | Builder . engine을 만들 때 사용 . | Engine . inference를 실행할 때 사용한다. . | Caffe Parser . caffe로 저장된 모델을 불러들일때 사용 . | UFF Parser . UFF로 저장된 모델을 불러들일때 사용 . | ONNX Parser . ONNX로 저장된 모델을 불러들일 . | . 지원하지 않는 노드/레이어 . object detection 혹은 다른 모델을 tensorRT에 적용할 때, 지원하지 않는 노드가 있을 수도 있다. 이럴때는 GraphSurgeon을 이용하여 따로 정의해주어야 한다. . 아래는 SSD에 tensorRT를 적용할 때의 예시이다. . # Model download and UFF convertion utils import os import sys import tarfile import requests import tensorflow as tf import tensorrt as trt import graphsurgeon as gs import uff class ModelData(object): # Name of input node INPUT_NAME = &quot;Input&quot; # CHW format of model input INPUT_SHAPE = (3, 300, 300) # Name of output node OUTPUT_NAME = &quot;NMS&quot; @staticmethod def get_input_channels(): return ModelData.INPUT_SHAPE[0] @staticmethod def get_input_height(): return ModelData.INPUT_SHAPE[1] @staticmethod def get_input_width(): return ModelData.INPUT_SHAPE[2] def ssd_unsupported_nodes_to_plugin_nodes(ssd_graph): &quot;&quot;&quot;Makes ssd_graph TensorRT comparible using graphsurgeon. This function takes ssd_graph, which contains graphsurgeon DynamicGraph data structure. This structure describes frozen Tensorflow graph, that can be modified using graphsurgeon (by deleting, adding, replacing certain nodes). The graph is modified by removing Tensorflow operations that are not supported by TensorRT&#39;s UffParser and replacing them with custom layer plugin nodes. Note: This specific implementation works only for ssd_inception_v2_coco_2017_11_17 network. Args: ssd_graph (gs.DynamicGraph): graph to convert Returns: gs.DynamicGraph: UffParser compatible SSD graph &quot;&quot;&quot; # Create TRT plugin nodes to replace unsupported ops in Tensorflow graph channels = ModelData.get_input_channels() height = ModelData.get_input_height() width = ModelData.get_input_width() Input = gs.create_plugin_node(name=&quot;Input&quot;, op=&quot;Placeholder&quot;, dtype=tf.float32, shape=[1, channels, height, width]) PriorBox = gs.create_plugin_node(name=&quot;GridAnchor&quot;, op=&quot;GridAnchor_TRT&quot;, minSize=0.2, maxSize=0.95, aspectRatios=[1.0, 2.0, 0.5, 3.0, 0.33], variance=[0.1,0.1,0.2,0.2], featureMapShapes=[19, 10, 5, 3, 2, 1], numLayers=6 ) NMS = gs.create_plugin_node( name=&quot;NMS&quot;, op=&quot;NMS_TRT&quot;, shareLocation=1, varianceEncodedInTarget=0, backgroundLabelId=0, confidenceThreshold=1e-8, nmsThreshold=0.6, topK=100, keepTopK=100, numClasses=91, inputOrder=[0, 2, 1], confSigmoid=1, isNormalized=1 ) concat_priorbox = gs.create_node( &quot;concat_priorbox&quot;, op=&quot;ConcatV2&quot;, dtype=tf.float32, axis=2 ) concat_box_loc = gs.create_plugin_node( &quot;concat_box_loc&quot;, op=&quot;FlattenConcat_TRT&quot;, dtype=tf.float32, axis=1, ignoreBatch=0 ) concat_box_conf = gs.create_plugin_node( &quot;concat_box_conf&quot;, op=&quot;FlattenConcat_TRT&quot;, dtype=tf.float32, axis=1, ignoreBatch=0 ) # Create a mapping of namespace names -&gt; plugin nodes. namespace_plugin_map = { &quot;MultipleGridAnchorGenerator&quot;: PriorBox, &quot;Postprocessor&quot;: NMS, &quot;Preprocessor&quot;: Input, &quot;ToFloat&quot;: Input, &quot;image_tensor&quot;: Input, &quot;MultipleGridAnchorGenerator/Concatenate&quot;: concat_priorbox, &quot;MultipleGridAnchorGenerator/Identity&quot;: concat_priorbox, &quot;concat&quot;: concat_box_loc, &quot;concat_1&quot;: concat_box_conf } # Create a new graph by collapsing namespaces ssd_graph.collapse_namespaces(namespace_plugin_map) # Remove the outputs, so we just have a single output node (NMS). # If remove_exclusive_dependencies is True, the whole graph will be removed! ssd_graph.remove(ssd_graph.graph_outputs, remove_exclusive_dependencies=False) return ssd_graph . https://github.com/NVIDIA/object-detection-tensorrt-example/tree/master/SSD_Model/utils | . Reference . https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#fit__fit2 .",
            "url": "https://fastpages.fast.ai/deeplearning/2019/09/08/tensorrt-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "relUrl": "/deeplearning/2019/09/08/tensorrt-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "date": " • Sep 8, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Activation atlas 정리글",
            "content": "Activation atlas 정리글 . Introduction . What have these networks learned that allows them to classify images so well? . 네트워크가 classification을 잘하는 이유를 찾기 위해서 다음과 같은 시도를 하였다. . 기본적으로 네트워크를 시각적으로 분석할려고 노력했다. . . individual neurons . 뉴런들을 독립적으로 시각화 . | Interaction between Neurons . 뉴런은 독립적으로 움직이는 것이 아니기 때문에 simple feature combination을 시각화함. 이런 시도는 문제점을 가지고 있었다. . 예를 들어, 수 많은 combination중 어떤 combination을 살펴봐야하는지 어떻게 알 수 있는가? . | spatial activation . 위의 질문에 대한 답은 activation을 시각화하는 것에 있다. 특정 input tensor에 대해서 activation되는 뉴런들의 combination을 시각화하는 것이다. . | . 위의 접근방법은 hidden layer를 다루는데 탁월하지만, 치명적인 결함이 있다. 하나의 input에 대해서만 시각화한다는 점이다. 이는 각 network의 전반적인 시각적 분석을 하기 힘들다는 뜻이다. . 해당 논문은 이런 문제의식을 가지고 “Activation Atlas”를 기획하였다. . 이런 global view를 얻기 위한 방법으로는 다음과 같은 방법이 있다. . CNN code visualization . | | . t-SNE 기반의 시각화 방법이다. 간략히 설명하면, 각 input tensor 혹은 activation value에 대해서 t-SNE로 mapping 시킬 좌표를 구하고 해당 좌표에 위와 같이 이미지를 시각화 하는 것이다. . | . activation atlas의 경우 위와 t-SNE와 유사한 방법을 이용하였으나, 주된 차이는 input tensor가 아닌 각 feature를 시각화하는 것에 있다. 각 feature를 위와 같이 시각화하면 feature간의 관계를 파악할 수 있다는 장점이 있다. . activation atlas는 . 각 feature의 관계를 잘 파악할 수 있다는 강점을 가지고 있으나, | data distribution에 영향을 받는다는 단점 또한 가지고 있다. | . Looking at single images . activation atlas를 살펴보기전에 activation vector를 시각화하는 spatial activation부터 살펴볼 것이다. 사용할 모델은 InceptionV1이이며, 시각화 과정은 다음과 같다. . feed the image into InceptionV1 . | collect activations . 여기서 수집한 activation은 단순한 vector이기 때문에 인간의 눈으로 해석하기 힘들다. 여기서 feature visualization이 필요하다. 단순하게 생각하면, feature visualization은 model이 생각하는 특정 activation vector를 생성하는 image를 시각화한 것이다. 일반적으로 image를 activation vector로 바꾸는 흐름과 다르게 activation atlas에서는 activation vector에서 image를 재현하는 흐름으로 간다고 생각하면 된다. . | InceptionV1은 convolution layers로 이루어져 있으므로, 각 layer마다 복수의 activation vector가 존재한다. (Filter의 수만큼) 또한 아래의 이미지 처럼 하나의 뉴런이 각 patch를 이동하면서 activation vector를 생성한다. (Parameter-sharing) . . 그러므로, network에 input image를 넣으면 하나의 뉴런은 많은 수의 evaluation을 받는다. 우리는 이를 각 뉴런이 각 patch에 대해서 얼마나 활성화됐는지 평가할 수 있다. . . . Aggregating Multiple Images . 위의 방법론은 single image에 대해서만 접근한 것이다. 하지만, global view를 얻고 싶다면 어떻게 해야할까? . 모든 이미지에 대해서 위의 방법론을 적용할 수도 있겠으나, 그러한 방법은 scale-up할 수 없으며 인간의 두뇌는 구조적인 정리없이 수많은 이미지를 모두 인지할 수 없다. . 우선, 먼저 수많은 이미지로부터 activation value를 수집해보자. 이는 위와 동일한 방법을 반복하면 된다. 수집한 activation은 위와 동일하게 feature visualization을 적용한다. . 이렇게 수집된 vector는 high-dimension(512 dim)의 성격을 가진다. 이를 dimensionality reduction방법론을 적용해서 2차원으로 mapping 하면 아래의 이미지처럼 나타나게 된다. . . Feature visualization을 적용할 때, regularization을 사용하였다.(ex: transformation robustness) . 다른 objective를 사용하기도 하였다. activation space $v$ 를 시각화 하기 위해서, point $x, y$의 activation vector $h_{x,y}$ 를 dot product하였다. ( $h_{x, y} cdot v$) . 해당 논문은 dot product에 cosine similarity를 곱하여 anlge을 강조하는 것이 효과적이라는 것을 발견하였다. (hx,y.v)n+1(∥hx,y∥⋅∥v∥)n+1 frac{(h_{x,y} .v)^{n+1}}{( rVert h_{x,y} rVert cdot rVert v rVert)^{n+1}}(∥hx,y​∥⋅∥v∥)n+1(hx,y​.v)n+1​ . We also find that whitening the activation space to unstretch it can help improve feature visualization. ??? . cosine similarity similarity=cos⁡(θ)=A⋅B∥A∥∥B∥similarity = cos( theta)= frac{A cdot B}{ rVert A rVert rVert B rVert}similarity=cos(θ)=∥A∥∥B∥A⋅B​ . 각 activation vector마다 attribution vector를 구할 수 있다. attribution vector란, 각 calss에 대한 항목이 있으며 각 class의 logit에 영향을 받은 activation vector의 값을 근사한다. attribution vector는 주변 contex에 영향을 받는다. hx,y⋅∇hxylogitch_{x, y} cdot nabla_{h_{x_y}}logit_chx,y​⋅∇hxy​​​logitc​ . Class c logit: $logit_c$ | 해당 수식은 뉴런이 logit에 미치는 영향을 측정하는 것 | GradCam과 유사하지만, gradient spatial averaging을 사용하지 않고 gradient의 noise를 continuous relaxtion을 통해서 감소 시켰다. | Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/AttrSpatial.ipynb | . . 위에서 보이는 이미지는 오른쪽의 feature space에 상단 좌측에 위치한 average attribution을 시각화 한 것이다. 위의 이미지는 모두 조금씩 다르지만, 비슷한 류의 동물의 형상을 하고 있다. 특히, 눈, 털, 코 등의 특징을 잡아내고 있다. 주의 할 점은 앞단의 레이어에서 실행하면, 상당히 혼란스러울수 있다는 점이다. (앞단의 레이어에서는 위와 같은 특징을 못잡을 수도 있다.) . . 위의 이미지는 좌측 하단의 위치한 average attribution vector이다. 위의 이미지와 다르게 바다 해변의 형상을 가지고 있다. . seashore class를 확인하기 위한 activation이 starfish나 sealion과 같은 class를 확인하는데도 쓰이는 것을 확인할 수 있었다. . 위의 두 사례를 보았을 때, 해당 activation atlas가 유의미한 2차원 좌표(semantic)를 가지고 있음을 확인할 수 있다. . Looking at Multiple Layers . 위에서는 하나의 레이어에서 다른 object가 어떻게 시각화되는지 확인하였다면, 이번 세션에서는 유사한 object에 대해서 서로 다른 layer에서 어떻게 나타타는지 알아볼 것이다. . 사용할 레이어는 다음과 같다.(강조된 부분) . . . 위의 이미지는 cabbage class를 시각화한 것이다. 왼쪽에서 오른쪽으로 갈 수록 더 cabbage처럼 구체적이고 복잡해지는 것을 확인할 수 있다. 이는 해당연구에서 기대했던 바인데 이유는 다음과 같다. . 뒷 단의 레이어일수록 receptive field가 크기 때문 | . . 위의 이미지는 sand와 water 그리고 sandbar의 이미지의 activation value를 나타낸 것이다. sandbar를 보면, 앞의 두 이미지를 합친 것과 유사해 보인다. . Focusing on a Single Classification . 이제부터는 network가 classification하는 것에 대해서 살펴볼 차례이다. . 예를 들어서, network가 어떤 과정을 거쳐서 ‘fireboat’라는 class로 결정하는지 살펴볼 것이다. . . 먼저 last layer(mixed5b)를 살펴볼 것이다. 뚜렷하게 보이는 부분일수록 ‘fireboat’로 결정하는데 큰 기여를 한 activation이다. classification 전의 layer이기 때문에 ‘fireboat’와 매우 유사한 이미지들이 진하게 보인다는 것을 확인 할 수 있다. . . 아래의 이미지는 mixed4d의 이미지로 여러가지 부분의 조합으로 ‘fireboat’로 인식하고 있음을 확인할 수 있다. 각 부분들은 fireboat와 유사해 보이지 않지만, 위의 fireboat사진을 보면 이렇게 인식하는 이유를 이해할 수 있다. . fireboat를 보면 창문 + 기중기 + 물로 이루어져 있음을 알 수 있다. 해당 부분도 물, 기중기, 창문들로 이루어져 있다. . . 이러한 특성은 ‘fireboat’와 ‘streetcar’와 비교해보면 잘 알 수 있다. (조금 유사하지만 다른 object) . . 해당 이미지를 보면 streetcar는 기중기나 물에서는 약한 activation을 가지고 있으나 창문과 집에 대한 activation에서는 매우 강한 activation을 가진다. 반대로 fireboat는 물과 기중기, 창문에서는 강한 activation을 가지지만 집에 대한 activation에서는 약한 activation을 가지고 있음을 확인할 수 있다. . Further Isolating Classes . 특정 class에 기여하는 activation만을 확인하고 싶다면, 다른 activation을 완전히 제외할 수 있다. 이를 class-specific activation이라고 한다. . . 스노쿨링 이미지 . | Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/activation-atlas/class-activation-atlas.ipynb . | . class activation atlas는 특정 class에 대해서 어떤 detector가 더 많은 기여를 했는지 명확하게 보여준다. 위의 스노쿨링 예시에서 강한 attribution만 보여주는 것이 아니라, strength가 약하더라도 해당 class에 전반적인 영향을 끼친 attribution도 보여준다. 특정 경우 우리가 보고 싶어하는 object와 매우 강하게 상관관계가 있는 object가 있다. (스노쿨러 - 물고기) 물고기는 우리가 보고 싶어하는 부분과 다른 부분이다. 따라서 적절한 filtering 방법이 필요하다 . . 위의 이미지 다른 필터링을 적용한 것이다. 위의 설명을 참고 바란다. . 이제는 유사한 두 클레스를 비교해볼 것이다. (magnitude 기준으로) . . 위의 이미지를 보면 두 클레스를 구분하기 힘들것이다. 아래의 이미지를 보면 도움이 될 것이다. . . To help make the comparison easier, we can combine the two views into one. We’ll plot the difference between the attributions of the “snorkel” and “scuba diver” horizontally, and use t-SNE to cluster similar activations vertically. . 위에 주목할 점은 locomotive(기관차)가 스쿠버 다이버와 연관이 깊게 나온다는 것이다. 이를 바탕으로 다음과 같은 실험을 진행하였다. . 스노쿨링 이미지에 조금씩 기관차 이미지를 사이즈 업하여 더한 것이다. . . 해당 이미지를 보면 조금 더 하면 스쿠버 다이버의 softmax값이 올라가나 일정수준이 넘으면 기관차로 인식함을 알 수 있다. 아마도 기관차의 스팀이 그런역할을 한 것으로 보이며 이와 같은 feature를 multi-use feature라고 칭한다.(시각적으로 유사해 보여도 서로 다른 시각적으로 다른 class에 반응) . 위와 같은 실험을 attack의 개념으로 1000여번을 진행했다. . . . . 위의 공격은 모든 클레스에 대해서 효과적인것은 아니었으나, 다섯개의 이미지에서 2개 정도로 target image로 인식하게 만들 수 있었다. . Conclusion . Surfacing Inner Properties of Models . | New interfaces . using AI to augment Human intelligence: 인간지능 보조 | 이미지의 알파벳처럼 activation을 조합할 수 있다. | classification 모델을 generative model처럼 … | Style transfer | Query large image datasets | Histogram | 새로운 데이터 셋 탐색 | . | . reference . https://distill.pub/2019/activation-atlas/ | .",
            "url": "https://fastpages.fast.ai/deeplearning/interpretability/2019/08/26/activation-atlas.html",
            "relUrl": "/deeplearning/interpretability/2019/08/26/activation-atlas.html",
            "date": " • Aug 26, 2019"
        }
        
    
  
    
        ,"post17": {
            "title": "MixConv: Mixed Depthwise Convolutional Kernels",
            "content": "“MixConv: Mixed Depthwise Convolutional Kernels” . Abstract . 다양한 kernel size의 효과 다양한 kernel size의 조합은 모델의 accuracy 및 efficient를 향상 시킬 수 있다. | . | 위를 바탕으로 depth-wise convolution(MixConv)제안 | MixConv의 효과를 증명하기 위해서 AutoML을 결합 | . Introduction . . Figure 1에서 볼 수 있듯이, kernel size가 클수록 모델의 성능이 올라간다고 할 수 없다. 올라가는 추세를 보이다가 k9*9를 넘어서면 accuracy가 떨어지는 것을 확인할 수 있다. . ConvNets 연구에서는 하나의 kernel size의 한계를 말한다. 결국 high-resolustion pattern을 위해서는 큰 kernel size가 필요하고 local-resolution pattern을 위해서는 작은 kernel-size가 필요하다. . 아래는 해당 논문의 MixConv의 개략적인 구조이다. . . Related Work . Efficient ConvNets . | Multi-Scale Networks and Features . 이전의 연구들은 모델 구조 자체를 변화시킴 | MixConv는 모델의 구조는 그대로 유지한채로 kernel size를 변화시키는 것이 목표 | . | Neural Arichitecture Search . | . MixConv . MixConv는 하나의 depth wise convolution op에서 multiple kernel을 섞는 것이다. 기대효과는 다양한 타입의 pattern을 수집하는 것이다. . 3.1 MixConv Feature Map . def mixconv(x, filters, **args): # x: input features with shape [N,H,W,C] # filters: a list of filters with shape [K_i, K_i, C_i, M_i] for i−th group. G = len(filters) # number of groups. y = [] for xi, fi in zip(tf.split(x, G, axis=−1), filters): y.append(tf.nn.depthwise_conv2d(xi, fi, ∗∗args)) return tf.concat(y, axis=−1) . $X^{(h, w, c)}$: input tensor with shape (h, w, c) h: height, w: width, c: channel | . | $W^{(k, k, c, m)}$ : depth wise convolution kernel $k times k$ : kernel size | $c$: input channel size | $m$: output channel size | . | $Y^{(h, w, c*m)}$: same spatial shape (h, w), multiplied output channel size $m cdot c$ | . Yx,y,z=∑−k2≤i≤k2,−k2≤j≤k2Xx+i,y+j,zm⋅Wi,j,z   ∀z=1,⋯ ,m⋅cY_{x, y, z} = sum_{- frac{k}{2} le i le frac{k}{2}, - frac{k}{2} le j le frac{k}{2}} X_{x+i, y+j, frac{z}{m}} cdot W_{i, j, z} forall z=1, cdots, m cdot cYx,y,z​=−2k​≤i≤2k​,−2k​≤j≤2k​∑​Xx+i,y+j,mz​​⋅Wi,j,z​   ∀z=1,⋯,m⋅c . 위의 식은 MixConv과 적용되는 과정을 수식화한 것이다. Input tensor를 g개의 그룹으로 분리하면 아래와 같다. . &lt;X^(h,w,c1),⋯ ,X^(h,w,cg)&gt;&lt; hat{X}^{(h,w,c_1)}, cdots, hat{X}^{(h,w,c_g)}&gt;&lt;X^(h,w,c1​),⋯,X^(h,w,cg​)&gt; . 각 tensor의 spatial height와 width는 모두 동일하며, 각 channel을 모두 더하면 다음과 같다. . c1+c2+⋯+cg=cc_1 + c_2 + cdots + c_g = cc1​+c2​+⋯+cg​=c . 또한 g개의 kernel group을 구성할 수 있다. . &lt;W^(k1,k1,c1,m),⋯ ,W^(kg,kg,cg,m)&gt;&lt; hat{W}^{(k_1,k_1,c_1, m)}, cdots, hat{W}^{(k_g,k_g,c_g, m)}&gt;&lt;W^(k1​,k1​,c1​,m),⋯,W^(kg​,kg​,cg​,m)&gt; . t-th group output은 다음과 같이 구성된다. . Yt^x,y,z=∑−kt2≤i≤kt2,−kt2≤j≤kt2Xt^x+i,y+i,zm⋅Wt^i,j,z   ∀z=1,⋯ ,m⋅ct hat{Y^t}_{x, y, z} = sum_{- frac{k_t}{2} le i le frac{k_t}{2}, - frac{k_t}{2} le j le frac{k_t}{2}} hat{X^t}_{x+i, y+i, frac{z}{m}} cdot hat{W^t}_{i, j, z} forall z=1, cdots, m cdot c_tYt^x,y,z​=−2kt​​≤i≤2kt​​,−2kt​​≤j≤2kt​​∑​Xt^x+i,y+i,mz​​⋅Wt^i,j,z​   ∀z=1,⋯,m⋅ct​ . 그리고 final output tensor는 아래와 같다. . Yx,y,z0=Concat(Y1^x,y,z1,⋯ ,Yg^x,y,zg)Y_{x, y, z_0}=Concat( hat{Y^1}_{x,y,z_1}, cdots, hat{Y^g}_{x,y,z_g})Yx,y,z0​​=Concat(Y1^x,y,z1​​,⋯,Yg^x,y,zg​​)` . 3.2 MixConv Design Choices . Group size . 하나의 input tensor에 얼마나 다양한 size의 kernel을 사용할 것인지 정해야 한다. . group이 하나라면 vanilla depth wise convolution과 같고, 해당 논문은 실험을 통해서 MobileNets에서는 $g=4$ 가 안정적으로 적용됨을 확인하였다. 하지만, neural architecture search를 통해서 group size가 1에서 5까지의 범위에서 변하면 더 좋은 성능을 낼 수 있음을 확인하였다. . | Kenel size per group . 다른 그룹간의 같은 kernel size을 사용한다면, 그룹을 분리하는 의미가 없다. 따라서 해당 논문에서는 다른 그룹은 다른 kernel size를 가지도록 제한하였다. . | channel size per group . 해당 논문에서는 두 가지 channel 분리 방법을 사용하였다. . equal parition: (8, 8, 8, 8) | exponential parition: (16, 8, 4, 4) | | dilated convolution . large size의 kernel은 많은 parameter와 computation을 필요로 한다. 따라서 이를 대처하기 위해서 dilated convolution을 사용하였다. . [Dilated convolution] . Dilated Convolution은 필터 내부에 zero padding을 추가해 강제로 receptive field를 늘리는 방법이다. 위 그림은 파란색이 인풋, 초록색이 아웃풋인데, 진한 파랑 부분에만 weight가 있고 나머지 부분은 0으로 채워진다. . 출처: https://3months.tistory.com/213 [Deep Play] . . | . 3.3 MixConv Performance on Mobile Nets . . . 3. 4 Ablation Study . ablation study . 모델이나 알고리즘의 특징들을 제거하면서 그게 퍼포먼스에 어떤 영향을 줄지 연구하는 거 . MixConv for Single Layer | . . ​ 하나의 layer씩 변화시켜본 결과는 위의 Figure 5와 같다. 2(s2)-(with large kernel size + stride2)를 보면 accuracy가 상승한 것을 확인할 수 있으며, 대부분의 레이어에서 비슷하거나 조금의 성능 향상이 있었다. . Channel Partition Method/ Dilated Convolution . . | . Figure 6에서 확인할 수 있듯이, dilated convolution은 parameter size가 커질 때 급격한 성능 하락이 있다. 해당 논문은 이는 local information을 잃어버리기 때문이라고 주장한다. . FLOPS . 플롭스(FLOPS, FLoating point OPerations per Second)는 컴퓨터의 성능을 수치로 나타낼 때 주로 사용되는 단위이다. . https://ko.wikipedia.org/wiki/%ED%94%8C%EB%A1%AD%EC%8A%A4 . MixNet . 4.1 Architecture Search . 이전의 Architecture Search 연구에서는 kernel size, expansion ratio, channel size등을 고려했다면, 해당 논문에서는 vanilla depth wise convolution을 baseline으로 삼고 MixConv를 search option으로 지정하였다. . MixConv 옵션으로는 $g=1, cdots, 5$ 가 있다. | search option을 단순화 시키기 위해서 exponential partion은 제외 | . 4.2 MixNet Performance on ImageNet . . . 4.3 MixNet Architectures . accuracy와 efficiency 향상의 이유를 알기 위해서 network architecture를 분석하였다. . 전반적으로 MixNet모델들은 다양한 크기의 kernel을 사용하였다. . small kernel은 앞단의 stage에서 computation cost를 줄이기 위하여 사용함 | large kernel은 뒷단의 stage에서 더 좋은 accuracy를 위해서 많이 나타남. | | . 또한, 큰 MixNet model일수록 parameter와 FLOPS를 비용으로 지불하면서 더 큰 size의 kernel을 많이 사용하고 더 많은 수의 layer를 사용하였다. . Figure 1을 보면 vanilla depthwise convolution은 일정크기 이상의 kernel을 사용하면 급격한 성능하락이 있었지만, MixConv는 large size의 kernel을 사용할 수 있었다. . 4.4 Transfer Learning Performance . | . Figure 9는 transfer learning을 MixNet에 적용하였을 때, 성능향상을 나타내는 이미지이다. MixNet-M은 97.92%의 성능향상을 했으며 이는 ResNet-50보다 1% 높은 수치이다. . 실험은 imagenet으로 pretrained된 모델을 CIFAR10, 100에 적용한 것이다. . Reference . https://arxiv.org/pdf/1907.09595.pdf | .",
            "url": "https://fastpages.fast.ai/deeplearning/2019/08/23/mixconv.html",
            "relUrl": "/deeplearning/2019/08/23/mixconv.html",
            "date": " • Aug 23, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "Python 변수할당의 개념",
            "content": "“Python 변수할당의 개념” . Python은 C언어와 다르게 Pointer가 존재하지 않는다. . Pointer란 . 프로그래밍 언어에서 다른 변수, 혹은 그 변수의 메모리 공간주소를 가리키는 변수를 말한다. 포인터가 가리키는 값을 가져오는 것을 역참조라고 한다. . . python에서는 메모리 주소를 변수를 구별하는 용도로 사용한다. . 참고로 변수의 메모리 주소는 id()를 이용하여 구할 수 있다. . a = &quot;String&quot; id(a) . python에서는 변수에 값을 할당하면 object를 생성해서 값을 저장한 후 변수는 해당 object의 메모리 주소를 의미하는 id값을 가지게 된다. 여기서 변수는 object의 label정도로 생각하면 된다. . . &gt;&gt;&gt; a = 1 &gt;&gt;&gt; id(a) 23282008 . 다음과 같은 두 가지 상황을 생각해 볼 수 있다. . 같은 object 값을 가지고 같은 메모리 주소를 가지는 경우 . &gt;&gt;&gt; a = b = 100 &gt;&gt;&gt; id(a) 23283616 &gt;&gt;&gt; id(b) 23283616 . 이 경우 변수 a를 변화시킨다면, 주소값도 같이 변한다. . &gt;&gt;&gt; a += 1 &gt;&gt;&gt; a 101 &gt;&gt;&gt; id(a) 140706122743760 &gt;&gt;&gt; b 100 &gt;&gt;&gt; id(b) 140706122743728 # b의 주소값은 변하지 않는다. &gt;&gt;&gt; b += 1 &gt;&gt;&gt; id(b) 140706122743760 # a와 같은 주소값을 가진다. &gt;&gt;&gt; b -= 1 &gt;&gt;&gt; id(b) 140706122743728 . | 같은 object 값을 가지고 다른 메모리 주소를 가지는 경우: 같은 주소값을 가질수도 안가질 수도 있다. . &gt;&gt;&gt; a = 12345 &gt;&gt;&gt; b = 12345 &gt;&gt;&gt; id(a) 23564368 &gt;&gt;&gt; id(b) 23564584 . 따라서 두 변수를 비교할 때는 해당 값을 비교하는 것인지, object의 id를 가르키는 것인지 명확히 해야한다. . &gt;&gt;&gt; a = 12345 &gt;&gt;&gt; b = 12345 &gt;&gt;&gt; id(a) 23564368 &gt;&gt;&gt; id(b) 23564584 &gt;&gt;&gt; a is b # object id 비교 False &gt;&gt;&gt; a == b # object value 비교 True . | . 정체성 , 동질성 . 동일한 객체를 참조하는 사례: dict 자료형 . dict 자료형의 경우, 변수 선언을 기존의 dict 형 변수로 할 경우 동일한 object를 참조하게 되며, 하나의 값이 변하면, 다른 하나의 변수도 같이 변하게 된다. . &gt;&gt;&gt; A = {&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3} &gt;&gt;&gt; B = A &gt;&gt;&gt; A is B # 같은 object True &gt;&gt;&gt; A == B True &gt;&gt;&gt; A[&#39;d&#39;] = 4 # A에 key &#39;d&#39; 추가 &gt;&gt;&gt; B # A의 변화가 B에도 반영 {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 4} . | 위의 두 변수를 독립적으로 사용하기 위해서는 아래와 같이 서로 다른 object를 참조해야 한다. . &gt;&gt;&gt; A = {&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3} &gt;&gt;&gt; B = {&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3} &gt;&gt;&gt; A is B # 서로 다른 object 참조 False &gt;&gt;&gt; A == B True . | . Reference . https://technote.kr/185 | https://technote.kr/289 | https://zzsza.github.io/development/2018/08/25/python-object/ | .",
            "url": "https://fastpages.fast.ai/python/pointer/2019/08/03/Python-%EB%B3%80%EC%88%98%ED%95%A0%EB%8B%B9%EC%9D%98-%EA%B0%9C%EB%85%90.html",
            "relUrl": "/python/pointer/2019/08/03/Python-%EB%B3%80%EC%88%98%ED%95%A0%EB%8B%B9%EC%9D%98-%EA%B0%9C%EB%85%90.html",
            "date": " • Aug 3, 2019"
        }
        
    
  
    
        ,"post19": {
            "title": "Image segmentation에서 사용되는 loss",
            "content": "“Image segmentation에서 사용되는 loss” . Basic: Cross Entropy . 일반적으로 classification문제에서 사용되는 term이다. CE(p,p^)=−(plog⁡(p^)+(1−p)log⁡(1−p^))CE(p, hat{p}) = -(p log( hat{p}) + (1- p) log(1- hat{p}))CE(p,p^​)=−(plog(p^​)+(1−p)log(1−p^​)) . Weighted cross entropy . WCE(p,p^)=−(βplog⁡(p^)+(1−p)log⁡(1−p^))WCE(p, hat{p}) = -( beta p log( hat{p}) + (1- p) log(1- hat{p}))WCE(p,p^​)=−(βplog(p^​)+(1−p)log(1−p^​)) . 위의 cross entropy의 변형으로 모든 positive samples는 가중치를 부여 받는다. 이는 class imbalace문제를 다루기 위해서 사용한다. 만약 positive sample이 is 90%고 negative sample이 10%라면 cross entropy는 잘 작동하기 힘들다. 위와 같은 상황에서는 positive sample에 $ beta$ 를 1보다 작은 값을 부여하여 해결하고 만약 negative sample이 더 많다면 반대로 $ beta$ 에 1보다 큰 값을 부여하여 해결한다. . Balanced cross entropy . 위의 WCE와 유사한 방법론이다. 차이점은 negative sample에도 특정 가중치를 곱하는데에 있다. BCE(p,p^)=−(βplog⁡(p^)+(1−β)(1−p)log⁡(1−p^))BCE(p, hat{p}) = -( beta p log( hat{p}) + (1- beta)(1- p) log(1- hat{p}))BCE(p,p^​)=−(βplog(p^​)+(1−β)(1−p)log(1−p^​)) 주의할 점은 만약 $ beta$가 1의 값을 가진다면 negative sample을 전혀 고려하지 못하게 된다. 이는 $ beta$ 가 fixed value가 아닐때 발생하는데, 예시는 다음과 같다. . beta = tf.reduce_sum(1 - y_true) / (BATCH_SIZE * HEIGHT * WIDTH) . 이러한 문제는 small value $ epsilon$ 을 임의로 더해주거나 tf.clip_by_value를 이용하여 해결한다. . Focal loss . 주된 목적은 쉬운 example에 대해서는 down-weight를 부여하여 모델이 조금 더 어려운 example에 집중하는 것이다. FL(p,p^)=−(α(1−p^)γplog⁡(p^)+(1−α)p^γ(1−p)log⁡(1−p^))FL(p, hat{p}) = -( alpha(1- hat{p}) ^ gamma p log( hat{p}) + (1- alpha) hat{p}^ gamma(1- p) log(1- hat{p}))FL(p,p^​)=−(α(1−p^​)γplog(p^​)+(1−α)p^​γ(1−p)log(1−p^​)) . 만약 $ gamma$ 가 0이라면, BCE를 얻게 된다. . 여기서 어려운 exapmle이란, 모델이 예측하기 어려운 class를 의미한다. . | . ## Distance to the nearest cell cross entropy 개념에 거리 개념을 더하여 모델이 붙어있는 object에 대해서 더 잘 구별하도록 하는 것이 목적이다. $$ BCE(p, hat{p}) + w_0 cdot exp(- frac{(d_1(x) + d_2(x))^2}{2 sigma^2}) $$ - $d_1(x), d_2(x)$ 는 가장 가까운 cell과의 거리, 두 번째로 가까운 cell과의 거리를 나타낸다. 거리가 멀어질 수록 total loss는 줄어들고, 거리가 가까울 수록 total loss가 증가하게 된다. - 여기서 cell은 다른 class를 의미한다. # Overlap measures ## Dice Loss /F1 score dice coefficient는 IOU와 유사한 term이다. $$ DC = frac{2TP}{2TP + FP + FN} = frac{2 rvert X cap Y rvert}{|X| + |Y|} $$ $$ IOU = frac{TP}{TP + FP +FN} = frac{ rvert X cap Y rvert}{|X| + |Y| - rvert X cap Y rvert} $$ ​ 여기서 $DC ge IOU$ 라는 것을 알 수 있다. Dice coefficient는 loss function으로 정의될 수 있다. $$ DL(p, hat{p}) = frac{2&lt;p, hat{p}&gt;}{ rVert p rVert_2^2 + rVert hat{p} rVert_2^2} $$ ~~~python def dice_loss(y_true, y_pred): numerator = 2 * tf.reduce_sum(y_true * y_pred) # some implementations don&#39;t square y_pred denominator = tf.reduce_sum(y_true + tf.square(y_pred)) return numerator / (denominator + tf.keras.backend.epsilon() ~~~ ## Tversky loss dice loss의 generalization이다. FP와 FN에 weight를 곱하는 것이다. $$ DL(p, hat{p}) = frac{2 &lt;p, hat{p}&gt;}{2&lt;p, hat{p}&gt; + &lt;1-p, hat{p}&gt; + &lt;p, 1 - hat{p}&gt;} $$ $$ TL(p, hat{p}) = frac{2 &lt;p, hat{p}&gt;}{2&lt;p, hat{p}&gt; + beta&lt;1-p, hat{p}&gt; + (1- beta)&lt;p, 1 - hat{p}&gt;} $$ ## Lovasz-softmax DL과 TL은 $ hat{p} in { 0, 1 } ^n$ 이라는 강한 제한을 둔다. 하지만 Lovasz-softmax에서는 surrogate loss function을 사용한다. &gt; https://github.com/bermanmaxim/LovaszSoftmax/blob/master/tensorflow/lovasz_losses_tf.py #### Reference - https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/#references",
            "url": "https://fastpages.fast.ai/2019/08/01/image-segmentation-loss.html",
            "relUrl": "/2019/08/01/image-segmentation-loss.html",
            "date": " • Aug 1, 2019"
        }
        
    
  
    
        ,"post20": {
            "title": "Dropout as Bayesian Approximation 정리글",
            "content": "Dropout as Bayesian Approximation 정리글 . Problem . 일반적으로 Bayesain model은 model의 uncertainty를 측정할 수 있다는 장점이 있지만, computation cost가 너무 커서 사용하기 힘들다는 문제를 가지고 있다. 이런 문제점을 해결하기 위해서 이 논문에서는 Dropout을 사용한 딥러닝 모델이 결국은 gaussian porcess에서의 bayesain inference를 근사한 것이라는 증명을 할 것이다. . Related Research . Bayesian learning for neural networks . infinite-wide neural network에 distribution을 가정하면, 결국 gaussian process를 approxiation하는 것이다. 하지만 finite-wide neural network에서는 증명되지 않았다. . 추가적으로 finite-wide neural network상에서 연구되었다. BNN 조건에서 overfitting문제에 견고했지만, computation cost 높다는 문제가 있었다. . | variational inference . BNN에서 variational inference가 적용되었다. 하지만, 부분적인 성과만 있었다. . | sampling-based variational inference . 위의 VI를 개선하기 위해서 sampling 기반의 방법론이 등장하였다. 이 방법론은 dropout만큼 성공적이였으나 computation cost가 매우 높았다. uncertainty를 측정하기 위해서 parameter가 기존의 방법론 대비 두 배가 필요했다. 이는 분포를 정의하기 위해서 평균값과 분산값을 정의해야 되기 때문이다. 게다가 수렴하는 시간도 오래걸렸으며, 기존의 방법론 대비 효과적이지도 않았다. . | . Dropout as a Bayesian Approximation . 이 section에서는 Dropout이 결국에는 Deep gaussian process를 근사한다는 것을 수학적으로 증명할 것이다. 특히, 어떠한 가정도 없이 증명할 수 있으며, 어떤 network에도 적용가능하다는 장점을 가지고 있다. . Dropout objective는 approximation distribution과 deep gaussian process의 posterior간의 Kl-divergence를 감소시킨다. . 우선 Dropout objective를 확인해보자. ( add regularization) . Ldropout=1N∑i=1NE(yi,y^i)+λ∑i=1N(∥Wi∥22+∥b∥22) mathcal{L}_{dropout} = frac{1}{N} sum_{i=1}^N E(y_i, hat{y}_i) + lambda sum_{i=1}^N ( rVert W_i rVert_2 ^ 2+ rVert b rVert_2 ^ 2)Ldropout​=N1​i=1∑N​E(yi​,y^​i​)+λi=1∑N​(∥Wi​∥22​+∥b∥22​) . dropout은 결국 모든 input point와 각 layer의 모든 network에 binary distribution을 적용한 것으로 해석할 수있다. 참고로 output에는 적용하지 않는다. 이는 아래의 이미지 처럼 적용될 수 있다. . . GP 모델에서 predictive probability는 아래와 같이 전개된다. ($x^*$ is unseen) . p(y∣x∗,X,Y)=∫p(y∣x∗,w)p(w∣X,Y)dwp(y mid x^*, X, Y) = int p(y mid x^*, w) p(w mid X, Y) dwp(y∣x∗,X,Y)=∫p(y∣x∗,w)p(w∣X,Y)dw . p(y∣x,w)=N(y;y^(x,w),τ−1ID)p(y mid x, w) = mathcal{N}(y; hat{y}(x, w), tau^{-1}I_D)p(y∣x,w)=N(y;y^​(x,w),τ−1ID​) . y^(x,w={W1,⋯ ,WL})=1KLWLσ(⋯1K1W2σ(W1x+m1)⋯ ) hat{y}(x, w= { W_1, cdots, W_L }) = sqrt frac{1}{K_L}W_L sigma( cdots sqrt frac{1}{K_1}W_2 sigma(W_1x + m_1) cdots)y^​(x,w={W1​,⋯,WL​})=KL​1​ . ​WL​σ(⋯K1​1​ . ​W2​σ(W1​x+m1​)⋯) . 여기서 posterior $p(w mid X, Y)$가 untractable한데 이를 해결하기 위해서 variational inference를 사용하며, simple distribution으로 $q(w)$를 가정한다. 이때 $q(w)$는 matrix형태를 가지고 있으며 random하게 0으로 값이 지정된다. (여기서 $K$ 는 matrix dimension을 의미한다. $W_i$ 의 dim은 $K_i * K_{i-1}$) . Wi=Mi⋅diag([zi,j]j=1Ki)W_i = M_i cdot diag([z_{i, j}]_{j=1}^{K_i})Wi​=Mi​⋅diag([zi,j​]j=1Ki​​) . Zi,j=Bernoulli(pi) for i=1,⋯ ,L, j=1,⋯ ,Ki−1Z_{i, j} = Bernoulli(p_i) for i = 1, cdots, L, j= 1, cdots, K_{i-1}Zi,j​=Bernoulli(pi​) for i=1,⋯,L, j=1,⋯,Ki−1​ . 여기서 $z_{i, j}$가 0이라면, layer $i - 1$ 의 unit $j$가 drop된다는 것을 의미한다. 위의 이미지에서는 layer에 dropout을 설정한 것을 시각화 한것이라면 위의 수식은 parameter자체에 dropout을 걸었다는 차이가 있다. (그리고 BNN을 적용하기 위해서는 위의 수식처럼 parameter 자체에 dropout을 설정하는 것이 타당하다고 생각한다.) . variational distribution $q(w)$는 highly multi modal한 특징을 가지고 있다. 왜냐하면, 각 layer에 대한 Bernoulli distribution의 output값이 layer의 크기 만큼 나와야 하기 때문이다. . $q(w)$를 바탕으로 objective를 도출하면 아래와 같다. (lower bound: variational inference 참고) . −∫q(w)log⁡p(Y∣X,w)dw+KL(q(w)∥p(w)).- int q(w) log p(Y mid X, w) dw +KL(q(w) rVert p(w)).−∫q(w)logp(Y∣X,w)dw+KL(q(w)∥p(w)). . $p(w)$ 는 prior distribution을 의미한다. | . 첫번째 term $- int q(w) log p(Y mid X, w) dw$은 아래처럼 바꿀 수 있다. . −∫q(w)log⁡p(Y∣X,w)dw≈−∑n=1N∫q(w)log⁡p(yn∣xn,w)dw- int q(w) log p(Y mid X, w) dw approx- sum_{n=1} ^N int q(w) log p(y_n mid x_n, w) dw−∫q(w)logp(Y∣X,w)dw≈−n=1∑N​∫q(w)logp(yn​∣xn​,w)dw . 두번째 term $ KL(q(w) rVert p(w))$에서는 아래와 같은 식을 얻을 수 있다. . ∑i=1L(pil22∥Mi∥22+l22∥mi∥22) sum_{i=1}^ L( frac{p_il^2}{2} rVert M_i rVert_2^2 + frac{l^2}{2} rVert m_i rVert_2^2)i=1∑L​(2pi​l2​∥Mi​∥22​+2l2​∥mi​∥22​) . $p_i$ bernoulli 분포의 확률값 | $l$ 은 prior length scale 값: appendix section 4.2 prior distribution에 대한 가정 | . | . appendix section 4.2 KL(q(w)∥p(w))≈∑i=1Lpi2(uiTui+tr(Σi)−K(1+log⁡2π)−log⁡∣Σi∣−C)KL(q(w) rVert p(w)) approx sum_{i=1}^L frac{p_i}{2}(u_i^Tu_i + tr( Sigma_i) - K(1 + log2 pi) - log rvert Sigma_i rvert - C)KL(q(w)∥p(w))≈∑i=1L​2pi​​(uiT​ui​+tr(Σi​)−K(1+log2π)−log∣Σi​∣−C) . model precision $ tau$를 고려하면, 아래와 같이 scale한 값이 도출된다. . LGP−MC∝1N∑i=1N−log⁡p(yn∣xn,wn^)τ+∑i=1L(pil22τN∥Mi∥22+l22τN∥mi∥22) mathcal{L}_{GP-MC} propto frac {1}{N} sum_{i=1}^N frac{- log p (y_n mid x_n, hat{w_n})}{ tau} + sum_{i=1}^{L}( frac{p_i l^2}{2 tau N} rVert M_i rVert_2^2 + frac{l^2}{2 tau N} rVert m_i rVert_2^2)LGP−MC​∝N1​i=1∑N​τ−logp(yn​∣xn​,wn​^​)​+i=1∑L​(2τNpi​l2​∥Mi​∥22​+2τNl2​∥mi​∥22​) . 여기서 $ tau$와 length-scale $l$은 hyperparameter이다. lengh-scale은 function frequency를 가정하는 것으로 만약 $l$을 강하게 준다면, regularization 효과는 더 강해진다. . λ1=l2p12Nτ lambda_1 = frac{l^2 p_1}{2N tau}λ1​=2Nτl2p1​​ . τ=l2p12Nλ1 tau = frac{l^2 p_1}{2N lambda_1}τ=2Nλ1​l2p1​​ . short length scale $l$ (high frequency data) + high precision $ tau$(small observation noise) result in small weight-decay $ lambda$ : 모델이 데이터에 더 잘 적합하게 된다. | long length scale $l$ (low frequency data) + low precision $ tau$ (large observation noise) result in large weight-decay | . Obtaining Model Uncertainty . predictive distributioin은 아래와 같이 주어진다. . q(y∗∣x∗)=∫p(y∗∣x∗,w)q(w)dwq(y^* mid x^*) = int p(y^* mid x^*, w) q(w) dwq(y∗∣x∗)=∫p(y∗∣x∗,w)q(w)dw . $w = { W_i}_{i=1} ^ L$ | unseen input data: $x^*$ | prediction from unseen input data: $y^*$ | . dropout을 이용하여, uncertainty를 estimate를 하기 위해서는 bernoulli distribution ${z_1^t, cdots, z_L^t}_{t=1}^T$를 sampling해주면 된다. 이 식에서는 T번의 sampling을 진행한 것이다. . sampling한 분포를 바탕으로 predictive mean값을 근사할 수 있다. 이를 MC-dropout이라고 부른다. . Eq(y∗∣x∗)(y∗)=1T∑i=1Ty^∗(x∗,W1t,⋯ ,WLt)E_{q(y^* mid x^*)}(y^*) = frac{1}{T} sum_{i=1}^T hat{y}^*(x^*, W_1^t, cdots, W_L^t)Eq(y∗∣x∗)​(y∗)=T1​i=1∑T​y^​∗(x∗,W1t​,⋯,WLt​) . 다음은 raw moment를 근사하는 과정이다. . Eq(y∗∣x∗)((y∗)Ty∗)≈τ−1ID+1T∑i=1Ty^∗(x∗,W1t,⋯ ,WLt)Ty^∗(x∗,W1t,⋯ ,WLt)E_{q(y^* mid x^*)}((y^*) ^T y^*) approx tau^{-1}I_D + frac{1}{T} sum_{i=1}^T hat{y}^*(x^*, W_1^t, cdots, W_L^t) ^ T hat{y}^*(x^*, W_1^t, cdots, W_L^t)Eq(y∗∣x∗)​((y∗)Ty∗)≈τ−1ID​+T1​i=1∑T​y^​∗(x∗,W1t​,⋯,WLt​)Ty^​∗(x∗,W1t​,⋯,WLt​) . predictive variance는 다음과 같이 도출된다. . Varq(y∗∣x∗)(y∗)≈τ−1ID+1T∑i=1Ty^∗(x∗,W1t,⋯ ,WLt)Ty^∗(x∗,W1t,⋯ ,WLt)−Eq(y∗∣x∗)(y∗)TEq(y∗∣x∗)(y∗)Var_{q(y^* mid x^*)}(y^*) approx tau^{-1}I_D + frac{1}{T} sum_{i=1}^T hat{y}^*(x^*, W_1^t, cdots, W_L^t) ^ T hat{y}^*(x^*, W_1^t, cdots, W_L^t) -E_{q(y^* mid x^*)}(y^*) ^T E_{q(y^* mid x^*)}(y^*)Varq(y∗∣x∗)​(y∗)≈τ−1ID​+T1​i=1∑T​y^​∗(x∗,W1t​,⋯,WLt​)Ty^​∗(x∗,W1t​,⋯,WLt​)−Eq(y∗∣x∗)​(y∗)TEq(y∗∣x∗)​(y∗) . 참고로 $y^$는 row vector를 의미하며 $ hat{y}^(x^, W_1^t, cdots, W_L^t) ^ T hat{y}^(x^*, W_1^t, cdots, W_L^t)$ 연산은 outer product이다. . weight-decay 값 $ lambda$와 length scale $l$이 주어지면 아래의 식으로 model precision $ tau$ 를 도출할 수 있다. . τ=l2p12Nλ1 tau = frac{l^2 p_1}{2N lambda_1}τ=2Nλ1​l2p1​​ . regression task에서 다음과 같이 predictive log-likelihood를 monte-carlo integration을 통해서 근사할 수 있다. 이를 통해서 model이 mean과 얼마나 일치하는지 uncertainty가 어떤지 알 수 있다. . with $w_t sim q(w)$ begin{align} log p(y^* mid x^*, X, Y) &amp;= log int p(y^* mid x^*, w) p(w midX, Y) dw &amp; approx log int p(y^* mid x^*, w) q(w) dw &amp; approx log frac{1}{T} sum_{t=1}^{T} p(y^* mid x^*, w_t) end{align} . At regression task, . log⁡p(y∗∣x∗,X,Y)≈logsumexp(−12τ∥y−y^∥2)−log⁡T−122π−12log⁡τ−1 log p(y^* mid x^*, X, Y) approx mathrm{logsumexp} ( - frac{1}{2} tau rVert y - hat{y} rVert^2) - log T - frac{1}{2}2 pi - frac{1}{2} log tau ^{-1}logp(y∗∣x∗,X,Y)≈logsumexp(−21​τ∥y−y^​∥2)−logT−21​2π−21​logτ−1 . $y$ : predictive mean | $ hat{y}$ : sample | . predictive distribution $q(y^* mid x^*)$은 highly multi modal이기 때문에 그 특성을 정확히 알 수 없다. 이는 weight element에 bi-modal한 distribution을 설정하였고 이들의 joint distribution은 multi modal이기 때문이다. . 하지만, 구현하기 매우 싶다. dropout을 수정하지 않고 사용하며, samples을 모아서 uncertainty를 측정할 수 있다. 또한 forward pass는 기존의 standard 한 모델과 차이가 나지 않는다. . Example code: image segmentaton . 아래는 test과정에서 predictive mean를 구하는 method의 예시이다. 주목할 점은 dropout을 낀채로 sampling을 진행해야 한다는 것이다. . def test_epistemic(model, test_loader, criterion, test_trials=20, epoch=1): &quot;&quot;&quot;Epistemic model Test Please turn on Dropout! model: pytorch model test_loader: test data loader crieterion: loss_fucntion Return test_loss, test_error &quot;&quot;&quot; model.train() # train mode: turn on dropout test_loss = 0 test_error = 0 for data, target in test_loader: if list(data.size())[0] != batch_size: break data = Variable(data.cuda(), volatile=True) target = Variable(target.cuda()) outputs = model(data)[0].data for i in range(test_trials - 1): # sampling outputs += model(data)[0].data output = outputs / test_trials # predictive mean pred = get_predictions(output) test_loss += criterion(output, target).data test_error += error(pred, target.data.cpu()) torch.cuda.empty_cache() test_loss /= len(test_loader) test_error /= len(test_loader) torch.cuda.empty_cache() return test_loss, test_error . 다음은 predictive variance를 구하는 과정이다. . def get_epistemic(outputs, predictive_mean, test_trials=20): result = torch.tensor( np.zeros((batch_size, img_shape[0], img_shape[1]), dtype=np.float32) ).cuda() target_sq = torch.einsum(&quot;bchw,bchw-&gt;bhw&quot;, [predictive_mean, predictive_mean]).data for i in range(test_trials): output_sq = torch.einsum( &quot;bchw,bchw-&gt;bhw&quot;, [outputs[i], outputs[i]] ).data result += output_sq - target_sq result /= test_trials return result . Reference . https://arxiv.org/abs/1506.02142 | .",
            "url": "https://fastpages.fast.ai/deeplearning/uncertainty/2019/08/01/Dropout-as-Bayesian-Approximation-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "relUrl": "/deeplearning/uncertainty/2019/08/01/Dropout-as-Bayesian-Approximation-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "date": " • Aug 1, 2019"
        }
        
    
  
    
        ,"post21": {
            "title": "mesh tensorflow 정리글",
            "content": "mesh tensorflow 정리글 . 요약 . batch-spliting이란, data-paralleism 방법론으로 분산화된 딥러닝 네트워크에서 많이 사용하며, Single-Program-Multiple-Data programing의 일종이다. 즉, 데이터가 클 때 분산시켜서 대처하는 방법론이라고 할 수 있다. . 하지만, 모델이 한번에 RAM에 올리기 클 경우에는 어떻게 해야할까? 혹은 모델의 크기 때문에 작은 batch size를 사용할 때 발생하는 high latency와 비효율성이 발생한다면 어떻게 해야할까? 이를 해결하기 위해서는 Model-parallenism을 사용해야한다. . 하지만, 효과적인 model-parallelism은 일반적으로 복잡한 편이다. 이런 문제를 간단하게 해결하기 위해서 Mesh-tensorflow를 제안한다. data-parallelism은 tensor와 operations를 batch dimension으로 나누는 것으로 치환한다. . Data-paralleism . 특징은 다음과 같다. . 각 core마다 복사되는 Parameters | core마다 분산되는 batch | sum(allreduce) parameters gradients | . 장점은 다음과 같다. . 보편적으로 사용되는 방식이다. | Compile 시간이 빠르다. (SPMD) | Full Utilization | locally-connected network조건하에서 allreduce가 빠르게 적용된다. | . 단점은 다음과 같다. . 모든 parameters가 하나의 core에 실을 수 있어야 한다. | . Transformer LM - 5B Parameters . Data-parellesim을 Transformer와 같은 큰 모델에 적용할 경우 문제가 발생한다. 각 core마다 모델 파라미터를 저장해야하는데, 이는 out-of-memory문제를 야기하거나 batch size를 크게 사용하지 못하는 상황이 발생한다. . Model-paralleism . 장점은 다음과 같다. . 거대한 모델을 학습시킬 수 있다. | potentially low latency | . 단점은 다음과 같다. . 적용하기 힘들다. | . Mesh-Tensorflow . Mesh-Tensorflow의 장점은 다음과 같다. . Every processor involved in every operation. | Single Program Multiple Devices(SPMD) | collective communication (like allreduce) | . Mesh-Tensorflow에서는 다음과 같은 역할을 하고자 한다. . Data-parallelism (batch-spliting) | Model-parallelism(model-spliting) | Spatial Spliting of large inputs | Combinations of these | 적용되는 하드웨어는 아래와 같은 특징을 가진다. . 유사한 프로세서로 구성되어 있으며 | n-dimensional mesh로 여길 수 있다 | like multi-gpu, multi-cpu | User defines which dimension is split . Data-parallelism같은 경우에는 batch dimentsion을 가지고 분리한다. . batch dimension이 있는 경우: batch dimension으로 나눈다. | batch dimension이 없는 경우: parameters를 복사한다. | . Model-Paralleism같은 경우 위와 다른 dimension을 분리한다. 예를 들면, hidden layer size dimension이 있을 수 있다. . Where does communication happen? . 대부분의 연산들은 같은 프로세서안에서의 input들의 조각을 계산한다. 하지만, allreduce처럼 다른 프로세서의 output에 대해서 연산을 해야할 때도 있다. 이 때 collective communication이 필요하다. . Case Study . . 위의 이미지는 간단한 뉴럴네트워크를 batch dimension기준으로 분리한 것이다. - data parreliesm . . 위의 이미지는 hidden layer dimension을 기준으로 분리한 것이다. . 위의 이미지는 data dimension을 기준으로 분리한 것이다. . 아래의 이미지는 data-parallelism과 Model parallelism을 함께 구성한 것이다. . . Layout for Transformer Model . . Picking a Good Layout . 반복되는 업무를 피하기 위해서, 연산량이 많은 matmul/einsum은 모든 mesh dimension에 따라서 분리되어야 한다. | 같은 tensor에서 두가지 종류의 dimension으로 분리할 수 없다 | 너무 잘게 나누면 communication 비용이 올라가므로 유의해야한다. | . Example . Describing the mathematical operations | . # tf_images is a tf.Tensor with shape [100, 28, 28] and dtype tf.float32 # tf_labels is a tf.Tensor with shape [100] and dtype tf.int32 import mesh_tensorflow as mtf graph = mtf.Graph() mesh = mtf.Mesh(graph, &quot;my_mesh&quot;) batch_dim = mtf.Dimension(&quot;batch&quot;, 100) rows_dim = mtf.Dimension(&quot;rows&quot;, 28) cols_dim = mtf.Dimension(&quot;cols&quot;, 28) hidden_dim = mtf.Dimension(&quot;hidden&quot;, 1024) classes_dim = mtf.Dimension(&quot;classes&quot;, 10) images = mtf.import_tf_tensor( mesh, tf_images, shape=[batch_dim, rows_dim, cols_dim]) labels = mtf.import_tf_tensor(mesh, tf_labels, [batch_dim]) w1 = mtf.get_variable(mesh, &quot;w1&quot;, [rows_dim, cols_dim, hidden_dim]) w2 = mtf.get_variable(mesh, &quot;w2&quot;, [hidden_dim, classes_dim]) # einsum is a generalization of matrix multiplication (see numpy.einsum) hidden = mtf.relu(mtf.einsum(images, w1, output_shape=[batch_dim, hidden_dim])) logits = mtf.einsum(hidden, w2, output_shape=[batch_dim, classes_dim]) loss = mtf.reduce_mean(mtf.layers.softmax_cross_entropy_with_logits( logits, mtf.one_hot(labels, classes_dim), classes_dim)) w1_grad, w2_grad = mtf.gradients([loss], [w1, w2]) update_w1_op = mtf.assign(w1, w1 - w1_grad * 0.001) update_w2_op = mtf.assign(w2, w2 - w2_grad * 0.001) . Describing tensor/computation layout: data-parallelism | . devices = [&quot;gpu:0&quot;, &quot;gpu:1&quot;, &quot;gpu:2&quot;, &quot;gpu:3&quot;] mesh_shape = [(&quot;all_processors&quot;, 4)] layout_rules = [(&quot;batch&quot;, &quot;all_processors&quot;)] # batch dimension을 각 gpu의 개수만큼 분산 mesh_impl = mtf.placement_mesh_impl.PlacementMeshImpl( mesh_shape, layout_rules, devices) lowering = mtf.Lowering(graph, {mesh:mesh_impl}) tf_update_ops = [lowering.lowered_operation(update_w1_op), lowering.lowered_operation(update_w2_op)] . Alternatively model-parallelism | . devices = [&quot;gpu:0&quot;, &quot;gpu:1&quot;, &quot;gpu:2&quot;, &quot;gpu:3&quot;] mesh_shape = [(&quot;processor_rows&quot;, 2), (&quot;processor_cols&quot;, 2)] # modified layout_rules = [(&quot;batch&quot;, &quot;processor_rows&quot;), (&quot;hidden&quot;, &quot;processor_cols&quot;)] # modified, row * col 사각형 형태의 mesh 형성 mesh_impl = mtf.placement_mesh_impl.PlacementMeshImpl( mesh_shape, layout_rules, devices) lowering = mtf.Lowering(graph, {mesh:mesh_impl}) tf_update_ops = [lowering.lowered_operation(update_w1_op), lowering.lowered_operation(update_w2_op)] . Reference . https://www.youtube.com/watch?v=HgGyWS40g-g | https://github.com/tensorflow/mesh | https://arxiv.org/abs/1811.02084 | .",
            "url": "https://fastpages.fast.ai/deeplearning/2019/07/30/mesh-tensorflow-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "relUrl": "/deeplearning/2019/07/30/mesh-tensorflow-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "date": " • Jul 30, 2019"
        }
        
    
  
    
        ,"post22": {
            "title": "Mixture density Network: Uncertainty estimation",
            "content": "Mixture density Network: Uncertainty estimation . Problem . 기존의 방법론들은 uncetrainty를 estimate하는데 sampling을 해야하는 한계가 있다. 이 논문에서는 gaussian mixure model을 활용한 sampling-free uncertainty estimation방법론을 제안한다. . Mixture Density Network . . . 위의 이미지처럼 MDN은 output이 fixed value가 아닌 distribution의 형태를 가진다. 이런 특성 때문에 multi modal한 상황에서도 학습이 잘 진행된다. . MDN을 수식으로 나타내면 아래와 같다. . p(y∣θ)=∑j=1KπjN(y∣uj,Σj)p(y mid theta) = sum_{j=1}^K pi_j mathcal{N}(y mid u_j, Sigma_j)p(y∣θ)=j=1∑K​πj​N(y∣uj​,Σj​) . where . $ theta={ pi_j, u_j , Sigma_j }_{j=1}^K$ . | $ pi_j$ : 가중치, 0 ~ 1사이의 값, $ sum_{j=1}^K pi_j = 1$ . | . πj=exp⁡(πj^−max⁡(π))∑k=1Kexp⁡(πk^−max⁡π) pi_j= frac{ exp( hat{ pi_j}- max( pi))}{ sum_{k=1}^K exp( hat{ pi_k}- max{ pi})}πj​=∑k=1K​exp(πk​^​−maxπ)exp(πj​^​−max(π))​ . uj=u^ju_j= hat{u}_juj​=u^j​ . Σj=σmax⁡diag(p(Σj^)) Sigma_j= sigma_{ max}diag(p( hat{ Sigma_j}))Σj​=σmax​diag(p(Σj​^​)) . p(x)=11+exp⁡(−x)p(x) = frac{1}{1 + exp(-x)}p(x)=1+exp(−x)1​ . $ pi_j$를 구할 때 max값을 빼주는 이유는 exponential 연산이 불안정하기 때문이다. 마찬가지로 $ sigma_{ max}$를 곱해주는 것도 possitive constant로 만들어주기 위함이다. . MDN의 loss function은 아래와 같이 구성된다. . c(θ;D)=−1N∑i=1Nlog⁡(∑j=1Kπj(xj)N(yi∣uj(xj),Σj(xj)+ϵ)))c( theta;D) = - frac{1}{N} sum_{i=1}^N log( sum_{j=1}^K pi_j(x_j) mathcal{N}(y_i mid u_j(x_j), Sigma_j(x_j)+ epsilon)))c(θ;D)=−N1​i=1∑N​log(j=1∑K​πj​(xj​)N(yi​∣uj​(xj​),Σj​(xj​)+ϵ))) . Uncertainty Estimation Method . uncertainty Acquisition in Deep Learning . 일반적인 딥러닝 모델을 아래와 같이 수식으로 전개한다. . y=f(x)+ϵy=f(x) + epsilony=f(x)+ϵ . $f(x)$는 target function을 의미하고, $ epsilon$은 measurement error를 의미한다. . 만약 우리가 $ hat{f}(x)$를 학습시킨다고 가정하면, 아래와 같이 분산식을 구할 수 있다.(epistemic) . σe2=E∥f(x)−f^(x)∥2 sigma_e^2 =E rVert f(x) - hat{f}(x) rVert^2σe2​=E∥f(x)−f^​(x)∥2 . 위의 수식은 다음과 같이 전개된다. . begin{align} E rVert y - hat{f}(x) rVert^2 &amp;=E rVert y -f(x) +f(x)- hat{f}(x) rVert^2 &amp;=E rVert y - f(x) rVert^2 + E rVert f(x) - hat{f}(x) rVert^2 &amp;= sigma_a^2 + sigma_e^2 end{align} . 여기서 $ sigma_a^2$은 aleatoric uncertainty를 의미하며 데이터가 많아도 감소시킬 수 없다(데이터 자체의 노이즈). 반면에 $ sigma_e^2$은 epistemic uncertainty를 의미하며 데이터의 수가 많아지면 감소시킬수 있는 uncertainty다.(lack of training data) . low aleatoric, high epistemic: training data 부족 . | high aleatoric, low epistemic: multiple possible steering angles . | . PROPOSED UNCERTAINTY ESTIMATION METHOD . p(y midθ)=∑j=1Kπj(x)N(y∣uj(x),Σj(x))p(y mid theta) = sum_{j=1}^K pi_j(x) mathcal{N}(y mid u_j(x), Sigma_j(x))p(y midθ)=j=1∑K​πj​(x)N(y∣uj​(x),Σj​(x)) . MDN의 output은 위와 같이 전개된다. . $ pi_j, u_j, Sigma_j$: j번째 가중치, 평균, 분산 | . [Note] density network에 비해서 MDN이 복잡하고 노이즈가 많은 distribution에 잘 학습된다. . MDN output의 분산을 구하기 위해서는 먼저 평균값에 대한 정보가 있어야 한다. 아래는 output의 평균에 대한 수식이다. . begin{align} E[y mid x]&amp;= sum_{j=1}^K pi_j(x) int mathcal{N}(y midu_j(x), Sigma_j(x)) dy &amp;= sum_{j=1}^K pi_j(x)u_j({x}) end{align} . 분산식은 아래와 같다. . begin{align} V[y mid x] &amp;= int rVert y-E[y mid x] rVert ^ 2p(y mid x)dy &amp;= sum_{j=1}^K pi_j int rVert y- sum_{k=1}^K pi_k(x)u_k({x}) rVert ^ 2 mathcal{N}(y mid u_j(x), Sigma_j(x)) dy end{align} . begin{align} int rVert y- sum_{k=1}^K pi_k(x)u_k({x}) rVert ^ 2 mathcal{N}(y mid u_j(x), Sigma_j(x)) dy &amp;= int rVert y-u_j rVert ^ 2 mathcal{N}(y mid u_j, Sigma_j) dy + &amp; int rVert u_j- sum_{k=1}^K pi_ku_k rVert ^ 2 mathcal{N}(y mid u_j, Sigma_j) dy &amp;+ 2 int(y-u)^T(u_j - sum_{k=1}^K pi_ku_k) mathcal{N}(y mid u_j, Sigma_j) dy &amp;= Sigma_j + rVert u_j- sum_{k=1}^K pi_ku_k rVert . end{align} . 위의 수식을 적용하면, 분산식은 아래와 같이 전개된다. . begin{align} V[y mid x] &amp;= sum_{j=1}^K pi_j(x) Sigma_j(x) + sum_{j=1} ^ K pi_j(x) rVert u_j(x)- sum_{k=1}^K pi_k(x)u_k(x) rVert^2 end{align} . 위에서 언급했듯이 total variance는 epistemic과 aleatoric으로 분리할수 있다. MDN은 조금 다르게 explainable variance와 unexplainable variance로 분리된다. . $ sum_{j=1} ^ K pi_j(x) rVert u_j(x)- sum_{k=1}^K pi_k(x)u_k(x) rVert^2 =E_{k sim pi}(V(y mid x, k))$: epistemic uncertainty | $ sum_{j=1}^K pi_j(x) Sigma_j(x) = V_{k sim pi}(E[y mid x, k])$:aleatoric uncertainty | .",
            "url": "https://fastpages.fast.ai/deeplearning/uncertainty/2019/07/26/Mixture-density-Network-Uncertainty-estimation.html",
            "relUrl": "/deeplearning/uncertainty/2019/07/26/Mixture-density-Network-Uncertainty-estimation.html",
            "date": " • Jul 26, 2019"
        }
        
    
  
    
        ,"post23": {
            "title": "Faster-RCNN 정리글",
            "content": "Faster-RCNN 정리글 . Problem: bottleneck . 기존의 state-of-the-art object detection network는 region proposal algorithm을 사용하였다. (ex-Fast R-CNN) region proposal algorithm은 이들 network상에서 bottleneck의 역할을 하고 있었다. 즉, region proposal algorithm 때문에 학습 시간 및 알고리즘 수행시간이 지체되고 있는 것을 확인했다. . 이러한 문제를 해결하기 위해서 Region Proposal Network를 제안하는데 이는 full-image convolutional feature를 region proposal하는데도 사용하여 cost-free하게 적용될 수 있다. . [비교] Fast-RCNN: region proposal algorithm . 한 이미지 당 cpu기준 약 2초의 시간이 걸린다. . . reference: https://donghwa-kim.github.io/SelectiveSearch.html | . Faster-RCNN . . Faster-RCNN은 두 가지 모듈로 구성된다. . Deep fully convolutional network that proposes regions | Fast R-CNN detector(classfier): 1에서 추출된 regioin을 사용한다. | 중요한 점은 1, 2가 진행되는 동안 feature를 공유한다는 것이다. (cost-free) . Region Proposal Networks . 어떤 사이즈의 이미지가 들어와도 직사각형의 object proposal을 output으로 가진다. . . 이 논문의 목표는 detector의 computation과 proposal의 computation을 공유하는 것이기 때문에 위와 같은 convolution layers를 공유한다고 가정하였다. . ZF: 5 sharable convolutional layers | VGG-16: 13 sharable convolutional layers | . 이렇게 공유된 feature들은 두 가지 용도로 사용하게 된다. ($n times n$의 spatial window를 사용하여 얻은 feature) . box regression: proposal | box classification: detector | . 해당 논문에서는 $n = 3$으로 feature를 생성했는데, 이는 일반적인 reception field로는 작은 숫자이다. 이렇게 작게 설정한 이유는 sliding window가 모든 spatial location을 탐색할 수 있게끔 설계한 것이다. . Anchors . sliding window의 위치가 변할 때 마다, multiple region proposals이 이루어진다. . 한 위치에서 나올수 있는 region proposal의 최대 개수를 $k$라고 한다면, . regression box: $4k$의 outputs, $k$박스의 각 좌표(꼭지점) . | classfier: $2k$의 outputs, object or not . | . | . 위의 그림은 anchor의 scale과 ratio에 변화이다. 해당 논문에서는 위의 그림과 같이 9 종류의 anchor를 사용하였다. 따라서 해당 이미지의 크기가 $W times H$라면 $W times H times k$개의 anchor가 존재한다. . Translation-Invariant Anchors . translation-invariant의 특성이란 . . 물체가 특정 위치에 존재할 때만 탐지되거나 혹은 특정 위치에서는 탐지가 잘 안되는 현상을 줄이는 것 . reference: https://medipixel.github.io/post/anchor-target/?fbclid=IwAR3sCN1gXjcpt0SNcBgCpVsW8Y6jo2u-2MrBkrQGQgy3CSIKkUPPHGt4YY8 | . 이를 확인하기 위해서 MultiBox method와 비교하게 되는데, 이는 k-means 방법론을 사용하며 800개의 anchor를 생성한다. 하지만 이는 translation invariant가 아니다. . translation invariant의 특성은 model size를 작게 사용할 수 있게 해준다. MultiBox의 경우 $(4 + 1) * 800$-dimensional fully-connected network를 사용하는 반면에, Faster-RCNN은 $(4 + 2) * 9$-dimensional convolutional output layer만 있으면 가능하다. 즉, $512 * (4 + 2) * 9 approx 2.8 * 10^4$의 파라미터만으로 해결 할 수 있다. (box: 4, cls: 2) 반면에, MultiBox는 약 $ 1536 * (4 + 1) * 800 approx 6.1 * 10^6 $의 파라미터가 필요하다. 따라서, overfitting 문제에 있어서 Faster-RCNN이 더 우수할 것으로 기대된다. ($1536=512 * 3$) . Multi-Scale Anchors as Regression References . . multi-scale prediction에는 두 가지 방법이 있다. 그리고 이 두가지 방법은 종종 혼합하여 사용된다. 해당 논문에서는 cost-efficient한 두 번째 방법을 사용하였다. . image/feature pyramids -Figure 1 (a) 이미지를 다양한 크기로 resize한 후 feature map은 각 resize된 이미지에서 뽑아낸다. | 효과적이긴 하나, computation 시간이 오래 걸린다. | . | multiple scale sliding window 다양한 크기의 window를 sliding하여 feature을 얻는다. | pyramid of filters | image/feature pyramids을 사용하지 않게 하기 때문에 scale을 다루는데 추가적인 cost가 발생하지 않는다.(rescale and feature) | . | . Loss Function . RPN을 학습할 때는 각 anchor에 대해서 binary class label로 학습이 진행된다. (object or not) positive label이 부여되는 경우는 두 가지이다. . anchor/ anchors with the highest IOU score with ground-truth boxes | union anchor: ground-truth box와의 IOU 점수가 0.7이상인 경우 | 보통은 2번째 조건으로만 postive sample을 만들 수 있지만, 희귀한 경우에 2번째 조건만으로 찾지 못하는 경우가 있다. 그래서 1번째 조건도 추가한다. . negative label의 경우 IOU점수가 0.3보다 낮은 anchor에 부여된다.(ground-truth) positive 혹은 negative에 속하지 못하는 anchor는 train objective에 영향을 주지 않는다. . 아래는 objective이다. . L({pi},{ti})=1Ncls∑iLcls(pi,pi∗)+λ1Nreg∑ipi∗Lreg(ti,ti∗)L( {p_i }, {t_i }) = frac{1}{N_{cls}} sum_iL_{cls}(p_i, p_i^*) + lambda frac{1}{N_{reg}} sum_i p_i^*L_{reg}(t_i, t_i^*)L({pi​},{ti​})=Ncls​1​i∑​Lcls​(pi​,pi∗​)+λNreg​1​i∑​pi∗​Lreg​(ti​,ti∗​) . $i$는 anchor의 index를 의미한다. | $p_i^*$는 ground-truth label을 의미하며 1이 postive이다. | $t_i^*$는 4차원의 vector로 bounding box를 의미한다. (ground-truth) | $p_i^L_{reg}(t_i, t_i^)$은 positive sample일때만 적용된다는 뜻이다. | . Bounding box regression은 어떻게 적용되는가? $L_1$ loss가 적용되며 vector가 다음과 같이 변환된다. . tx=(x−xa)/wa,ty=(y−ya)/hatw=log⁡(w/wa),th=log⁡(h/ha)tx∗=(x∗−xa)/wa,ty∗=(y∗−ya)/hatw∗=log⁡(w∗/wa),th∗=log⁡(h∗/ha)t_x = (x-x_a)/w_a, t_y=(y-y_a)/h_a t_w = log(w/w_a), t_h= log(h/h_a) t_x^* =(x^* - x_a)/w_a, t_y^*=(y^*-y_a)/h_a t_w^*= log(w^*/w_a), t_h^*= log(h^*/h_a)tx​=(x−xa​)/wa​,ty​=(y−ya​)/ha​tw​=log(w/wa​),th​=log(h/ha​)tx∗​=(x∗−xa​)/wa​,ty∗​=(y∗−ya​)/ha​tw∗​=log(w∗/wa​),th∗​=log(h∗/ha​) . $x$: predicted box | $x_a$: anchor box | $x^*$: ground truth box | . 위의 방법은 기존에 사용되던 Region of Interest methods와 다르다. ROI method에서는 bounding-box regression이 arbitrarily sized ROI에서 뽑힌 feature를 바탕으로 진행되며, 모든 region size에 대해서 weight를 공유한다. 반면에 위의 방법론은 feature map의 spatial size가 3 * 3으로 고정되어 있으며 변하는 size에 다루기 위해서는 다양한 anchor를 사용한다. 이 때, 각 anchor끼리는 weight를 공유하지 않는다. . 하지만, 위에서 언급했듯이 이전의 region proposal 방법론은 효율적이지 못하다는 단점이 있다. . Training RPNs . image-centric sampling 방법론을 사용한다. . 각 mini-batch에서 image상에는 많은 positives와 negatives가 존재한다. 이를 그대로 학습시킨다면 일반적으로 negative sample의 수가 많기 때문에 편향될 위험이 있다. 따라서, positive sample과 negative sample을 random하게 1:1로 추출한다음 학습을 진행한다. (p: 128, n:128) . Sharing Features for RPN and Fast R-CNN . 이제 detection network를 고려해보자. detection network와 regioin proposal network를 독립적으로 학습시킨다면 이는 서로 다른 방법으로 학습이 될 것으로 기대된다. 따라서 convolution layer를 공유하기 위해서는 새로운 방법이 필요하다. . Alternating training . 먼저 RPN을 학습시킨다 다음, RPN에서 나오는 Proposal을 바탕으로 Fast-RCNN을 학습시킨다. 해당 논문에서는 이 방법론을 사용한다. . | Apporximate joint training: . Fast-RCNN과 RPN을 하나의 network로 만든다. SGD interations동안 forward pass에서 region proposal을 생성하고 이는 Fast-RCNN detector를 학습시키는 동안 미리 계산되고 고정되어 있다.(not differential) backward pass에서는 RPN loss와 Fast_RCNN loss가 더해져서 total loss로 정의된다. 이는 쉽게 적용될 수 있으나, 이는 proposal box에 대한 gradient값을 무시하게 된다. (approximate한다) . 해당 실험에서는 학습시간을 줄이면서 결과는 유사하게 나오는 것을 확인할 수 있었다. (reduce 25% ~ 50% train time) . | Non-approximate joint training . Apporximate joint training과 다르게 box coordinates에 대해서 미분가능하다. . bbox에 미분가능한 ROI pooling layer가 필요하다. . | . 4-Step Alternating Training . RPN 학습, (initialized with an ImageNet-pre-trained model ) | 1에서 학습학 RPN을 바탕으로 regioin proposal 생성 후 Fast-RCNN detector 학습(initialized with an ImageNet-pre-trained model) 이 step에서는 layer를 공유하지 않는다. | detector network를 RPN initialization에 사용한다. 공유되는 layer는 고정시키고 RPN에만 적용되는 layer만 학습시킨다. 이 step에서 layer를 공유한다. | 공유되는 layer는 고정시키고 Fast-RCNN에만 적용되는 layer를 학습시킨다. | Implementation Detatils . image/feature pyramids -Figure 1 (a)과 multi scale sliding window 두 가지 방법 모두 실험해보았다. 하지만 첫번째 방법론은 speed-accuracy trade-off가 좋지 않음을 확인할 수 있었다. . Recall to IOU . 일반적으로 전반적인 detection accuracy와 관련있는 metric이다. . . 위의 이미지를 보면 알 수 있듯이, SS, EB가 더 빠르게 감소하는 recall을 확인할 수 있다. . recall이란 . TP / total ground-truth .",
            "url": "https://fastpages.fast.ai/deeplearning/detection/2019/07/26/Faster-RCNN-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "relUrl": "/deeplearning/detection/2019/07/26/Faster-RCNN-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "date": " • Jul 26, 2019"
        }
        
    
  
    
        ,"post24": {
            "title": "Mixup 정리글",
            "content": ". title: “Mixup 정리글” toc: true branch: master badges: true comments: true categories: [‘deeplearning’, ‘augmentation’] metadata_key1: mixup — . Mixup: Beyond Empirical Risk Minimization . mixup은 deep learning model의 memorization문제나 adversarial examples에 민감한 이슈를 해결하기 위해 나온 Data Augmentation기법입니다. . memorization: | . 모델이 학습을 진행할 때, 정답만을 기억하고 내리는 행동. 즉, 데이터 분포를 학습하는 것이 아니라 해당 데이터가 어떤 라벨에 해당하는지 기억하게 되는 것. 결론적으로는 test distribution에 대한 generalization을 하지 못한다. . sensitivity to adversarial examples: adversarial attack에 취약하다. | . mixup은 network를 data pair간의 convex combination을 이용하여 학습시킵니다. 이는 결과적으로 모델이 training sample간에 simple linear behavior를 하지 않도록 하는 효과가 있습니다. . simple linear behavior between training examples: | . Empirical Risk Minimization(ERM) VS Vicinal Risk Minimization(VRM) . [성공적인 neural networks의 두 가지 특징] . trained as to minimize their average error over the training data. . | the size of neural networks scales linearly with the number of training examples. . | 하지만, learning thoery에 따르면 ERM의 수렴은 모델의 복잡도가 데이터의 수보다 크지 않을 때 보장된다. 이는 [2]번 조건과 모순되어 보일 수 있으나 overfitting issue를 고려해보면 이해가 된다. . [memorization의 측정] adversarial examples에 예측하는 라벨이 크게 변하는 정도가 클수록 memorization이 크다고 평가할 수 있다. 직관적으로 생각해보면, 사람의 지각으로는 큰 차이가 없는 데이터에 대해서 딥러닝 모델이 서로 다른 예측을 한다면 이는 generalization을 못한 것으로 평가할 수 있다. . [Vicinial Risk Minimization이란] data augmentation의 이론적 배경으로 training data와 유사한 주변 데이터를 묘사하는 것이다. 이것이 가능해지면, virtual example(만들어진 데이터)는 training distribution의 support를 확대하는 효과를 가져온다. (training distribution중 모호한 부분을 채워주는 것으로 해석) . 참고: [support of distribution] . Contribution . Xnew=λXi+(1−λ)XjX_{new} = { lambda}X_i + (1 - { lambda})X_jXnew​=λXi​+(1−λ)Xj​ . Ynew=λYi+(1−λ)YjY_{new} = { lambda}Y_i + (1 - { lambda})Y_jYnew​=λYi​+(1−λ)Yj​ . 위의 수식대로 data augmentation을 하는 것이 mixup의 전부이다. . 아래의 영상은 lambda값에 따라서 mixup 데이터의 pca결과값이 어떻게 변하는지 시각화한 것이다. 아래의 그림들처럼 linear하게 PCA value가 변하는 것을 확인하였는데 이는 VRM 가정이 옳다는 것을 보여준다. . . . . From Empirical Risk Minimization To Mixup . Supervised learning에서의 task는 결국 random feature vector X와 random target vector Y간의 관계를 설명할 수 있는 함수 f를 찾는 것이다. 함수 f는 joint distribution P(X, Y)를 따른다. 이 때, loss function의 역할은 prediction f(X)와 target Y간의 차이를 나타내며 학습을 진행하면서 average loss를 감소시킨다. 여기서 average loss는 expected risk으로 해석된다. 이를 수식으로 나타내면 아래와 같다. . R(f)=∫loss(f(x),y)dP(x,y)  [1]{R}(f) = int {loss}({f}(x), y)dP(x,y) [1]R(f)=∫loss(f(x),y)dP(x,y)  [1] . 하지만, distribution P는 실제상황에서 알기 힘들다. (intractable distribution) 이를 해결하기 위해서 empricial distribution으로 근사하는 방법론을 이용한다. 이를 수식으로 나타내면 아래와 같다. . Pσ=1nΣi=1nσ(x=xi,y=yi)  [2]{P}_{ sigma}= frac{1}{n} Sigma_{i=1}^{n} sigma(x={x}_i, y={y}_i) [2]Pσ​=n1​Σi=1n​σ(x=xi​,y=yi​)  [2] . σ(x=xi,y=yi)는 Dirac mass centered at (xi,yi) 성질을가지고있다. sigma(x={x}_i, y={y}_i) 는 Dirac mass centered at (x_i, y_i) 성질을 가지고 있다.σ(x=xi​,y=yi​)는 Dirac mass centered at (xi​,yi​) 성질을가지고있다. . [1]수식에 [2]수식을 대입하면 아래와 같이 전개될 수 있다. . 참고: Dirac mass centered 아래 그림과 같이 굉장히 naive한 가정이다. . . Rσ=∫loss(f(x),y)dPσ(x,y)=1nΣi=1nloss(f(xi),yi)R_{ sigma}= int loss(f(x), y)dP_{ sigma}(x,y)= frac{1}{n} Sigma_{i=1}^{n}loss(f(x_i),y_i)Rσ​=∫loss(f(x),y)dPσ​(x,y)=n1​Σi=1n​loss(f(xi​),yi​) . 위와 같이 ERM은 간단하게 계산될 수 있지만, Memorization이라는 현상을 야기할 수 있다. joint distribution P에 어떤 가정을 하느냐에 따라서 결과가 달라질 수 있는데 이 논문에서는 Vicinal Risk Minimization Principle을 적용하고 있다. VRM을 가정하게 되면 P는 다음과 같이 정의할 수 있다. . Pv(x~,y~)=1nΣi=1nV(x~,y~∣xi,yi)  where V is a vicinity distributionP_v( tilde{x}, tilde{y})= frac{1}{n} Sigma_{i=1}^{n}V( tilde{x}, tilde{y} mid x_i, y_i) text{where V is a vicinity distribution}Pv​(x~,y~​)=n1​Σi=1n​V(x~,y~​∣xi​,yi​)  where V is a vicinity distribution . vicinity distribution은 만들어낸 feature-target pair의 probability를 측정한다. 즉, 얼마나 그럴듯한 데이터인지 판단하는 것이다. . [what is mixup doing?] . . (Mixup) Effect of mixup on a toy problem. Green: Class 0 Orrange: Class 1 Blue shading indicates $P(y = 1 mid x)$. . 위의 그림을 통해서 할 수 있듯이, mixup은 uncertainty를 측정하는데 더 효과적이다. 파란색 부분은 해당 데이터 x가 주어졌을 때, Class 1일 확률이다. ERM을 보면 파란색 부분이 Class 1가 가까운 것과 가깝지 않는 것 사이의 차이를 나타내지 못한다. 반면에 mixup은 가까운 부분은 더 짙은 파란색으로 나타내어, uncertainty를 smoother하게 측정할 수 있다. . . (a) 는 prediction을 나타낸 것이고, (b)는 gradient norm을 나타낸 것이다. (a)를 보면 mixup으로 학습시킨 것이 더 prediction측면에서 좋은 성능을 보이고 있다. . 한편, (b)를 보면 gradient norm이 더 작게 나는 것을 알 수 있는데, 이는 더 안정적인 학습을 보이고 있다는 것을 보여준다. . 참고: norm . 일반적으로 크기 혹은 길이를 나타낸다. | . Experiments . 3.1 IMAGENET CLASSIFICATION] . evaluation과정에서 224 * 224 크기의 중간부분이 소실된 이미지를 test하였다. 실험결과 hyperparameter alpha는 0.1에서 0.4사이의 값이 우수한 성능을 보였으며, alpha값이 이보다 더 크면 underfitting의 부작용을 가져왔다. 또한 mixup은 higer capacities와 longer training run의 효과를 가져왔다. 이는 ERM과 비교했을 때 , epoch 90에서 mixup이 더 큰 성능개선효과를 보였다는 것으로 증명하였다. . 3.2 CIFAR10 AND CIFAR100 . . 3.3 SPEECH DATA] . . 3.4 MEMORIZATION OF CORRUPTED LABELS] . . mixup interpolation alpha가 더 커질수록 memorization이 더 어려워진다는 가설을 세웠다. corruption label을 가지고 학습을 시켰다. 이는 memorizaiton을 검증하는 실험에서 사용된다. (더 찾아보기) 실험결과 test과정에서 large alpha가 dropout(0.7, 0.8)보다 test error를 더 줄일 수 있었으며, real label에 대해서는 낮은 training error를 보이고 noisy label에 대해서는 높은 training error를 보였다. 주목할 점은 dropout과 mixup을 같이 사용했을 때 성능이 가장 좋았다. . 3.5 ROBUSTNESS TO ADVERSARIAL EXAMPLES . . 3.6 TABULAR DATA . . 3.7 STABILIZATION OF GENERATIVE ADVERSARIAL NETWORKS . . mixup은 disciriminator의 gradient regularizer의 역할을 하기 때문에, 더 안정적인 학습을 할 수 있다. . max⁡gmin⁡dEx,zl(d(x),1)+l(d(g(z)),0) max_g min_d E_{x, z} mathcal{l}(d(x), 1) + l(d(g(z)), 0)gmax​dmin​Ex,z​l(d(x),1)+l(d(g(z)),0) . 기존의 GAN objective . 아래는 mixup이 적용된 GAN distriminator objective이다. . maxgmindEx,y,λl(d(λx+(1−λ)g(z),λ).max_gmin_dE_{x, y, lambda}l(d( lambda x + (1- lambda)g(z), lambda).maxg​mind​Ex,y,λ​l(d(λx+(1−λ)g(z),λ). . 3.8 ABLATION STUDIES . 이 논문에서는 convex combination을 통한 data augmentation방법론을 제안했지만, 다양한 방법론이 있을 수 있다. . [예시] . feature map을 섞는다. | 같은 클레스의 데이터 | 비슷한 거리에 있는 데이터 | 다른 클레스의 데이터 | . 실험결과 현 논문에서 제시한 mixup이 제일 좋은 성능을 보였다. .",
            "url": "https://fastpages.fast.ai/2019/07/21/mixup-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "relUrl": "/2019/07/21/mixup-%EC%A0%95%EB%A6%AC%EA%B8%80.html",
            "date": " • Jul 21, 2019"
        }
        
    
  
    
        ,"post25": {
            "title": "LogSumExp Trick",
            "content": "LogSumExp Trick . 머신러닝 학습을 진행하다보면, 종종 loss가 제대로 계산되지 않는 현상이 발생한다. 이는 loss를 계산하는 과정에서 불안정한 수식을 계산하기 때문에 발생한다. 특히 cross-entropy와 같이 log함수와 연관있는 수식은 주의가 필요하다. . 아래 이미지는 로그 함수 그래프이다. 이 그래프에서 알 수 있듯이 $x$의 값이 0에 가까워질수록 $ log_2(x)$의 값은 음의 무한대의 값을 가지게 되며, 컴퓨터 연산과정에서 이는 연산이 불가능하다. (overflow) . . 이러한 현상을 방지하기 위해서 사용하는 것이 LogSumExp trick이다. . LogSumExp (LSE) function is a smooth maximum – a smooth approximation to the maximum function, mainly used by machine learning algorithms. . $LSE(x_1, cdots, x_n) = log( exp(x_1) + cdots + exp(x_n))$ . LogSumExp는 convex function인데 따라서 loss 함수에 적용하기에 이론적으로도 적절하다.(http://www.math.uwaterloo.ca/~hwolkowi/henry/teaching/w10/367.w10/367miscfiles/pages48to60.pdf) . Numerical Stability . 아래의 코드를 보면 $-1000$을 지수로 설정해도 안정성이 확보됨을 확인 할 수 있다. . &gt;&gt;&gt; import math &gt;&gt;&gt; math.e ** -1000 0.0 . Softmax . 아래는 softmax의 수식이다. . exj∑i=1nexj frac{e^{x_j}}{ sum_{i=1}^ne^{x_j}}∑i=1n​exj​exj​​ . softmax는 특정 수를 non-linear한 방식으로 probability로 변환하는 것으로 해석할 수 있다. 이는 위의 LogSumExp pattern을 나타내고 있다. . begin{align} log left( frac{e^{x_j}}{ sum_{i=1}^{n} e^{x_i}} right) &amp;= log(e^{x_j}) :-: log left( sum_{i=1}^{n} e^{x_i} right) &amp;= x_j :-: log left( sum_{i=1}^{n} e^{x_i} right) &amp; (1) end{align} . 지수의 곱셈은 다음과 같이 전개된다. ea⋅eb=ea+be^a cdot e^b = e^{a+b}ea⋅eb=ea+b 이는 log함수에서 다음과 같이 표현된다. log⁡(a⋅b)=log⁡(a)+log⁡(b) log(a cdot b) = log(a) + log(b)log(a⋅b)=log(a)+log(b) 위의 두 공식을 이용하면 LogSumExp의 공식은 아래와 같이 전개된다. . begin{align} LogSumExp(x_1…x_n) &amp;= log big( sum_{i=1}^{n} e^{x_i} big) &amp;= log big( sum_{i=1}^{n} e^{x_i – c}e^{c} big) &amp;= log big( e^{c} sum_{i=1}^{n} e^{x_i – c} big) &amp;= log big( sum_{i=1}^{n} e^{x_i – c} big) + log(e^{c}) &amp;= log big( sum_{i=1}^{n} e^{x_i – c} big) + c &amp; (2) end{align} . 그렇다면, softmax에 적용해보자. 위에서 log softmax는 다음과 같이 전개되었다. . begin{align} log(Softmax(x_j, x_1…x_n)) &amp;= x_j :-: LogSumExp(x_1…x_n) &amp;= x_j :-: log left( sum_{i=1}^{n} e^{x_i} right) &amp;= x_j :-: log big( sum_{i=1}^{n} e^{x_i – c} big) :-: c end{align} . Logistic loss in Tensorflow . # sigmoid input: x # output: y = 1 / (1 + exp(-x)) # 1 - y = exp(-x) / (1 + exp(-x)) # target: z import tensorflow as tf Logistic_loss = -[z * tf.log(y) + (1 - z) * tf.log(1 - y)] . Logistic loss . = -[z * log(y) + (1-z) * log(1-y)] . = z * log(1 + exp(-x)) - (1-z) * [-x - log(1 + exp(-x))] . = z * log(1 + exp(-x)) + x + log(1 + exp(-x)) - z_x - z * log(1 + exp(-x)) . = x - z_x + log(1 + exp(-x)) . 여기서 overflow를 피하기 위해서 tensorflow에서는 다음과 같이 수식을 변형한다. . LogisticLoss = max(x, 0) - z*x + log(1 + exp(-abs(x))) . 여기서 절대값을 취해주는 이유는 overflow를 피하기 위해서이다. x가 만약 음수의 값을 가지면서 더 작아지면 exp(-x)의 값은 빠른 속도로 증가하게 된다. . def loss_func(y_true, y_pred): &quot;&quot;&quot;Log sum exp tricks Check https://github.com/tensorflow/tensorflow/issues/172 y_true: a * (abs(z1-z2)) + b, shape of [batch_size, ] y_pred: match, shape of [batch_size, ] &quot;&quot;&quot; maxes = tf.where(tf.greater_equal(y_pred, 0), y_pred, tf.broadcast_to(0.0, shape=tf.shape(y_pred))) z_x = tf.multiply(y_true, y_pred) loglogit = tf.log(tf.broadcast_to(1.0, shape=tf.shape(y_pred)) + tf.exp(tf.clip_by_value(-tf.abs(y_pred), -1e-8, 0))) return maxes - z_x + loglogit . https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/ | https://github.com/tensorflow/tensorflow/issues/172 | https://blog.feedly.com/tricks-of-the-trade-logsumexp/ | .",
            "url": "https://fastpages.fast.ai/deeplearning/2019/07/21/LogSumExp-Trick.html",
            "relUrl": "/deeplearning/2019/07/21/LogSumExp-Trick.html",
            "date": " • Jul 21, 2019"
        }
        
    
  
    
        ,"post26": {
            "title": "How Does Batch Normalization Help Optimization? 정리글",
            "content": "How Does Batch Normalization Help Optimization? 정리글 . Main Contribution . Batch normalization에 대하여에서 BN이 결국 internal covariate shift현상을 해결하여, 모델의 수렴속도를 높인다고 주장하였다. 하지만, 해당 논문에서는 internal covariate shift현상을 감소하여 그러는 것이 아니며, BN이 실제로 감소시키지 않는다고 주장한다. . 이 논문에서는 BN이 optimization problem을 smoother하게 만들어서 성공적이라고 주장한다. 이로 인해서 gradient는 predictive해지고 더 큰 learning rate를 사용할 수 있다. . optimization problem이 smoother 해진다는 것은… . https://ifm.mathematik.uni-wuerzburg.de/~schmidt/publications.php . Batch normalization and internal covariate shift . . train, test 그래프에서는 batch normalization의 역할을 잘 보여주고 있다. 높은 learning rate를 사용할 수 있는 것을 보여주고 있는데, 오른쪽의 그래프를 보면 BN을 적용한 모델의 activation과 그렇지 않은 모델의 activation의 분포가 그리 큰 차이를 가지고 있지 않는 것을 확인 할 수 있다. 이런 결과를 가지고 다음과 같은 질문을 할 수 있다. . Batch Normalization의 효과가 internal covariate shift와 연관이 있는 것인가? | Batch Normalization이 internal covariate shift를 감소시키는 역할을 하는가? | . Does BatchNorm’s performance stem from controlling internal covariate shift? . layer input의 distribution의 mean, variance를 조정하는 것이 training performance를 향상시킬 수 있는것 인가? 이를 어떻게 입증할 것인가? . 다음과 같은 실험환경을 구성하였다. . BN을 적용한 후, $random$ noise를 추가하였다. 이 noise는 non-zero mean을 가지며 non-unit variance distribution이다. 또한 training step마다 noise distribution은 바뀐다. | noise가 추가되면 결국 covariate shift현상이 생기는 것이다. | . . 위의 그림을 보면, Standard + BatchNorm과 Standard + ‘noisy’BatchNorm과의 성능 차이가 거의 없음을 알 수 있다. 즉, internal covariate shift를 해결하는 것과 batch normalization의 효과를 무관하다고 볼 수 있다. 또한 오른쪽 이미지를 보면, Standard + ‘noisy’BatchNormdl Standard보다 덜 안정적인 distribution을 가지고 있는 것을 확인할 수 있다. 하지만, 실험결과는 Standard + ‘noisy’BatchNorm이 우수한 걸로 보아 stable distribution이 training performance에 주는 영향은 미비한 것으로 보인다. 또한 Standard에 noise를 섞을 때, 전혀 학습이 안되는 것을 확인할 수 있었다. . 결국 internal covariate shift를 감소시키는 것과 batch normalization효과는 관련이 있다고 보기 힘들다. . Is BatchNorm reducing internal covariate shift? . 위에서 internal covariate shift와 training performance와 직접적인 관계가 없다는 것을 증명했다. 하지만, 보다 넓은 관점에서의 training performance와 연관된 internal covariate shift(ICS)이 있을까라는 궁금증이 들 수 있으며, 만약 그렇다면 BatchNorm은 ICS를 감소시킬까? . 각 layer는 empirical risk minimization을 수행하고 있다. 만약 layer가 학습도중에 update된다면, 이전 layer가 변하기 때문에 input도 변한다. . empirical risk minimization . risk란 loss function의 expectation을 의미한다. | . R(h)=∫L(f(x),y)dP(x,y)R(h) = int L(f(x), y) dP(x, y)R(h)=∫L(f(x),y)dP(x,y) Is BatchNorm reducing internal covariate shift? 이 질문에 답하기 위해서는 더 넓은 개념의 internal covariate shift를 다뤄야 한다. 이는 optimization problem과 연관이 깊다. 일반적으로 training은 first-order method를 사용하기 때문의 loss의 gradient는 친숙한 편이다. layer내의 parameters가 이전 layer의 update영향으로 얼마큼 조정해야하는지 측정하기 위해서는 이전 layer가 update되기 전과 후의 gradient간의 차이를 구해야한다. . Notation . loss: $ mathcal{L}$ . | $k$ layers의 parameters(time = t): $W_1^{(t)}, W_2^{(t)}, cdots W_k{(t)} $ . | batch of input-label pairs(time = t):$(x^{(t)}, y^{(t)})$ . | internal covariate shift =$ rVert G_{t, i} - acute{G_{t, i}} rVert_2 $ Gt,i=∇Wi(t)L(W1(t),W2(t),⋯Wk(t);x(t),y(t))G_{t, i} = nabla_{W_i^{(t)}} mathcal{L(W_1^{(t)}, W_2^{(t)}, cdots W_k^{(t)} ;x^{(t), y^{(t)}} )}Gt,i​=∇Wi(t)​​L(W1(t)​,W2(t)​,⋯Wk(t)​;x(t),y(t)) . Gt,iˊ=∇Wi(t+1)L(W1(t+1),⋯Wi−1(t+1),Wi(t),Wi+1(t)⋯Wk(t);x(t),y(t)) acute{G_{t, i}} = nabla_{W_i^{(t+1)}} mathcal{L(W_1^{(t+1)}, cdots W_ {i-1}^{(t+1)},W_ {i}^{(t)}, W_ {i+1}^{(t)} cdots W_k^{(t)} ;x^{(t), y^{(t)}} )}Gt,i​ˊ​=∇Wi(t+1)​​L(W1(t+1)​,⋯Wi−1(t+1)​,Wi(t)​,Wi+1(t)​⋯Wk(t)​;x(t),y(t)) | . . 위의 internal covariate shift 산출방법으로 bath norm을 적용한 경우와 그렇지 않은 경우를 비교하였다. 이전의 주장은 BN이 ICS를 감소시킨다고 주장하였으나, 실험결과 BN이 ICS를 증가시키기도 하였다. 위의 그림을 보면 확인 할 수 있다. 이런 현상은 DLN에서 더 도드라 진다. DLN을 보면 오히려 Standard한 것이 ICS가 적게 나타나는데 비해서 BN을 적용할 때는 $G, acute{G}$는 서로 상관관계가 없어보인다. (하지만 training 결과는 loss, acc 측면에서 더 좋게 나온다.) . 결국 batch normalization은 internal covariate shift를 감소시키지 않는다는 것을 증명하였다. . Why dose BatchNorm Work? . 위에서 밝혔듯이 BatchNorm과 ICS는 관련이 없다. 하지만, BatchNorm은 exploding gradient 혹은 vanishing gradient 문제에 있어서 효과적이다. 하지만 이는 BatchNorm이 training performance를 향상 시키는 본질적인 이유라고 할 수 없다. . The smoothing effect of BatchNorm . it reparametrizes the underlying optimization problem to make its landscape significantly more smooth. . 결론부터 말하면, BatchNorm은 optimization 문제를 smooth하게 바꾸어 training performance를 개선시키고 있다. . 그 이유는 loss function의 Lipschitzness 를 개선시키기 때문이다. . gradient의 변화가 loss의 변화보다 적은 상태, 따라서 loss가 작은 learning rate로 변하게 되면 gradient도 작게 변하게 된다. . $f$ is L-Lipschitz if $ rvert f(x_1) - f(x_2) rvert le L rVert x_1-x_2 rVert $ for all $x_1, x_2$ . 이 특성은 BatchNorm의 reparameterization을 만나게 되면 더 커지는데, loss는 effective한 $ beta$-smoothness 효과를 가지게 된다. 아래의 그래프에서 확인 할 수 있다. . $ beta$-smoothness란? . Recall that f is β-smooth if its gradient is β-Lipschitz . ∣f(x)−f(y)−∇f(y)T(x−y)∣≤β2∥x−y∥2 rvert f(x) -f(y) - nabla f(y)^T(x- y) rvert le frac{ beta}{2} rVert x-y rVert^2∣f(x)−f(y)−∇f(y)T(x−y)∣≤2β​∥x−y∥2 https://arxiv.org/pdf/1405.4980.pdf . . 이 smooth 효과는 training algorithm에 매우 효과적이다. non-BatchNorm의 loss function은 non-convex할 뿐만 아니라 kinks, flat regions , sharp minima의 문제를 가지고 있다. 이 문제는 gradient 방법론이 수렴하기 불안정하도록 만든다. 하지만 BatchNorm을 적용하게 되면, gradient가 reliable하고 predictive한 방향으로 나오게 된다. 무엇보다도 개선된 Lipschitzness는 learning step을 크게 잡을 수 있게 해준다. 아래의 그래프를 보면 그 효과를 확인할 수 있다. . . Exploration of the optimization landscap . . 위의 그래프의 Figure 4(a)를 보면 step마다의 loss변화를 알 수 있다. 이를 통해서 non-BatchNorm의 방식은 loss의 변화량이 BatchNorm보다 크다는 것을 알 수 있다. 이런 현상은 초기 학습과정에서 특히 심하다. . 또한 gradient의 predictiveness도 살펴볼수 있다. Figure 4(b)를 보면, 이를 확인할 수 있는데, predictiveness는 주어진 시점에서의 loss gradient와 다른 시점에서의 loss gradient간의 $l_2$ distance로 정의하였다. . Figure 4(c)에서는 BatchNorm의 loss gradient stability/Lipschitzness의 향상을 확인할 수 있다. 이는 gradient direction 뿐만 아니라, random direction에 대해서도 결과가 유사하게 나왔다. . Is BatchNorm the best (only?) way to smoothen the landscape? . 실험결과 $l_1$ normalization의 결과가 BatchNorm보다 좋게 나왔다. 기억할 것은 $l_p$ normalization은 distribution shift를 일으킨다는 것이다. 하지만, 여전히 성능을 향상 시킨다. (결국, ICS와 성능향상은 무관하다는 것이다.) . 아래의 실험결과를 보면, 꼭 BatchNorm을 고집할 이유는 없어 보인다. . . Theoretical Analysis . Setup . fully-connected layer W에 single BatchNorm을 추가한 효과를 분석하고자 한다. Figure 5(b)와 같은 상황을 가정한다. 주목할 점은 input에 BatchNorm을 적용한 것이 아니라, layer W의 output에 BatchNorm을 적용한다. 이는 해당 논문의 분석이 단지 input에 대한 normalization 효과에 대한 것이 아니라 reparameterization에 대한 것임을 알려준다. . . Notation . layer weights: $W_{ij}$ | Figure 5(a) network와 Figure 5(b) network는 모두 동일한 loss function을 가지고 있다. (loss function내부에는 non-linear layer들이 추가적으로 있을 수 있다.) | BatchNorm loss: $ hat{ mathcal{L}}$ | 위의 두 network모두 input $x$과 activation $y = Wx$를 가진다. BatchNorm의 경우에는 $ hat{y}$를 추가적으로 가지게 되는데 이는 normalized된 activation이다.(mean=0, var=1) | $ gamma, beta$는 모두 constant라고 가정한다. | $ sigma_j$는 batch의 output $y_i in R^m$의 stadard deviation이다. | . Theoretical Results . activation $y_i$에 대한 optimization landscape를 생각해보자. 앞서 BatchNorm이 결국 landscape가 더 잘 작동하도록 만든다는 것을 실험을 통해 증명하였다. (Lipschitz-continuity, and predictability of the gradients ) 이 논문에서는 activation-space상에서 landscape가 weight -space landscape에서의 worst-case bounds가 된다는 것을 증명할 것이다. . gradient magnitude $ rVert nabla_{y_i} mathcal{L} rVert$에 대해서 먼저 생각해보자. 이는 Lipschitzness를 나타내주는 지표이다. loss의Lipschitzness는 optimization문제에서 큰 역할을 한다고 알려져 있다. (이 지표는 결국 loss가 training step에 따라서 얼마나 변할지 알려주기 때문) . Theorem 4.1 (The effect of BatchNorm on the Lipschitzness of the loss). . For a BatchNorm network with loss $ hat{ mathcal{L}}$ and an identical non-BN network with (identical) loss $ mathcal{L}$, . ∥∇yiL∥2≤γ2σj2(∥∇yiL∥2−1m⟨1,∇yiL⟩2−1m⟨∇yiL,y^j⟩2) rVert nabla_{y_i} mathcal{L} rVert^2 le frac{ gamma^2}{ sigma_j^2} left( rVert nabla_{y_i} mathcal{L} rVert^2 - frac{1}{m} left langle 1, nabla_{y_i} mathcal{L} right rangle ^ 2 - frac{1}{m} left langle nabla_{y_i} mathcal{L}, hat{y}_j right rangle ^ 2 right)∥∇yi​​L∥2≤σj2​γ2​(∥∇yi​​L∥2−m1​⟨1,∇yi​​L⟩2−m1​⟨∇yi​​L,y^​j​⟩2) . 해당 논문에서는 어떤 가정도 없이 BatchNorm이 더 개선된 Lipschitzness를 가진다고 증명하였다. 게다가 Lipschitz constant는 normalized activation $ hat{y}$가 gradient $ nabla_{y_i} mathcal{L}$ 혹은 0에서의 gradient deviates값의 mean값과 상관관계가 있을 때 감소되는 것을 확인할 수 있었다. 이 효과는 BN의 scaling이 기존 layer의 scaling과 일치할 때도 나타났다. 아래는 appendix에서 가져온 것이다. . Fact C.1 Gradient throgh BatchNorm . notation . gradient through BN: $ frac{ partial f}{ partial A^{(b)}}$ | another function:$f := f(C) $ where $C = gamma cdot B + beta$ and $B = BN_{0, 1}(A) := frac{A-u}{ sigma}$ | scalar elements of a batch size of size m and variance $ sigma^2$: $A^{(b)}$ | . ∂f∂A(b)=γmσ(m∂f∂C(b)−∑k=1m∂f∂C(k)−B(b)∑k=1m∂f∂C(k)B(k)) frac{ partial f}{ partial A^{(b)}} = frac{ gamma}{m sigma} left( m frac{ partial f}{ partial C^{(b)}} - sum_{k=1}^{m} frac{ partial f}{ partial C_{(k)}} - B^{(b)} sum_{k=1}^{m} frac{ partial f}{ partial C_{(k)}}B^{(k)} right)∂A(b)∂f​=mσγ​(m∂C(b)∂f​−k=1∑m​∂C(k)​∂f​−B(b)k=1∑m​∂C(k)​∂f​B(k)) . Fact C.2 Gradient of normalized outputs . convenient gradient of BN ∂y^(b)∂y(k)=1σ(1[b=k]=1m−1my^(b)y^(k) ) frac{ partial hat{y}^{(b)}}{ partial y^{(k)}} = frac{1}{ sigma} left(1[b=k] = frac{1}{m} - frac{1}{m} hat{y}^{(b)} hat{y}^{(k)} right)∂y(k)∂y^​(b)​=σ1​(1[b=k]=m1​−m1​y^​(b)y^​(k) ) 그러므로, frac}{ partial y^{(k)}} = frac{ gamma}{ sigma} left(1[b=k] = frac{1}{m} - frac{1}{m} hat{y}^{(b)} hat{y}^{(k)} right) . $ left langle 1, nabla_{y_i} mathcal{L} right rangle ^ 2$ 은 해당 차원에서 quadratically하게 증가한다. 그러므로 중요한 term이다. 게다가 $ left langle nabla_{y_i} mathcal{L}, hat{y}_j right rangle ^ 2$은 zero 값으로 부터 조금 떨어진 값이라고 기대되는데 이는 variable과 variable의 gradient term이 일반적으로 uncorrelated하기 때문이다. $ sigma_j$는 커지는 경향이 있는데$ gamma $-scaling을 해줌으로써 flatness가 되도록 해주는 효과를 기대할 수 있다. . proof . Fact C.1을 이용하여 다음과 같이 전개한다. . 자세한 증명은 해당 논문의 appendix를 참고하길 바란다. . . . Theorem 4.2 (The effect of BN to smoothness). . **Let $ hat{g}i = nabla{y_i} mathcal{L}$ and $H_{j j} = frac{ partial mathcal{L}}{ partial{y_i} partial{y_i}}$ be the gradient and Hessian of the loss with respect to the layer outputs respectively. Then ** (∇yiL^)T∂L^∂yi∂yi(∇yiL^)≤γ2σj2(∂L^∂yi)Hjj(∂L∂yi)−γmσ2⟨gj^,yj^⟩∥∂L∂yi∥2 left( nabla_{y_i} hat{ mathcal{L}} right)^T frac{ partial hat{ mathcal{L}}}{ partial{y_i} partial{y_i}} left( nabla_{y_i} hat{ mathcal{L}} right) le frac{ gamma^2}{ sigma_j^2} left( frac{ partial hat{ mathcal{L}}}{ partial{y_i}} right) H_{jj} left( frac{ partial mathcal{L}}{ partial{y_i}} right) - frac{ gamma}{m sigma^2} left langle hat{g_j}, hat{y_j} right rangle rVert frac{ partial mathcal{L}}{ partial{y_i}} rVert^2(∇yi​​L^)T∂yi​∂yi​∂L^​(∇yi​​L^)≤σj2​γ2​(∂yi​∂L^​)Hjj​(∂yi​∂L​)−mσ2γ​⟨gj​^​,yj​^​⟩∥∂yi​∂L​∥2 . 만약,$ hat{g_j}, nabla_{y_i} hat{ mathcal{L}}$의 relative norm을 보존하는 $H_{jj}$를 가지고 있다면? . (∇yiL^)T∂L^∂yi∂yi(∇yiL^)≤γ2σj2(gi^THjjg^i−1mγ⟨gj^,yj^⟩)∥∂L∂yi∥2 left( nabla_{y_i} hat{ mathcal{L}} right)^T frac{ partial hat{ mathcal{L}}}{ partial{y_i} partial{y_i}} left( nabla_{y_i} hat{ mathcal{L}} right) le frac{ gamma^2}{ sigma_j^2} left( hat{g_i}^TH_{jj} hat{g}_i - frac{1}{m gamma} left langle hat{g_j}, hat{y_j} right rangle right) rVert frac{ partial mathcal{L}}{ partial{y_i}} rVert^2(∇yi​​L^)T∂yi​∂yi​∂L^​(∇yi​​L^)≤σj2​γ2​(gi​^​THjj​g^​i​−mγ1​⟨gj​^​,yj​^​⟩)∥∂yi​∂L​∥2 . 이제 landscape의 second-order 특성을 살펴보자. BatchNorm이 더해지면, loss Hessian(gradient direction에 대한 activation에 대한 hessian)은 input variance에 의해서 rescaled되고 increasing smoothness에 의해서 감소하게 된다. 이는 Taylor expansion에 의해서 도출할 수 있으며, 이 term을 감소시키는 것은 gradient가 더 predictive한 성격을 가지게 한다. . $ left langle hat{y_j}, hat{g_j} right rangle$이 non-negative 한 성격을 가지고 Hessian을 가진다면, 위의 theorem은 더 예측가능한 gradient값을 가진다.(predictive gradient) Hessian은 loss가 locally convex하게 되면, positive semi-definite한 성격을 가지게 된다. . Theorem 4.4 (Minimax bound on weight-space Lipschitzness). . For a BatchNorm network with loss $ hat{ mathcal{L}}$ and an identical non-BN network (with identical loss $ mathcal{L}$), if . 여기서는 BatchNorm이 layer weights에 대한 worst-case bound역할을 하는 것을 보일 것이다. gj=max⁡∥X∥≤λ∥∇WL∥2g_j = max_{ rVert X rVert le lambda} rVert nabla_W mathcal{L} rVert^2gj​=max∥X∥≤λ​∥∇W​L∥2 . gj^=max⁡∥X∥≤λ∥∇WL^∥2⇒g^j≤γσj2(gj2−mugj2−λ2⟨∇yjL,y^j⟩2) hat{g_j} = max_{ rVert X rVert le lambda} rVert nabla_W hat{ mathcal{L}} rVert^2 Rightarrow hat{g}_j le frac{ gamma}{ sigma_j^2} left( g_j^2 - mu^2_{g_j} - lambda^2 left langle nabla_{y_j} mathcal{L}, hat{y}_j right rangle^2 right)gj​^​=∥X∥≤λmax​∥∇W​L^∥2⇒g^​j​≤σj2​γ​(gj2​−mugj​2​−λ2⟨∇yj​​L,y^​j​⟩2) . 아래는 이에 대한 증명이다. . . . Lemma 4.5 (BatchNorm leads to a favourable initialization). . *Let $W^$ and $ hat{W}^*$ be the set of local optima for the weights in the normal and BN networks, respectively. For any initialization $W_0$ ** . initialization에서도 성능 향상이 있었다. . Reference . https://arxiv.org/abs/1805.11604 | .",
            "url": "https://fastpages.fast.ai/interview/deeplearning/2019/07/11/How-Does-Batch-Normalization-Help-Optimization.html",
            "relUrl": "/interview/deeplearning/2019/07/11/How-Does-Batch-Normalization-Help-Optimization.html",
            "date": " • Jul 11, 2019"
        }
        
    
  
    
        ,"post27": {
            "title": "Batch Normalization에 대하여",
            "content": "Batch Normalization에 대하여 . Problem Define . 학습하는 과정에서 이전 layer의 parameter가 변하면서, 각 layer의 input들의 distribution이 training과정마다 변하게 된다. 이런 문제는 학습이 불안정하게 하며, 낮은 learning rate를 사용해야 학습이 진행된다. 결론적으로는 saturating non-linearity의 모델을 학습하기 어려워진다. 이런 현상을 internal covariate shift 라고 부른다. . saturating non-linearity: 어떤 입력이 무한대로 갈 때 함수값이 어떤 범위내에서만 움직이는 것 . ex) sigmoid . not-saturating non-linearity: 어떤 입력이 무한대로 갈 때 함수값도 무한대로 가는 것을 의미 . ex) Relu . sigmoid activation에 대해서 생각해보면, 위의 문제가 왜 심각한지 알 수 있다. sigmoid function은 saturating function중 하나로 $ rvert x rvert $가 증가할 수록 gradient값이 0에 수렴한다. . g(x)=11+exp⁡(−x)g(x) = frac{1}{1 + exp(-x)}g(x)=1+exp(−x)1​ . . layer의 depth가 깊어질수록 이 문제는 더 커지게 되는데, 이런 문제를 해결하기 위해서 Relu를 많이 사용한다. 하지만, Batch normalizing을 사용하게 되면 stable한 distribution을 가지게 되어서 이런 문제를 해결할 수 있다. . Towards Reducing Internal Covariate Shift . deep learning model은 많은 layer가 연결되어 있는 구조이다. layer가 2인 모델을 가정해보면, 다음과 같이 수식으로 나타낼 수 있다. h1=F1(x)h_1 = F_1(x)h1​=F1​(x) . output=F2(h1)output = F_2(h_1)output=F2​(h1​) . where $h_1$ is hidden layer, $F_1$ is first layer, $F_2$ is second layer. . 위의 구조에서 볼 수 있듯이 $F_2$는 $F_1$의 dependent 하다고 할 수 있다. 학습이 진행되는 과정을 보면 internal covariate shift에 대해서 알 수 있다. . 첫번째 batch로 output을 구하고 실제 target과의 차이로 loss를 정의한다. | loss를 바탕으로 gradient를 구한다. | parameters를 update한다. | 위의 1 ~ 3번의 과정을 반복하는게 학습의 과정이다. 하지만, 3번의 parameter update과정에서 $F_1, F_2$ layer가 변하게 되는데 이는 distribution이 변하는 것으로 해석할 수 있다. 직관적으로 생각해보면, $F_2$ layer는 update 되기전의 $F_1$을 바탕으로 학습을 진행했는데, 그 다음 step에서 갑자기 변한 $F_1$ layer를 바탕으로 학습을 진행해야 하는 것이다. . 이렇게 학습이 진행되면, gradient step은 normalization이 진행되야되는 방향으로 학습이 진행되며 이는 gradient의 효과를 경감시킨다. 이는 아래의 수식 전개를 보면 확인할 수 있다. 해당 수식은 bias parameter만 가진다고 가정한다. . Notation . input: $u$ | learned bias: $b$ | activation computed over training set: $ hat{x} = x - E[x]$ | $x=u +b$, $ mathcal{X} ={ x_{1 cdots N} }$ | $E[x] = frac{1}{N} sum_{i=1}^Nx_i$ | . 만약 $E[x]$가 $b$에 미치는 영향을 무시하고 학습한다면, 아래와 같이 update된다. . b←b+∇bb leftarrow b + nabla bb←b+∇b . ∇b∝−∂l∂x^ nabla b varpropto frac{- partial l}{ partial hat{x}}∇b∝∂x^−∂l​ . 다음 feed forward 과정을 생각해보면 다음과 같다. ($b$는 update되기전 parameter) u+(b+∇b)−E[u+∇b]=u+b−E[u+b]u + (b + nabla b) - E[u + nabla b] = u + b - E[u + b]u+(b+∇b)−E[u+∇b]=u+b−E[u+b] 위의 식에서 볼 수 있듯이 $ nabla b$가 사라짐으로써 학습효과가 없게 된다. . Fixed Distribution . 이런 문제를 해결하기 위해서, 어떤 parameter 값들을 가지든 의도한 distribution이 나오도록 만들어야 한다. distribution이 고정되면, gradient가 normalization에 dependent하게 만들어진다. . notation . layer input: $x$ | set of inputs over training set: $ mathcal{X}$ | normalization: $ hat{x} = Norm(x, mathcal{X})$ | . 위의 normalization term은 training example $x$뿐만 아니라 모든 examples $ mathcal{X}$에 영향을 받는다. 만약 $x$가 이전 layer의 output이라면, $ mathcal{X}$은 이전 layer parameter에 영향을 받는다. . backpropagation 과정에서는 Jacobians를 계산해야한다. . (1) $x$에 대한 gradient ∂Norm(x,X)∂x frac{ partial Norm(x, mathcal{X})}{ partial x}∂x∂Norm(x,X)​ . (2) $ mathcal{X}$ 에 대한 gradient ∂Norm(x,X)∂X frac{ partial Norm(x, mathcal{X})}{ partial mathcal{X}}∂X∂Norm(x,X)​ . 만약 (2)를 고려하지 않게되면, 위에서 언급한 문제가 발생할 수 있다. 하지만 이는 매우 비싼 computation cost를 치뤄야 한다. . [covariance matrix] Cov[x]=Ex∈X[XXT]−E[x]E[x]TCov[x] = E_{x in mathcal{X}}[XX^T] -E[x]E[x]^TCov[x]=Ex∈X​[XXT]−E[x]E[x]T [inverse square root]: to produce the whitened activations Cov[x]−1/2(x−E[x])Cov[x]^{-1/2}(x - E[x])Cov[x]−1/2(x−E[x]) 기타 backpropagation과정에서의 derivatives들도 많은 computation cost를 치뤄야한다. . 어떻게 하면 합리적인 computation cost로 모델의 representation ability를 보존할 수 있을까? . Normalization via Mini-Batch Statistics . 위에서 언급했듯이, 모든 layer의 input에 대한 full whitening은 computation cost가 높고, 미분가능하지 않을 수도 있다. 이런 문제를 해결하기 위해서 두가지 가정을 한다. . [whitening] . 이는 기저벡터(eigenbasis) 데이터를 아이겐밸류(eigenvalue) 값으로 나누어 정규화는 기법이다. 화이트닝 변환의 기하학적 해석은 만약 입력 데이터가 multivariable gaussian 분포를라면 화이트닝된 데이터는 평균은 0이고 공분산(covariance)는 단위행렬을 갖는 정규분포를 갖게된다. 와이트닝은 다음과 같이 구할 수 있다: . # whiten the data: # divide by the eigenvalues (which are square roots of the singular values) Xwhite = Xrot / np.sqrt(S + 1e-5) . layer의 input과 output의 feature를 jointly하게 구하는 것이 아니라, 각 feature를 독립적으로 mean=0, var=1을 가지도록 정규화한다. ($ hat{x}^{(k)}$는 각 layer의 input의 k번째 dimension 성분) x^(k)=x^(k)−E[x^(k)]Var[x^(k)] hat{x}^{(k)} = frac{ hat{x}^{(k)}- E[ hat{x}^{(k)}]}{ sqrt{Var[ hat{x}^{(k)}]}}x^(k)=Var[x^(k)]​x^(k)−E[x^(k)]​ decorrelated feature에도 불구하고 해당 normalization은 convergence 속도를 빠르게 한다고 알려져 있다.(Neural Networks: Tricks of the trade - 1998) | 하지만, 기억해야 될 것은 normalizing이 layer의 representation능력에 변화를 준다는 것이다. 예를 들어, Sigmoid activation을 사용할 경우, -1 +1 사이로 normalizing이 진행되어 non-linearity의 특성을 잃어버리게 된다. . 이 문제를 해결하기 위해서는 결국 normalizing이 같은 representation을 하도록 해야한다. . $ hat{x}^{(k)}$는 activation을 의미하고 $ gamma^{(k)}, beta^{(k)}$는 parameter를 의미한다. 이 parameter는 학습시에 모델의 다른 parameter와 같이 학습되며 모델의 representation power를 유지하는 방향으로 학습이 진행된다. y^(k)=γ(k)x^(k)+β(k) hat{y}^{(k)} = gamma^{(k)} hat{x}^{(k)}+ beta^{(k)}y^​(k)=γ(k)x^(k)+β(k) . 만약 아래와 같이 파라미터 값을 지정한다면, 원래의 representation을 복원할 수 있다. . γ(k)=Var[x(k)] gamma^{(k)} = sqrt{Var[x^{(k)}]}γ(k)=Var[x(k)]​ β(k)=E[x(k)] beta^{(k)} = E[x^{(k)}]β(k)=E[x(k)] 일반적으로 Stochastic Gradient Training을 하기 때문에 각 mini-batch activation에 해당하는 variance mean를 사용하게 된다. 이는 normalization이 backpropagation과정에 적절히 관여하게 만든다. . | 중요한 점은 per-dimension variance를 구하는 것이 computation cost를 낮춘다는 것이다. (Singluar covariance matrices) 아래는 batch normalization algorithm에 대한 설명이다. 주목할 점은 learned parameter $ gamma, beta$가 training example 뿐 아니라 mini-batch 안에 있는 다른 training example에 영향을 받는다는 것이다. ($ epsilon$은 stability를 위한 constant term이다.) . . [Backpropagation 전개] . . Training and Inference with BatchNormalized Networks . Training은 위와 같이 진행하면 되지만, inference를 할 때는 더 적절한 방법이 필요하다. 모델의 output이 input에 deterministic하게 나오도록 해야한다. 이를 위해서 다음과 같은 normalization이 필요하다. x^=x−E[x]Var[x]+ϵ hat{x} = frac{x-E[x]}{ sqrt{Var[x] + epsilon}}x^=Var[x]+ϵ​x−E[x]​ 주목할 점은 여기서 나오는 $E[x], Var[x]$는 모두 mini-batch에서 얻은 것이 아니라 population에서 얻은 것이다. 위의 식에서도 mean=0, var=1로 유지된다. $Var[x]$는 sample variance $ sigma_b^2$로 부터 다음과 같이 구한다. Var[x]=mm−1⋅EB[σB2]Var[x] = frac{m}{m-1} cdot E_B[ sigma_B^2]Var[x]=m−1m​⋅EB​[σB2​] training과정에서 moving average를 사용하는 것과 다르게 inference과정에서는 고정된 mean과 variance를 통해서 deterministic한 output을 도출한다. 이는 각 layer마다 linear transformation을 한 것으로 해석할 수 있다. 알고리즘은 아래와 같다. . . Batch-Normalized Convolutional Networks . Affine transformation에 어떻게 적용될 수 있는지 살펴보자. . affine transformation . 선형변환에 평행이동이 더해진 개념이다. . z=g(Wu+b)z = g(Wu + b)z=g(Wu+b) . where $g( cdot)$ is non-linear such as sigmoid or Relu. . 위의 수식은 inputs $u$에 대해서 $x=Wu + b$에 직접 Batch normalization을 적용할 수 있다. 이런 생각을 할 수 있다. input $u$에 batch normalization을 적용 할 수 있지 않을까? 하지만 문제가 있다. $u$는 결국 다른 non-linearity layer의 output이기 때문에 distribution이 training 과정마다 바뀐다. 그리고 첫번째 두번째에 제한을 한다고 해도 결국 covariate shift는 제거할 수 없다. . 반면에, $Wu +b$는 symmetric, non-sparse distribution을 가진다. 따라서 이를 normalizing한다면 더 안정적인 결과를 얻을 수 있다. . 위에서 언급했듯이 bias $b$는 backpropagation과정에서 무시될 수 있는데 이런 문제를 해결위해서 아래와 같이 위의 수식을 바꿀 수 있다. z=g(BN(Wu))z=g(BN(Wu))z=g(BN(Wu)) 여기서 $BN$은 $Wu$의 각 dimention마다 독립적으로 적용되며, $ gamma^{(k)}, beta^{(k)}$를 얻을 수 있다. ($k$는 각 dimension index를 의미한다.) . 그렇다면, Convolution layer에서는 어떻게 적용될 지 살펴보자. 아래는 Convolution filter의 작동방법이다. . . CNN의 특징중 하나는 ‘local connected’라는 것이다. . local connected . 국지적인 정보의 집합을 이용하는 것을 의미한다. 즉 위의 그림처럼 filter안에 있는 정보끼리만 영향을 주며 filter가 이동할 때 이전 filter의 영향을 받지 않는다. . Convolution layer에서는 BN transformation이 다음과 같이 적용된다. . 추가적인 normalization이 필요하다. 이를 통해서 같은 feature map상에서 서로 다른 위치에 있는 구성요소를 공통적으로 normalize할 수 있다. . | 이를 위해서 mini-batch상에 모든 activation을 location에 대해서 normalize를 진행한다. (jointly) . | 아래의 Alg.1에서 $B$를 한 feature map에서의 각 location에 대한 activation value로 설정한다.(in mini-batch). mini-batch $B$의 크기는 feature map크기가 $p times q$라고 가정하고 $m cdot pq$가 된다. 따라서 $ gamma^{(k)}, beta^{(k)}$는 activation마다가 아니라 feature map마다 구하게 된다. . | . | . Batch Normalization enables higher . Batch Normalization을 적용하면 더 높은 learning rate를 사용할 수 있으며, 이는 모델의 수렴속도를 높여준다. . 일반적으로 parameter scale이 높으면, model explosion현상이 발생한다. 하지만, batch normalization을 사용하면 parameter scale의 영향을 받지 않는다. 또한 큰 parameter scale에도 smaller gradient를 가진다. BN(Wu)=BN((aW)U)BN(Wu) = BN((aW)U)BN(Wu)=BN((aW)U) . ∂BN((aW)u)∂u=∂BN(Wu)∂u frac{ partial BN((aW)u)}{ partial u} = frac{ partial BN(Wu)}{ partial u}∂u∂BN((aW)u)​=∂u∂BN(Wu)​ . ∂BN((aW)u)∂aW=1a⋅∂BN(Wu)∂aW frac{ partial BN((aW)u)}{ partial aW} = frac{1}{a} cdot frac{ partial BN(Wu)}{ partial aW}∂aW∂BN((aW)u)​=a1​⋅∂aW∂BN(Wu)​ . 또한, 해당논문에서는 BN이 layer jacobians가 1에 가까운 singular value를 가진다는 것을 발견했다. (이는 train할 때 유용한 특성이다.) . normalized vector: $ hat{z} =F( hat{x})$ . | 가정: $ hat{x}, hat{z}$는 uncorrelated되어 있으며, gaussian을 따른다. 또한, 함수 $F( hat{x}) approx J hat{x}$는 linear transformation이다. . | $ hat{x}, hat{z}$은 다음과 같은 covariance를 가진다. I=Cov[z^]=JCov[x^]JT=JJTI = Cov[ hat{z}] = JCov[ hat{x}]J^T=JJ^TI=Cov[z^]=JCov[x^]JT=JJT 따라서, $JJ =I$이고 singular value는 1이다. 이는 gradient magnitude를 보존하는 역할을 한다. . 사실 real-world에서는 위의 가정이 사실이라고 하기 힘들지만 그래도 BN의 역할을 알 수 있다. . | . Batch Normalization regularizes the model . batch normalization은 Drop-out처럼 regularization효과가 있다. . Reference . https://arxiv.org/abs/1502.03167 | .",
            "url": "https://fastpages.fast.ai/interview/deeplearning/2019/07/08/Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC.html",
            "relUrl": "/interview/deeplearning/2019/07/08/Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC.html",
            "date": " • Jul 8, 2019"
        }
        
    
  
    
        ,"post28": {
            "title": "MLE 와 MAP에 대하여",
            "content": "MLE: Maximum Likelihood Estimation . liklihood란, 이미 주어진 표본적 증거로 비추어보았을 때, 모집단에 관해 어떠한 통계적 추정이 그럴듯한 정도를 의미합니다. 수식으로 나타내면, $P(D mid W)$ 처럼 나타낼 수 있다. (D = observation, W = parameters) . . 동전던지기 예시를 생각해보면, 쉽게 이해할 수 있다. 일반적으로 동전 던지기를 해서 앞면이 나올 확률은 0.5라고 생각합니다. 하지만, 이는 우리가 가정한 값이지 실제의 값은 아닙니다. 이런 이유로 몇 번의 수행결과로 동전의 앞면이 나올 확률 $P(H)$를 정의하고자 합니다. . 만약 동전을 100번 던졌을 때, 동전의 앞면이 56번 나왔다면 ‘동전의 앞면이 나올 확률’은 몇이라고 얘기할 수 있을까? 이 문제의 해답이 Maximum Likelihood를 구하는 것이다. 즉, observation이 주어졌을 때, 가장 그럴듯한 가설(혹은 parameter)를 찾는 문제가 Maximum Likelihood Estimation이다. . 동전 던지기는 이항분포를 따릅니다. x는 100번 던졌을 때 앞면이 나온 횟수를 의미하고 p는 앞면이 나올 확률을 의미합니다. p(x)=(nx)px(1−p)n−xp(x) = begin{pmatrix} n x end{pmatrix} p^x(1-p)^{n-x}p(x)=(nx​)px(1−p)n−x likelihood는 다음과 같이 정의됩니다. ($X$는 앞면이 나온 횟수, $ theta$는 앞면이 나올 확률) P(X∣θ)P(X mid theta)P(X∣θ) 아래는 likelihood의 그래프입니다. . . 앞면이 나올 확률이 0. 56인 지점에서 가장 높은 likelihood를 가지고 있음을 알 수 있습니다. . θ^=argmaxθP(X∣θ) hat{ theta} = argmax_{ theta}P(X mid theta)θ^=argmaxθ​P(X∣θ) . θ^=0.56 hat{ theta} = 0.56θ^=0.56 . Classification: MLE와 KL-divergence, Cross Entropy . KL-divergence는 다음과 같이 정의됩니다. . DKL(P∥Q)=−∑x∈XP(x)log⁡Q(x)P(x)D_{KL}(P rVert Q) = - sum_{x in X}P(x) log{ frac{Q(x)}{P(x)}}DKL​(P∥Q)=−x∈X∑​P(x)logP(x)Q(x)​ . DKL(P∥Q)=−∑x∈XP(x)(log⁡Q(x)−log⁡P(x))D_{KL}(P rVert Q) = - sum_{x in X}P(x)( log{Q(x)} - log{P(x)})DKL​(P∥Q)=−x∈X∑​P(x)(logQ(x)−logP(x)) . 즉, Q distribution과 P distribution간의 차이를 나타내주는 역할을 합니다. 만약 두 분포간의 차이를 나타내는데 필요한 정보량이 많다면, 두 분포는 차이가 크다고 할 수 있습니다. . Cross entropy는 다음과 같이 정의 됩니다. . H(p,q)=−∑x∈Xp(x)log⁡q(x)=Ep[−log⁡q]H(p, q) = - sum_{x in X}p(x) log{q(x)} = E_p[- log{q}]H(p,q)=−x∈X∑​p(x)logq(x)=Ep​[−logq] . 실제 분포 $p$에 대해서 $- log{q}$를 평균내는 것입니다. 만약, $q$가 p와 유사할 수록 cross entropy는 낮게 나올 것입니다. 사실 cross entropy는 kl-divergence에서 출발했다고 해석할 수도 있습니다. 아래 수식을 보면 $ log{P(x)}$는 실제 분포가 주어지면, constant term이 됩니다. 따라서 이를 제외하고 본 것이 cross entropy라고 해석할 수 있습니다. . DKL(P∥Q)=−∑x∈XP(x)(log⁡Q(x)−log⁡P(x))D_{KL}(P rVert Q) = - sum_{x in X}P(x)( log{Q(x)} - log{P(x)})DKL​(P∥Q)=−x∈X∑​P(x)(logQ(x)−logP(x)) . 그렇다면, MLE와 KLD, cross entropy는 어떤 관계를 가질까요? 결론부터 말씀드리면, 이들은 본질적으로 같은 일을 합니다. 아래는 수식입니다. begin{align} theta_{ML} = arg max_{ theta}P_{model}(X mid theta) = arg max_{ theta}E_{X sim P_{data}}[ log{P_{model}(x mid theta)}] end{align} . DKL(P∥Q)=Ex∼Pdata[log⁡Pdata(x)−log⁡Pmodel(x)]D_{KL}(P rVert Q) = E_{x sim P_{data}}[ log{P_{data}(x)} - log{P_{model}(x)}]DKL​(P∥Q)=Ex∼Pdata​​[logPdata​(x)−logPmodel​(x)] . H(p,q)=Ep[−log⁡q]H(p, q) = E_p[- log{q}]H(p,q)=Ep​[−logq] . KLD를 최소화한다는 것은 결국 cross entropy를 최소화하는 것과 같습니다. 그리고 cross entropy를 최소화 하는 것은 결국 likelihood를 최대화하는 것과 같습니다. 따라서 MLE가 하는 것은 모델이 추정한 데이터의 분포와 실제 데이터의 분포를 가장 유사하게 만들어주는 parameter를 찾는 것이라고 해석할 수 있습니다. . Regression: MLE와 Method of Least Squares . regression task에서는 MLE를 다음과 같이 정의할 수 있습니다. . begin{align} theta_{ML} = arg max_{ theta} P_{model}(Y mid X; theta)= arg max_{ theta} sum_{i=1}^{m} log{P_{model}(y_i mid x_i; theta)} end{align} . $P_{model}$이 Gaussian distribution이라고 가정하겠습니다. 위의 log term은 아래와 같이 전개됩니다. . log⁡Pmodel(yi∣xi;θ)=−mlog⁡σ−m2log⁡2π−∑i=1m∥y^i−yi∥22σ2 log{P_{model}(y_i mid x_i; theta)} = -m log{ sigma} - frac{m}{2} log{2 pi}- sum_{i=1}^m frac{ rVert hat{y}_i - y_i rVert^2}{2 sigma^2}logPmodel​(yi​∣xi​;θ)=−mlogσ−2m​log2π−i=1∑m​2σ2∥y^​i​−yi​∥2​ . $ sigma^2$(분산)은 고정되어 있다고 가정하면, 결국 $ rVert hat{y}_i - y_i rVert^2$를 최소화하는 parameter $ theta$를 찾는 것이 MLE입니다. 이는 아래와 같이 결국 least square와 같은 일을 하게 됩니다. . Least squares는 모델에 input을 넣었을때 나오는 output이 실제 target y와 차이를 나타내는 수식이며 이를 가장 작게하는 파라미터 $ theta$를 찾는 것이 관건입니다. MSE=1m∑i=1m∥y^i−yi∥2MSE = frac{1}{m} sum_{i=1}^m rVert hat{y}_i - y_i rVert ^ 2MSE=m1​∑i=1m​∥y^​i​−yi​∥2 . MAP: Maximum a Posterior Estimation . MAP는 MLE와 다르게, prior라는 assumption을 사용합니다. . posterior $P(w mid D)$ 는 아래와 같이 정의됩니다. ($P(w)$는 prior $P(D mid W)$는 likelihood) P(w∣D)=P(D∣w)P(w)P(D)P(w mid D) = frac{P(D mid w) P(w)}{P(D)}P(w∣D)=P(D)P(D∣w)P(w)​ 위의 동전 던지기 예를 다시 살펴보겠습니다. . 만약 동전을 던졌을 때, 앞면이 나올확률이 0.5이라는 가정을 했다고 생각해봅시다. 그리고 실제 100번을 던졌을 때, 앞면이 70번이 나온상황을 보면 posterior는 다음과 같습니다. . P(T=0.5∣E=0.7)=P(E=0.7∣T=0.5)P(T=0.5)P(E=0.7)P(T=0.5 mid E=0.7)= frac{P(E=0.7 mid T=0.5)P(T=0.5)}{P(E=0.7)}P(T=0.5∣E=0.7)=P(E=0.7)P(E=0.7∣T=0.5)P(T=0.5)​ . 만약 Maximum a posterior estimation을 한다고 하면, prior를 일종의 variable로 설정하고 구할 수 있습니다. . P(T=x∣E=0.7)=P(E=0.7∣T=x)P(T=x)P(E=0.7)P(T=x mid E=0.7)= frac{P(E=0.7 mid T=x)P(T=x)}{P(E=0.7)}P(T=x∣E=0.7)=P(E=0.7)P(E=0.7∣T=x)P(T=x)​ 결국은 가장 데이터에 어울리는 prior를 구하는 과정이 MAP라고 할 수 있습니다. . Bayesian interpretation of Ridge regularization . . regression task를 가정했을 때, residual sum of squares(RSS)가 최소화되는 parameter $ beta$를 찾아야합니다. β^=arg⁡min⁡β(y−Xβ)T(y−Xβ) hat{ beta} = arg min_{ beta}(y-X beta)^T(y-X beta)β^​=argminβ​(y−Xβ)T(y−Xβ) . 여기서 ridge regression은 다음과 같이 penalty term의 역할을 합니다. 위의 식과 다르게 RSS와 $ beta$의 크기도 함께 고려해야 하는 것을 의미합니다. ($ rVert beta rVert_2^2 = beta_1^2 + beta_2^2 + cdots beta_p^2$) 결국 model의 complexity를 제한 하는 역할을 하는 것으로 해석할 수 있습니다. . β^=arg⁡min⁡β(y−Xβ)T(y−Xβ)+λ∥β∥22 hat{ beta} = arg min_{ beta}(y-X beta)^T(y-X beta) + lambda rVert beta rVert_2^2β^​=argβmin​(y−Xβ)T(y−Xβ)+λ∥β∥22​ . ridge regression은 bayesian 관점에서 해석할 수 있습니다. 다음은 regression task에 gaussian distribution을 가정한 것입니다. y∣X,β∼N(Xβ,σ2I)y mid X, beta sim mathcal{N}(X beta, sigma^2I)y∣X,β∼N(Xβ,σ2I) frequentism에서는 $ beta$가 고정된 값이지만, bayesian에서 $ beta$는 prior distribution을 가진다. $ beta$를 Normal distributioni으로 가정해보겠습니다. β∼N(0,τ2I) beta sim mathcal{N}(0, tau^2 I)β∼N(0,τ2I) posterior는 다음과 같이 전개 됩니다. begin{align} p( beta mid y, X)&amp; varpropto p( beta) cdotp(y mid X, beta) &amp; varpropto exp[- frac{1}{2}( beta-0)^T frac{1}{ tau^2}I( beta - 0)] cdot exp[- frac{1}{2}(y-X beta)^T frac{1}{ sigma^2}(y-X beta) &amp;= exp[- frac{1}{2 sigma^2}(y-X beta)^T(y-X beta) - frac{1}{2 tau^2} rVert beta rVert_2^2] end{align} 위의 식을 통해서 maximum a posterior를 구할 수 있습니다. begin{align} hat{ beta} &amp;= arg max_{ beta} exp[- frac{1}{2 sigma^2}(y-X beta)^T(y-X beta) - frac{1}{2 tau^2} rVert beta rVert_2^2] &amp;= arg min frac{1}{ sigma^2}(y-X beta)^T(y-X beta) + frac{1}{ tau^2} rVert beta rVert_2^2 &amp;= arg min (y-X beta)^T(y-X beta) + frac{ sigma^2}{ tau^2} rVert beta rVert_2^2 end{align} 위의 식을 통해서 $ lambda = frac{ sigma^2}{ tau^2}$으로 구할 수 있습니다. . 참고로 lasso regularization은 prior distribution을 laplace distribution을 사용하면 위와 같이 유도할 수 있습니다. . Reference . https://ratsgo.github.io/statistics/2017/09/23/MLE/ | 베이즈 정리와 MLE, MAP | https://statisticaloddsandends.wordpress.com/2018/12/29/bayesian-interpretation-of-ridge-regression/ | .",
            "url": "https://fastpages.fast.ai/interview/machine%20learning/2019/07/07/MLE-Maximum-Likelihood-Estimation.html",
            "relUrl": "/interview/machine%20learning/2019/07/07/MLE-Maximum-Likelihood-Estimation.html",
            "date": " • Jul 7, 2019"
        }
        
    
  
    
        ,"post29": {
            "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? 정리글",
            "content": "이 논문에서는 epistemic uncertainty와 aleatoric uncertainty를 하나의 모델에서 측정하는 것을 제안하고 있습니다. (이전의 연구에서는 위의 uncertainty를 따로 분리하여 측정했다고 합니다.) . . regression task에서 각각의 uncertainty에 대해서 알아보도록 하겠습니다. . Epistemic uncertainty . . [Fig. 1]을 보면, 파란색 영역은 variance를 나타내며, observation(흑색 점)이 있는 data point에서는 낮은 unceratinty를 보이고 그렇지 않은 곳에서는 높은 uncertainty를 보인다. 만약 높은 variance를 보이는 영역에 data point가 더 있다면 uncertainty는 줄어들 것이다. . 학습할 수 있는 데이터가 더 있다면 줄어들 수 있는 uncertainty를 ‘epistemic uncertainty’라고 한다. . epistemic uncertainty는 다음과 같은 모델에서 중요합니다. . Safety-ciritical applications: 자율주행자동차, 의료영상 등등 | Small dataset: training 데이터가 부족한 domain | . 만약, Gaussian Process에 대해서 더 자세히 알고 싶다면 A Visual Exploration of Gaussian Process를 참고하길 바랍니다. . Aleatoric uncertainty . . Aleatoric uncertainty는 data point 자체의 noisy를 의미한다. 이는 epistemic uncertainty와 다르게 더 많은 data point가 추가되어도 줄어들 수 없는 특성을 가지고 있다. . Heteroscedastic Aleatoric uncertainty . . heteroscedastic noise model은 homescedastic noise model과 다르게 data point마다 다른 uncertainty를 가지고 있습니다. 이를 바탕으로 Aleatoric heteroscedastic uncetainty를 생각해보면, input data에 따라서 달라지는 uncertainty를 가진다고 생각하면 될 것 같습니다. 이 논문에서는 ‘Heteroscedastic Aleatoric uncertainty’를 가정하고 문제를 풀고 있습니다. . aleatoric uncertainty는 다음과 같은 모델에서 중요합니다. . Large data situations | Real-time applications: Monte-Carlo sampling없이 aleatoric model을 만들수 있기 때문입니다. | . Related work . 그럼 기존의 연구에서 어떻게 epistemic, aleatoric uncertainty를 접근했는지 살펴보겠습니다. . 2.1 Epistemic Uncertainty in Bayesian Deep Learning . 우선, epistemic uncertainty는 model의 weight에 prior distribution을 가정합니다. 그리고 data에 따라서 weight가 변하는 양상을 측정합니다. . . nueral network의 weight에 prior distribution을 설정하기 위해서 일반적으로 Gaussian prior distribution 을 사용합니다 ($W sim mathbf{N}(0, I) $) 그리고 이를 Bayesian neural network(BNN)라고 합니다. . Bayesian neural network는 deterministic한 weight paramters를 distribution으로 바꾸고 network의 weight를 직접적으로 바꾸는 것이 아니라, 모든 가능한 weight의 marginalization을 구하여 평균값을 구합니다 . P(X)=∑yP(X,Y=y)=∑yP(X∣Y=y)×P(Y=y)P(X) = sum_y P(X, Y =y) = sum_y P(X|Y=y) times P(Y=y)P(X)=y∑​P(X,Y=y)=y∑​P(X∣Y=y)×P(Y=y) . Notation . $f^{W}(x)$: BNN의 random output . | $X=[{x_1, x_2, cdots, x_{n}}]$, $Y=[y_1, y_2, cdots, y_{n}]$ : datasets . | $p(y mid f^{W}(x))$ : likelihood . | $p(W mid X, Y)$: posterior distribution . | . BNN에서 posterior distribution의 역할은 주어진 data에서 가장 적절한 parameter를 찾아주는 것입니다.. (주어진 데이터에서 해당 prameter가 얼마큼의 확률을 가지는지 나타내주는 값) . regression regression task에서 어떻게 적용되는지 아래의 수식을 보면 알 수 있습니다. . p(y∣fW(x))=N(fW(x),σ2)p(y|f^{W}(x)) = mathbf{N}(f^{W}(x), sigma^2)p(y∣fW(x))=N(fW(x),σ2) . 수식을 보면, likelihood는 평균이 $f^{W}(x)$이고 observation noise가 $ sigma$인 Gaussian distribution을 따르는 것을 알 수 있습니다. . classification . p(y∣fW(x))=softmax(fW(x))p(y|f^{W}(x)) = mathbf{softmax}(f^{W}(x))p(y∣fW(x))=softmax(fW(x)) . classifiaction에서는 다른 방법을 취합니다. 일반적으로 위와 같이 output에 softmax를 취하는 형태로 likelihood를 측정합니다. . 하지만, bayesian neural network는 수식적으로는 쉽게 해결할 수 있으나 inference과정에서 어려움이 있습니다. 이는 marginal probability $p(X mid Y)$ 때문입니다. 이 probability는 아래와 같이 posterior p(W∣X,Y)p(W mid X, Y)p(W∣X,Y)를 계산할 때 필요합니다. . p(W∣X,Y)=p(Y∣X,W)P(W)/p(X∣Y)p(W|X, Y) =p(Y|X, W)P(W)/p(X|Y)p(W∣X,Y)=p(Y∣X,W)P(W)/p(X∣Y) . marginal probability $p(X mid Y)$가 가지는 의미를 한번 생각해보면, 이해하기 쉬울 것 같습니다. . 먼저, MNIST dataset의 경우에는 label 1이 주어졌을 때 image1이 나올 확률을 구해야합니다. 이는 계산하기는 힘들지만 할 수는 있을 것 같습니다. 하지만, label = $[1, 3, 5, cdots ]$가 주어졌을 때, $[image1, imga2, cdots]$가 나올확률을 모두 계산하는 것은 비효율적이며 현실적으로 불가능합니다. (모든 data들의 경우의 수를 탐색해야합니다.) . 위에서 posterior p(W∣X,Y)p(W mid X, Y)p(W∣X,Y)를 계산하기 힘든 이유를 언급하였습니다. 이러한 문제를 해결하기 위해서 Variational inference라는 방법론을 사용합니다. 간단하게 생각하면, posterior p(W∣X,Y)p(W mid X, Y)p(W∣X,Y) 대신에 Simple distribution qθ(W)q_{ theta}(W)qθ​(W)을 가정하고 parameter $ theta$에 의해서 posterior p(W∣X,Y)p(W mid X, Y)p(W∣X,Y)와 유사한 분포를 가지도록 조정합니다. . . 일반적으로 dropout은 overfitting을 막는 방법론으로 사용되고 있습니다. 하지만, dropout은 BNN과 유사한 inference를 할 수 있습니다. 이는 Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning 에서 확인할 수 있습니다. 간단하게 설명하면, 일반적으로 dropout은 train과정에서 적용하고 test과정에서는 제외하는데 반해서, bayesian inference를 하기 위해서 dropout을 train과 test과정에서 둘다 적용하여 사용합니다. 특히 test과정에서는 sampling을 하는데 dropout을 사용하고 있습니다. 이렇게 나온 sample을 바탕으로 inference를 진행합니다. . 결론적으로, dropout은 아래와 같이 ‘variational bayesian approximation’으로 해석될 수 있습니다. . L(θ,p)=−1N∑_i=1Nlog⁡p(y_i∣fW^_i(xi))+1−p2N∥θ∥2 mathbf{L}( theta, p) = - frac{1}{N} sum _{i=1}^{N} log{p(y _{i}|f^{ hat{W} _{i}}(x_{i}))} + frac{1-p}{2N} lVert theta rVert ^2L(θ,p)=−N1​∑_i=1Nlogp(y_i∣fW^_i(xi​))+2N1−p​∥θ∥2 . $N$은 data point를 의미하며, $p$는 dropout probability를 의미합니다. $ hat{W_i} sim q_{ theta}(W)$의 samples weight를 가지고 있고 $ theta$를 이용하여 최적화합니다. . regression task에서는 위의 log likelihood를 아래처럼 변형할 수 있습니다. . −logp(yi∣fW^i(xi))∝12σ2∥yi−fW^i(xi))∥2+12log⁡θ2-log{p(y_{i}|f^{ hat{W}_{i}}(x_{i}))} varpropto frac{1}{2 sigma^2} lVert y_i -f^{ hat{W}_{i}}(x_{i})) rVert ^2 + frac{1}{2} log{ theta ^ 2}−logp(yi​∣fW^i​(xi​))∝2σ21​∥yi​−fW^i​(xi​))∥2+21​logθ2 . 여기서 $ theta$는 output에 대한 noise를 의미합니다. (data 자체의 noise가 아닙니다.) . 위에서 언급했듯이 epistemic uncertainty 는 data point를 관측하면 감소되는 uncertainty입니다. . epistemic uncertainty를 이용하여, prediction uncertainty를 구할 수 있습니다. 아래는 classification task에서 Monte Carlo integration을 이용한 approximation입니다. . p(y=c∣x,X,Y)≈1T∑t=1TSoftmax(fW^i(xi))p(y=c|x,X,Y) approx frac{1}{T} sum_{t=1}^{T} mathbf{Softmax}(f^{ hat{W}_{i}}(x_{i}))p(y=c∣x,X,Y)≈T1​t=1∑T​Softmax(fW^i​(xi​)) . masked model weight $ hat{W_i} sim q_{ theta}(W)$, $q_{ theta}(W)$은 dropout distiribution입니다.. . probability vector $p$에 대한 uncertainty를 구할 때, entropy 개념을 이용하는데 이를 식으로 나타내면 아래와 같습니다. . H(p)=−∑c=1Cpclog⁡pcH(p) = - sum_{c=1}^{C}p_c log{p_c}H(p)=−c=1∑C​pc​logpc​ . regression task의 경우 epistemic uncertainty는 predictive variance로 나타낼 수 있으며, 이는 아래의 식과 같습니다. . Var(y) ≈θ2+1T∑t=1TfW^i(xi)TfW^i(xi)−E(y)TE(y) mathbf{Var}(y) approx theta^2 + frac{1}{T} sum_{t=1}^{T}f^{ hat{W}_{i}}(x_{i})^{T}f^{ hat{W}_{i}}(x_{i}) - mathbf{E}(y) ^ {T} mathbf{E}(y)Var(y) ≈θ2+T1​t=1∑T​fW^i​(xi​)TfW^i​(xi​)−E(y)TE(y) . 이 epistemic model은 $ mathbf{E}(y) approx frac{1}{T} sum_{t=1}^{T}f^{ hat{W}{i}}(x{i}) $ predictive mean과 근사하는 방향으로 학습이 진행됩니다.($E(y)$ 는 predictive mean) 첫번째 term $ theta ^ 2$은 data 자체의 noise를 의미합니다. (aleatoric) 이는 뒷부분에서 자세히 다루겠습니다. 두번째 term은 predictive variance로 예측값에 대한 uncertainty를 나타냅니다. (epistemic) . 참고로 aleatoric과 epistemic은 linear regression의 SSR, SSE의 개념과 유사합니다. . . 2. 2 Heteroscedastic Aleatoric Uncertainty . Aleatoric uncertainty는 model의 output에 distribution을 가정합니다. 그리고 이를 위해서 ‘observation noise parameter $ theta$’를 학습시킵니다. . 위에서 언급했듯이, Homoscedastic regression은 모든 data point마다 동일한 observation constant noise $ sigma$를 가집니다. 반면에, Heteroscedastic model에서는 각 data point마다 서로 다른 observation noise를 가지고 있습니다. Non-Bayesian neural network에서는 대게 constance noise $ sigma$를 가정하거나, 무시하곤 합니다. 하지만, 아래 수식과 같이 data-dependent하게 학습시킨다면, data에 대한 fucntion의 형태로 학습될 수 있습니다. . LNN(θ)=1N∑i=1N12σ(xi)2∥yi−f(xi)∥2+12log⁡σ(xi)2 mathbf{L}_{NN}( theta) = frac{1}{N} sum_{i=1}^{N} frac{1}{2 sigma(x_i)^2} rVert y_i - f(x_i) rVert ^ 2 + frac{1}{2} log{ sigma(x_i)^2}LNN​(θ)=N1​i=1∑N​2σ(xi​)21​∥yi​−f(xi​)∥2+21​logσ(xi​)2 . function의 형태로 학습시킨다는 것은 각 data point마다 변하는 uncertainty를 측정할 수 있다는 것을 의미합니다. 또한 epistemic uncertainty를 구하는 것과 다르게 variational inference 대신에 MAP inference를 사용합니다. - finding single value for the model parameters $ theta$ . 참고로 이런 방법은 epistemic uncertainty를 측정하지 못하는데, 위의 접근방법은 data자체의 uncertainty를 구하는 방법이기 때문입니다. . Combining Aleatoric and Epistemic Uncertainty in One Model . aleatoric uncertainty가 noisy data에 더 robust하게 만드는 과정으로 해석될 수 있다는 발견이 두 과정을 합칠 수 있게 하였습니다. (일종의 regularization term으로 해석) . 3.1 Combining Heteroscedastic Aleatoric Uncertainty and Epistemic Uncertainty . Epistemic uncertainty와 aleatoric uncertainty를 함께 구하기 위해서 [2. 2 Heteroscedastic Aleatoric Uncertainty]을 bayesian NN에 적용하였습니다. 본 논문에서는 BNN의 posterior를 dropout variational distribution으로 근사하였습니다. (2.1 Epistemic Uncertainty in Bayesian Deep Learning의 dropout 참고) . 위의 모델의 output은 다음과 같이 predictive mean, predictive variance를 가지게 됩니다. . [y^,σ^2]=fW^(X)[ hat{y}, hat{ sigma}^2] = f^{ hat{W}}(X)[y^​,σ^2]=fW^(X) . 이때 model weight ${ hat{W}} sim q(W)$로 근사합니다. . LBNN(θ)=1D∑i12σ^−2∥yi−yi^∥2+12log⁡σi^2 mathbf{L}_{BNN}( theta) = frac{1}{D} sum_{i} frac{1}{2} hat{ sigma}^{-2} rVert y_i - hat{y_i} rVert ^2 + frac{1}{2} log{ hat{ sigma_i}^2}LBNN​(θ)=D1​i∑​21​σ^−2∥yi​−yi​^​∥2+21​logσi​^​2 . where . $D$는 image $x$에 해당하는 output pixel $y_i$의 개수이다. (pixel 단위의 objective) | $ hat{ sigma_i}^2$은 pixel $i$에 대한 $BNN$의 output(predictive variance) | . 위의 term은 두가지 성분으로 분리 될 수 있습니다. . residual regression: $ frac{1}{D} sum_{i} frac{1}{2} rVert y_i - hat{y_i} rVert ^2$ | uncertainty regularization: $ frac{1}{2} log{ hat{ sigma_i}^2}$ | 실제로 위의 수식을 적용할 때는 아래와 같이 조금 변형된 수식으로 학습을 진행합니다. 이는 division-zero의 문제를 해결하기 위해서라고 합니다. . LBNN(θ)=1D∑i12exp(−log⁡σ^2)∥yi−yi^∥2+12log⁡σi^2 mathbf{L}_{BNN}( theta) = frac{1}{D} sum_{i} frac{1}{2} exp(- log{ hat{ sigma}^2}) rVert y_i - hat{y_i} rVert ^2 + frac{1}{2} log{ hat{ sigma_i}^2}LBNN​(θ)=D1​i∑​21​exp(−logσ^2)∥yi​−yi​^​∥2+21​logσi​^​2 . 아래는pixel y 에 대한 위의 모델의 predictive uncertainty를 근사하는 수식입니다. . Var(y)≈1T∑t=1Ty^t2−(1T∑t=1Ty^t)2+1T∑t=1Tσ^t2Var(y) approx frac{1}{T} sum_{t=1}^{T} hat{y}_t^2-( frac{1}{T} sum_{t=1}^{T} hat{y}_t)^2 + frac{1}{T} sum_{t=1}^T hat{ sigma}_t^2Var(y)≈T1​t=1∑T​y^​t2​−(T1​t=1∑T​y^​t​)2+T1​t=1∑T​σ^t2​ . with $[ hat{y}t, hat{ sigma}_t]{t=1}^{T}$ a set of T smapled outputs: $[ hat{y}_t, hat{ sigma}_t^2] = f^{ hat{W}_t}(X)$ for randomly masked weights ${ hat{W}_t} sim q(W)$ . 위에서의 설명을 이해하셨다면, 첫번째 term은 epistemic을 두번째 term은 aleatoric을 의미한다는 것을 알 수 있습니다. . Experiment . Semantic Segmentation . table1.png . . [Table 1 - a]Semantic segmentation task에서 실험한 결과 새로운 new state-of-the-art의 결과를 내었습니다. (IOU 67.5%) . [Table 1 - b] NYUv2는 위의 a의 dataset보다 더 어려운 task이다. (더 많은 class를 가지고 있다.) 결과는 아래의 이미지에서 확인할 수 있습니다. . . Pixel-wise Depth Regression . . pixel의 depth regression task에서도 실험을 진행하였습니다. 실험결과 aleatoric uncertainty는 depth-regression task에서 많은 부분 기여할 수 있었습니다. 다음 이미지들을 보면 확인할 수 있습니다. . . . 위의 이미지를 보면, aleatoric uncertainty는 depth가 깊을수록, 반사되는 표면을 가질수록, occlusion boundary일수록 높아지는 것을 확인할 수 있습니다. 이는 monocular depth algorithm들이 겪는 어려움들입니다. 반면에 epistemic uncertainty는 data가 부족한 점을 이용하여 이런 어려움들을 잡아냅니다. 예를 들어서 [Figure 5]에서 맨밑의 예를 보면, 사람이 있습니다. 이는 train data에 거의 없는 data로 epistemic uncertainty가 높아짐을 확인할 수 있습니다. . Analysis: What Do Aleatoric and Epistemic Uncertainties Capture? . 5.1 Quality of Uncertainty Metric . . 위의 이미지는 precision-recall 그래프이다. 해당 그래프는 threshhold보다 높은 uncertainty를 가지는 pixel을 제거할 때마다 model의 performance가 증가하는 것을 보여주고 있다. uncertainty가 높은 pixel을 제거하면 precision은 증가하지만, recall은 감소하게 된다. . precision: $ frac{tp}{tp + fp}$ | recall: $ frac{tp}{tp + fn}$ | . 첫번째로 uncertainty measurement가 accuracy와 상관관계가 있음을 보여주고 있다. . 두번째로 epistemic uncertainty와 aleatoric uncertainty 그래프는 상당히 유사한 모양을 가지고 있다. 이것은 각 uncertainty가 다른 uncertainty와 비슷한 역할을 할 수 있다는 것을 의미한다.(다른 uncertainty가 없어도) 이는 하나의 uncertainty만 modeling하더라도 다른 uncertainty의 부족함을 채울려고 한다고 생각하면 된다. . . 위의 이미지는 test set에 대한 calibration plot을 나타낸다. calibration plot이란 예측하는 확률값이 실제의 확률과 얼마나 유사한지 나타내는 역할을 한다. 예를 들어서, softmax value= 0.7이 실제로 70%의 확률로 나타나는지 확인하는 것이다. discret한 확률구간을 정의하고 각 확률구간에 대한 빈도를 측정한다. $y=x$ 그래프와 유사할수록 더 정확한 calibration이라고 할 수 있습니다. . 5.2 Uncertainty with Distance from Training Data . . Aleatoric uncertainty는 더 많은 데이터가 있어도 설명할 수 없습니다. | Aleatoric uncertainty는 out-of-example(이상치)에 대해서 증가하지 않지만, epistemic은 증가합니다. | . 위의 실험결과는 epistemic uncertainty는 unseen data가 있는 상황에서 효과적이며 이는 safety-critical한 domain에서 효과적이라는 것을 보여줍니다. . 5.3 Real-Time Application . aleatoric 자체는 모델을 계산할 때 많은계산량을 요구하지 않는반면, epistemic은 Monte-Calro sampling을 이용해야하기 때문에 이는 계산량이 많습니다. 따라서 real-time이 요구되는 application에서는 alatoric uncertainty만 적용하는 것도 방법이 될 수 있습니다. . 현업에서 epistemic uncertainty를 적용하기에는 무리가 있는 상황입니다. 따라서 앞으로의 연구방향으로 real-time epistemic uncertainty을 deep learning에 적용하는 것은 중요한 이슈입니다. .",
            "url": "https://fastpages.fast.ai/uncertainty/computer_vision/2019/06/29/What-Uncertainties-Do-We-Need-in-Bayesian-Deep.html",
            "relUrl": "/uncertainty/computer_vision/2019/06/29/What-Uncertainties-Do-We-Need-in-Bayesian-Deep.html",
            "date": " • Jun 29, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "PROJECTS . Mixup implementation . Data augmentation via convex combination. . . Hedged instance embedding implementation . Modeled uncertainty by hedging the location of each input in embedding space. . [low uncertainty in clean image] . [high uncertainty in occluded image] . . Error encoding network implementation . Visualized that the distance of different latent variables measures the similarity of images. . **Mixture density network implementation ** . Visualized explainable variance and unexplainable variance. . AWARDS AND HONORS . Ocean litter detector | Stock network analysis | Innovation hackaton | .",
          "url": "https://fastpages.fast.ai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://fastpages.fast.ai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fastpages.fast.ai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}